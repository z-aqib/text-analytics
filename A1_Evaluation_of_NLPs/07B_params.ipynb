{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n",
    "performing all necessary imports here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T10:17:45.806544Z",
     "iopub.status.busy": "2025-02-02T10:17:45.806211Z",
     "iopub.status.idle": "2025-02-02T10:18:05.871179Z",
     "shell.execute_reply": "2025-02-02T10:18:05.870287Z",
     "shell.execute_reply.started": "2025-02-02T10:17:45.806511Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import model related libraries\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# import huggingface login\n",
    "from huggingface_hub import login\n",
    "\n",
    "# import the accesstoken of huggingface, which is saved in .env file\n",
    "# from dotenv import load_dotenv # it was not running on kaggle so i have commented it\n",
    "import os\n",
    "\n",
    "# import time to compute how long it took the model to run\n",
    "import time\n",
    "\n",
    "# import date-time to display the date and time of last run\n",
    "from datetime import datetime\n",
    "\n",
    "# import text wrap to make sure its fully displayable\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Inputs\n",
    "here we initailize all inputs given, like the input text, the prompt, and the model name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Text\n",
    "here is the email we will be passing for all models: this is given in the question and stays constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T10:18:05.872592Z",
     "iopub.status.busy": "2025-02-02T10:18:05.872054Z",
     "iopub.status.idle": "2025-02-02T10:18:05.878770Z",
     "shell.execute_reply": "2025-02-02T10:18:05.877964Z",
     "shell.execute_reply.started": "2025-02-02T10:18:05.872567Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "input_text = \"\"\"Subject: Concerns About Professor X’s Conduct \n",
    " \n",
    "Dear Dr. Ustaad, \n",
    "I hope this email finds you well. I am writing to express my concerns about Professor X’s conduct during the Introduction to Zoology class last semester. On multiple occasions, Professor X made dismissive remarks about students’ questions and failed to provide clear feedback on assignments.  \n",
    " \n",
    "Additionally, the grading seemed inconsistent and unfair, with no opportunity for clarification or appeal. \n",
    " \n",
    "I found this experience deeply frustrating and demotivating, and I believe it affected my performance in the course. I would appreciate it if the department could look into this matter and ensure that future students have a more positive and supportive learning environment. \n",
    " \n",
    "Thank you for your attention to this matter. \n",
    "Sincerely, \n",
    "Shaagird\n",
    "\"\"\"\n",
    "\n",
    "print(\"Input Text:\\n\", input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Params\n",
    "here we intialize the model name, its parameters, and then the text_generator we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T10:18:05.879813Z",
     "iopub.status.busy": "2025-02-02T10:18:05.879556Z",
     "iopub.status.idle": "2025-02-02T10:20:37.688143Z",
     "shell.execute_reply": "2025-02-02T10:20:37.687445Z",
     "shell.execute_reply.started": "2025-02-02T10:18:05.879793Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_name = \"tiiuae/falcon-7b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T10:20:37.689918Z",
     "iopub.status.busy": "2025-02-02T10:20:37.689640Z",
     "iopub.status.idle": "2025-02-02T10:20:37.695705Z",
     "shell.execute_reply": "2025-02-02T10:20:37.694929Z",
     "shell.execute_reply.started": "2025-02-02T10:20:37.689896Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login, Authentication\n",
    "here we authenticate the hugging face access token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T10:20:37.697059Z",
     "iopub.status.busy": "2025-02-02T10:20:37.696826Z",
     "iopub.status.idle": "2025-02-02T10:20:37.706817Z",
     "shell.execute_reply": "2025-02-02T10:20:37.706097Z",
     "shell.execute_reply.started": "2025-02-02T10:20:37.697039Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# first we load the access token from the .env hidden file (it is a gitignore file)\n",
    "# load_dotenv() # it was not running on kaggle so i have commentated it\n",
    "# api_token = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    "api_token = # put your api token here, have removed for github security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T10:20:37.707950Z",
     "iopub.status.busy": "2025-02-02T10:20:37.707660Z",
     "iopub.status.idle": "2025-02-02T10:20:37.760218Z",
     "shell.execute_reply": "2025-02-02T10:20:37.759677Z",
     "shell.execute_reply.started": "2025-02-02T10:20:37.707921Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "login(api_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Running\n",
    "here we run the model, but before that we display the model details and also compute the timetaken to run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Details\n",
    "here we display how many parameters and how much memory the model took"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T10:20:37.761100Z",
     "iopub.status.busy": "2025-02-02T10:20:37.760917Z",
     "iopub.status.idle": "2025-02-02T10:20:37.767906Z",
     "shell.execute_reply": "2025-02-02T10:20:37.767208Z",
     "shell.execute_reply.started": "2025-02-02T10:20:37.761083Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(model.dtype)\n",
    "\n",
    "# Parameters Computation\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model Total Parameters: {total_params / 1e9:.2f} billion\")\n",
    "\n",
    "# Memory used Computation (in MBs)\n",
    "memory = total_params * 2 / (1024 ** 2)  \n",
    "print(f\"Estimate Memory Footprint: {memory:.2f} MB\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Running\n",
    "here for every task, we will first save the start_time and the end_time and then compute the timetaken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T10:20:37.768987Z",
     "iopub.status.busy": "2025-02-02T10:20:37.768707Z",
     "iopub.status.idle": "2025-02-02T10:20:37.779095Z",
     "shell.execute_reply": "2025-02-02T10:20:37.778435Z",
     "shell.execute_reply.started": "2025-02-02T10:20:37.768955Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_time(start_time, end_time):\n",
    "    \"\"\"This function calculates the time difference between two given times in seconds. For this given problem, this function computes the timetaken by a model to run and perform a task\n",
    "\n",
    "    Args:\n",
    "        start_time (time): start time of the model\n",
    "        end_time (time): end time of the model\n",
    "\n",
    "    Returns:\n",
    "        string: the time taken in hours, minutes, seconds, and mili-seconds\n",
    "    \"\"\"\n",
    "    elapsed_time = end_time - start_time\n",
    "    hours, rem = divmod(elapsed_time, 3600)\n",
    "    minutes, rem = divmod(rem, 60)\n",
    "    seconds, milliseconds = divmod(rem, 1)\n",
    "    milliseconds *= 1000  # Convert seconds fraction to milliseconds\n",
    "    \n",
    "    return f\"Time taken: {int(hours)}h {int(minutes)}m {int(seconds)}s {int(milliseconds)}ms\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Making\n",
    "here we define the prompt we will use for all tasks of summarization, question answering, keyword extraction and translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T10:38:31.984167Z",
     "iopub.status.busy": "2025-02-02T10:38:31.983833Z",
     "iopub.status.idle": "2025-02-02T10:38:31.987836Z",
     "shell.execute_reply": "2025-02-02T10:38:31.987045Z",
     "shell.execute_reply.started": "2025-02-02T10:38:31.984141Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Perform the following actions: \n",
    "1 - Summarize the following text delimited by triple backticks.\n",
    "2 - Answer “What was the reason for the student's disappointment with Professor X? ” based on the email content.\n",
    "3 - Identify key details such as incidents, concerns, and requested actions. \n",
    "4 - Translate the following text into French.\n",
    "\n",
    "Separate your answers with line breaks.\n",
    "Text:\n",
    "```{input_text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Execution\n",
    "here we run the model by passing the prompt to a text_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T10:38:35.557538Z",
     "iopub.status.busy": "2025-02-02T10:38:35.557197Z",
     "iopub.status.idle": "2025-02-02T10:38:59.568132Z",
     "shell.execute_reply": "2025-02-02T10:38:59.567317Z",
     "shell.execute_reply.started": "2025-02-02T10:38:35.557510Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# generate the output\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "output = generator(messages)\n",
    "\n",
    "# end the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# display time taken with output\n",
    "print(compute_time(start_time, end_time))\n",
    "print(\"Output:\\n\", output[0][\"generated_text\"])#textwrap.fill(output[0][\"generated_text\"], width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T10:39:19.731965Z",
     "iopub.status.busy": "2025-02-02T10:39:19.731672Z",
     "iopub.status.idle": "2025-02-02T10:39:19.735570Z",
     "shell.execute_reply": "2025-02-02T10:39:19.734845Z",
     "shell.execute_reply.started": "2025-02-02T10:39:19.731942Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt_back_to_eng = \"\"\"\n",
    "Translate the following text to English:\n",
    "Texte traduit en français :\n",
    "Objet : Préoccupations concernant le comportement de M. X\n",
    "\n",
    "Cher Dr. Ustaad,\n",
    "J'espère que cette lettre vous trouve bien. Je vous écris pour exprimer mes préoccupations concernant le comportement de M. X lors de la classe d'Introduction à la zoologie l'an dernier. De nombreuses fois, M. X a fait des remarques moqueuses sur les questions des étudiants et n'a pas fourni une feedback claire sur les devoirs.\n",
    "De plus, la notation semblait inégale et injuste, sans possibilité de clarification ou de recours.\n",
    "Je trouve cette expérience profondément frustrante et démotivante, et je crois qu'elle a affecté ma performance dans le cours. J'apprécierais que la département examine ce sujet et assure un environnement de formation plus positif et soutenu pour les futurs étudiants.\n",
    "Merci de votre attention à ce sujet.\n",
    "Sincèrement,\n",
    "Shaagird\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T10:39:23.085257Z",
     "iopub.status.busy": "2025-02-02T10:39:23.084970Z",
     "iopub.status.idle": "2025-02-02T10:39:33.144240Z",
     "shell.execute_reply": "2025-02-02T10:39:33.143528Z",
     "shell.execute_reply.started": "2025-02-02T10:39:23.085236Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# generate the output\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt_back_to_eng}\n",
    "]\n",
    "output = generator(messages)\n",
    "\n",
    "# end the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# display time taken with output\n",
    "print(compute_time(start_time, end_time))\n",
    "print(\"Output:\\n\", output[0][\"generated_text\"])#textwrap.fill(output[0][\"generated_text\"], width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End\n",
    "here we display the last time this jupyter notebook was run to always remember it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T10:39:49.826758Z",
     "iopub.status.busy": "2025-02-02T10:39:49.826464Z",
     "iopub.status.idle": "2025-02-02T10:39:49.831619Z",
     "shell.execute_reply": "2025-02-02T10:39:49.830679Z",
     "shell.execute_reply.started": "2025-02-02T10:39:49.826729Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
