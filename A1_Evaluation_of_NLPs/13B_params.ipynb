{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries. \nperforming all necessary imports here","metadata":{}},{"cell_type":"code","source":"# import model related libraries\nfrom transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n\n# import huggingface login\nfrom huggingface_hub import login\n\n# import the accesstoken of huggingface, which is saved in .env file\n# from dotenv import load_dotenv # it was not running on kaggle so i have commented it\nimport os\n\n# import time to compute how long it took the model to run\nimport time\n\n# import date-time to display the date and time of last run\nfrom datetime import datetime\n\n# import text wrap to make sure its fully displayable\nimport textwrap\n\n# import torch as we want to set its datatype as this is a larger model\nimport torch","metadata":{"execution":{"iopub.status.busy":"2025-02-02T16:49:36.440159Z","iopub.execute_input":"2025-02-02T16:49:36.440488Z","iopub.status.idle":"2025-02-02T16:49:59.165994Z","shell.execute_reply.started":"2025-02-02T16:49:36.440460Z","shell.execute_reply":"2025-02-02T16:49:59.165358Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Initialize Inputs\nhere we initailize all inputs given, like the input text, the prompt, and the model name","metadata":{}},{"cell_type":"markdown","source":"### Input Text\nhere is the email we will be passing for all models: this is given in the question and stays constant","metadata":{}},{"cell_type":"code","source":"input_text = \"\"\"Subject: Concerns About Professor X’s Conduct \n \nDear Dr. Ustaad, \nI hope this email finds you well. I am writing to express my concerns about Professor X’s conduct during the Introduction to Zoology class last semester. On multiple occasions, Professor X made dismissive remarks about students’ questions and failed to provide clear feedback on assignments.  \n \nAdditionally, the grading seemed inconsistent and unfair, with no opportunity for clarification or appeal. \n \nI found this experience deeply frustrating and demotivating, and I believe it affected my performance in the course. I would appreciate it if the department could look into this matter and ensure that future students have a more positive and supportive learning environment. \n \nThank you for your attention to this matter. \nSincerely, \nShaagird\n\"\"\"\n\nprint(\"Input Text:\\n\", input_text)","metadata":{"execution":{"iopub.status.busy":"2025-02-02T16:49:59.167059Z","iopub.execute_input":"2025-02-02T16:49:59.167563Z","iopub.status.idle":"2025-02-02T16:49:59.172821Z","shell.execute_reply.started":"2025-02-02T16:49:59.167538Z","shell.execute_reply":"2025-02-02T16:49:59.171757Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Input Text:\n Subject: Concerns About Professor X’s Conduct \n \nDear Dr. Ustaad, \nI hope this email finds you well. I am writing to express my concerns about Professor X’s conduct during the Introduction to Zoology class last semester. On multiple occasions, Professor X made dismissive remarks about students’ questions and failed to provide clear feedback on assignments.  \n \nAdditionally, the grading seemed inconsistent and unfair, with no opportunity for clarification or appeal. \n \nI found this experience deeply frustrating and demotivating, and I believe it affected my performance in the course. I would appreciate it if the department could look into this matter and ensure that future students have a more positive and supportive learning environment. \n \nThank you for your attention to this matter. \nSincerely, \nShaagird\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Model Params\nhere we intialize the model name, its parameters, and then the text_generator we will be using","metadata":{}},{"cell_type":"code","source":"model_name = \"microsoft/phi-4\"\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n    trust_remote_code=True\n)\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2025-02-02T17:10:41.926431Z","iopub.execute_input":"2025-02-02T17:10:41.926764Z","iopub.status.idle":"2025-02-02T17:10:44.583270Z","shell.execute_reply.started":"2025-02-02T17:10:41.926738Z","shell.execute_reply":"2025-02-02T17:10:44.582523Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42314bc4001f47f9b845373a7aa82429"}},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"generator = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    return_full_text=False,\n    max_new_tokens=5000,\n    do_sample=False\n)","metadata":{"execution":{"iopub.status.busy":"2025-02-02T17:10:44.584382Z","iopub.execute_input":"2025-02-02T17:10:44.584602Z","iopub.status.idle":"2025-02-02T17:10:44.614096Z","shell.execute_reply.started":"2025-02-02T17:10:44.584583Z","shell.execute_reply":"2025-02-02T17:10:44.613206Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Device set to use cpu\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Login, Authentication\nhere we authenticate the hugging face access token","metadata":{}},{"cell_type":"code","source":"# first we load the access token from the .env hidden file (it is a gitignore file)\n# load_dotenv() # it was not running on kaggle so i have commentated it\n# api_token = os.getenv(\"HUGGINGFACE_API_TOKEN\")\napi_token = \"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\"# put your api token here, have removed for github security","metadata":{"execution":{"iopub.status.busy":"2025-02-02T17:10:44.616292Z","iopub.execute_input":"2025-02-02T17:10:44.616591Z","iopub.status.idle":"2025-02-02T17:10:44.631194Z","shell.execute_reply.started":"2025-02-02T17:10:44.616569Z","shell.execute_reply":"2025-02-02T17:10:44.630329Z"},"trusted":true},"outputs":[],"execution_count":16},{"cell_type":"code","source":"login(api_token)","metadata":{"execution":{"iopub.status.busy":"2025-02-02T17:10:44.632365Z","iopub.execute_input":"2025-02-02T17:10:44.632708Z","iopub.status.idle":"2025-02-02T17:10:44.687541Z","shell.execute_reply.started":"2025-02-02T17:10:44.632678Z","shell.execute_reply":"2025-02-02T17:10:44.686925Z"},"trusted":true},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# Model Running\nhere we run the model, but before that we display the model details and also compute the timetaken to run the model","metadata":{}},{"cell_type":"markdown","source":"### Model Details\nhere we display how many parameters and how much memory the model took","metadata":{}},{"cell_type":"code","source":"print(model.dtype)\n\n# Parameters Computation\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Model Total Parameters: {total_params / 1e9:.2f} billion\")\n\n# Memory used Computation (in MBs)\nmemory = total_params * 2 / (1024 ** 2)  \nprint(f\"Estimate Memory Footprint: {memory:.2f} MB\\n\")","metadata":{"execution":{"iopub.status.busy":"2025-02-02T17:10:44.688317Z","iopub.execute_input":"2025-02-02T17:10:44.688535Z","iopub.status.idle":"2025-02-02T17:10:44.694841Z","shell.execute_reply.started":"2025-02-02T17:10:44.688513Z","shell.execute_reply":"2025-02-02T17:10:44.694145Z"},"trusted":true},"outputs":[{"name":"stdout","text":"torch.bfloat16\nModel Total Parameters: 14.66 billion\nEstimate Memory Footprint: 27960.79 MB\n\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"## Model Running\nhere for every task, we will first save the start_time and the end_time and then compute the timetaken","metadata":{}},{"cell_type":"code","source":"def compute_time(start_time, end_time):\n    \"\"\"This function calculates the time difference between two given times in seconds. For this given problem, this function computes the timetaken by a model to run and perform a task\n\n    Args:\n        start_time (time): start time of the model\n        end_time (time): end time of the model\n\n    Returns:\n        string: the time taken in hours, minutes, seconds, and mili-seconds\n    \"\"\"\n    elapsed_time = end_time - start_time\n    hours, rem = divmod(elapsed_time, 3600)\n    minutes, rem = divmod(rem, 60)\n    seconds, milliseconds = divmod(rem, 1)\n    milliseconds *= 1000  # Convert seconds fraction to milliseconds\n    \n    return f\"Time taken: {int(hours)}h {int(minutes)}m {int(seconds)}s {int(milliseconds)}ms\"","metadata":{"execution":{"iopub.status.busy":"2025-02-02T17:10:44.695544Z","iopub.execute_input":"2025-02-02T17:10:44.695748Z","iopub.status.idle":"2025-02-02T17:10:44.707314Z","shell.execute_reply.started":"2025-02-02T17:10:44.695729Z","shell.execute_reply":"2025-02-02T17:10:44.706707Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### Prompt Making\nhere we define the prompt we will use for all tasks of summarization, question answering, keyword extraction and translation","metadata":{}},{"cell_type":"code","source":"prompt = f\"\"\"\nPerform the following actions: \n1 - Summarize the following text delimited by triple backticks.\n2 - Answer “What was the reason for the student's disappointment with Professor X? ” based on the email content.\n3 - Identify key details such as incidents, concerns, and requested actions. \n4 - Translate the following text into French.\n\nSeparate your answers with line breaks.\nText:\n```{input_text}```\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2025-02-02T17:10:44.708009Z","iopub.execute_input":"2025-02-02T17:10:44.708200Z","iopub.status.idle":"2025-02-02T17:10:44.720603Z","shell.execute_reply.started":"2025-02-02T17:10:44.708180Z","shell.execute_reply":"2025-02-02T17:10:44.719835Z"},"trusted":true},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"### Model Execution\nhere we run the model by passing the prompt to a text_generator","metadata":{}},{"cell_type":"code","source":"# start the timer\nstart_time = time.time()\n\n# generate the output\n# messages = [\n#     {\"role\": \"user\", \"content\": prompt}\n# ]\noutput = generator(prompt)\n\n# end the timer\nend_time = time.time()\n\n# display time taken with output\nprint(compute_time(start_time, end_time))\nprint(\"Output:\\n\", output[0][\"generated_text\"])#textwrap.fill(output[0][\"generated_text\"], width=80))","metadata":{"execution":{"iopub.status.busy":"2025-02-02T17:10:44.722600Z","iopub.execute_input":"2025-02-02T17:10:44.722818Z","iopub.status.idle":"2025-02-02T17:14:51.388028Z","shell.execute_reply.started":"2025-02-02T17:10:44.722787Z","shell.execute_reply":"2025-02-02T17:14:51.387330Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Time taken: 0h 4m 6s 653ms\nOutput:\n \n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"prompt_back_to_eng = \"\"\"\nTranslate the following text to English:\nTexte traduit en français :\nObjet : Préoccupations concernant le comportement de M. X\n\nCher Dr. Ustaad,\nJ'espère que cette lettre vous trouve bien. Je vous écris pour exprimer mes préoccupations concernant le comportement de M. X lors de la classe d'Introduction à la zoologie l'an dernier. De nombreuses fois, M. X a fait des remarques moqueuses sur les questions des étudiants et n'a pas fourni une feedback claire sur les devoirs.\nDe plus, la notation semblait inégale et injuste, sans possibilité de clarification ou de recours.\nJe trouve cette expérience profondément frustrante et démotivante, et je crois qu'elle a affecté ma performance dans le cours. J'apprécierais que la département examine ce sujet et assure un environnement de formation plus positif et soutenu pour les futurs étudiants.\nMerci de votre attention à ce sujet.\nSincèrement,\nShaagird\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2025-02-02T17:14:51.389102Z","iopub.execute_input":"2025-02-02T17:14:51.389445Z","iopub.status.idle":"2025-02-02T17:14:51.392839Z","shell.execute_reply.started":"2025-02-02T17:14:51.389400Z","shell.execute_reply":"2025-02-02T17:14:51.392157Z"},"trusted":true},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# start the timer\nstart_time = time.time()\n\n# generate the output\nmessages = [\n    {\"role\": \"user\", \"content\": prompt_back_to_eng}\n]\noutput = generator(messages)\n\n# end the timer\nend_time = time.time()\n\n# display time taken with output\nprint(compute_time(start_time, end_time))\nprint(\"Output:\\n\", output[0][\"generated_text\"])#textwrap.fill(output[0][\"generated_text\"], width=80))","metadata":{"execution":{"iopub.status.busy":"2025-02-02T17:14:51.393527Z","iopub.execute_input":"2025-02-02T17:14:51.393729Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# End\nhere we display the last time this jupyter notebook was run to always remember it","metadata":{}},{"cell_type":"code","source":"print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}