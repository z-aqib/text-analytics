{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries. \n",
    "performing all necessary imports here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T17:20:26.091634Z",
     "iopub.status.busy": "2025-02-02T17:20:26.091363Z",
     "iopub.status.idle": "2025-02-02T17:20:48.081174Z",
     "shell.execute_reply": "2025-02-02T17:20:48.080341Z",
     "shell.execute_reply.started": "2025-02-02T17:20:26.091603Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import model related libraries\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# import huggingface login\n",
    "from huggingface_hub import login\n",
    "\n",
    "# import the accesstoken of huggingface, which is saved in .env file\n",
    "# from dotenv import load_dotenv # it was not running on kaggle so i have commented it\n",
    "import os\n",
    "\n",
    "# import time to compute how long it took the model to run\n",
    "import time\n",
    "\n",
    "# import date-time to display the date and time of last run\n",
    "from datetime import datetime\n",
    "\n",
    "# import text wrap to make sure its fully displayable\n",
    "import textwrap\n",
    "\n",
    "# import torch as we want to set its datatype as this is a larger model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Inputs\n",
    "here we initailize all inputs given, like the input text, the prompt, and the model name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Text\n",
    "here is the email we will be passing for all models: this is given in the question and stays constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T17:20:48.082592Z",
     "iopub.status.busy": "2025-02-02T17:20:48.082022Z",
     "iopub.status.idle": "2025-02-02T17:20:48.087195Z",
     "shell.execute_reply": "2025-02-02T17:20:48.086459Z",
     "shell.execute_reply.started": "2025-02-02T17:20:48.082567Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:\n",
      " Subject: Concerns About Professor X’s Conduct \n",
      " \n",
      "Dear Dr. Ustaad, \n",
      "I hope this email finds you well. I am writing to express my concerns about Professor X’s conduct during the Introduction to Zoology class last semester. On multiple occasions, Professor X made dismissive remarks about students’ questions and failed to provide clear feedback on assignments.  \n",
      " \n",
      "Additionally, the grading seemed inconsistent and unfair, with no opportunity for clarification or appeal. \n",
      " \n",
      "I found this experience deeply frustrating and demotivating, and I believe it affected my performance in the course. I would appreciate it if the department could look into this matter and ensure that future students have a more positive and supportive learning environment. \n",
      " \n",
      "Thank you for your attention to this matter. \n",
      "Sincerely, \n",
      "Shaagird\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"Subject: Concerns About Professor X’s Conduct \n",
    " \n",
    "Dear Dr. Ustaad, \n",
    "I hope this email finds you well. I am writing to express my concerns about Professor X’s conduct during the Introduction to Zoology class last semester. On multiple occasions, Professor X made dismissive remarks about students’ questions and failed to provide clear feedback on assignments.  \n",
    " \n",
    "Additionally, the grading seemed inconsistent and unfair, with no opportunity for clarification or appeal. \n",
    " \n",
    "I found this experience deeply frustrating and demotivating, and I believe it affected my performance in the course. I would appreciate it if the department could look into this matter and ensure that future students have a more positive and supportive learning environment. \n",
    " \n",
    "Thank you for your attention to this matter. \n",
    "Sincerely, \n",
    "Shaagird\n",
    "\"\"\"\n",
    "\n",
    "print(\"Input Text:\\n\", input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Params\n",
    "here we intialize the model name, its parameters, and then the text_generator we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T17:20:48.088424Z",
     "iopub.status.busy": "2025-02-02T17:20:48.088110Z",
     "iopub.status.idle": "2025-02-02T17:34:28.657138Z",
     "shell.execute_reply": "2025-02-02T17:34:28.656228Z",
     "shell.execute_reply.started": "2025-02-02T17:20:48.088396Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a9fc31630148bcbb1885814722e537",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/820 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628669a269154042a8b19c4f37734981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/20.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a68856f34f4fc1b44de2b6c71af40e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ef8fb8158fa4c0bb6b621d35cdb67a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00006.safetensors:   0%|          | 0.00/4.93G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8ff61d5ff348dbb5a7d44f38447b3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00006.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14658c7ec3794213b43888ba513dddce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00006.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a92b826a24e44839b22324185f11e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00006.safetensors:   0%|          | 0.00/4.77G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b713fec466440ca73af028aab689f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00006.safetensors:   0%|          | 0.00/4.77G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1c8ece5c164ea48810843ea2402fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00006.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b9ffc7a2f04f44a2d922c4f7aa2c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee41ac6e5344a138612922a256442f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8f4aa97a454b97b917d24a00f5cc8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/17.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b0e1c0342349ba87245b5ba22cfa5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.61M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15fb4ea0fb494ae1a89cce9717f784df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/917k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "634d95e81dbd49b8bbb5eb34f70b5c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/4.25M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ccfbd2ed48404ca98183a682eaa871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/2.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c002fdc12fd44d78d29fa06daa01f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"microsoft/phi-4\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T17:34:28.659634Z",
     "iopub.status.busy": "2025-02-02T17:34:28.659331Z",
     "iopub.status.idle": "2025-02-02T17:34:28.665854Z",
     "shell.execute_reply": "2025-02-02T17:34:28.665177Z",
     "shell.execute_reply.started": "2025-02-02T17:34:28.659612Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login, Authentication\n",
    "here we authenticate the hugging face access token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T17:34:28.667414Z",
     "iopub.status.busy": "2025-02-02T17:34:28.667107Z",
     "iopub.status.idle": "2025-02-02T17:34:28.680767Z",
     "shell.execute_reply": "2025-02-02T17:34:28.680140Z",
     "shell.execute_reply.started": "2025-02-02T17:34:28.667392Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# first we load the access token from the .env hidden file (it is a gitignore file)\n",
    "# load_dotenv() # it was not running on kaggle so i have commentated it\n",
    "# api_token = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    "api_token = \"\" # put your api token here, have removed for github security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T17:34:28.681637Z",
     "iopub.status.busy": "2025-02-02T17:34:28.681441Z",
     "iopub.status.idle": "2025-02-02T17:34:28.952667Z",
     "shell.execute_reply": "2025-02-02T17:34:28.951884Z",
     "shell.execute_reply.started": "2025-02-02T17:34:28.681613Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "login(api_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Running\n",
    "here we run the model, but before that we display the model details and also compute the timetaken to run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Details\n",
    "here we display how many parameters and how much memory the model took"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T17:34:28.953743Z",
     "iopub.status.busy": "2025-02-02T17:34:28.953483Z",
     "iopub.status.idle": "2025-02-02T17:34:28.961957Z",
     "shell.execute_reply": "2025-02-02T17:34:28.961224Z",
     "shell.execute_reply.started": "2025-02-02T17:34:28.953716Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n",
      "Model Total Parameters: 14.66 billion\n",
      "Estimate Memory Footprint: 27960.79 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.dtype)\n",
    "\n",
    "# Parameters Computation\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model Total Parameters: {total_params / 1e9:.2f} billion\")\n",
    "\n",
    "# Memory used Computation (in MBs)\n",
    "memory = total_params * 2 / (1024 ** 2)  \n",
    "print(f\"Estimate Memory Footprint: {memory:.2f} MB\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Running\n",
    "here for every task, we will first save the start_time and the end_time and then compute the timetaken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T17:34:28.963030Z",
     "iopub.status.busy": "2025-02-02T17:34:28.962741Z",
     "iopub.status.idle": "2025-02-02T17:34:28.976570Z",
     "shell.execute_reply": "2025-02-02T17:34:28.975791Z",
     "shell.execute_reply.started": "2025-02-02T17:34:28.963010Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_time(start_time, end_time):\n",
    "    \"\"\"This function calculates the time difference between two given times in seconds. For this given problem, this function computes the timetaken by a model to run and perform a task\n",
    "\n",
    "    Args:\n",
    "        start_time (time): start time of the model\n",
    "        end_time (time): end time of the model\n",
    "\n",
    "    Returns:\n",
    "        string: the time taken in hours, minutes, seconds, and mili-seconds\n",
    "    \"\"\"\n",
    "    elapsed_time = end_time - start_time\n",
    "    hours, rem = divmod(elapsed_time, 3600)\n",
    "    minutes, rem = divmod(rem, 60)\n",
    "    seconds, milliseconds = divmod(rem, 1)\n",
    "    milliseconds *= 1000  # Convert seconds fraction to milliseconds\n",
    "    \n",
    "    return f\"Time taken: {int(hours)}h {int(minutes)}m {int(seconds)}s {int(milliseconds)}ms\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Making\n",
    "here we define the prompt we will use for all tasks of summarization, question answering, keyword extraction and translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T17:34:28.977594Z",
     "iopub.status.busy": "2025-02-02T17:34:28.977356Z",
     "iopub.status.idle": "2025-02-02T17:34:28.991377Z",
     "shell.execute_reply": "2025-02-02T17:34:28.990593Z",
     "shell.execute_reply.started": "2025-02-02T17:34:28.977563Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Perform the following actions: \n",
    "1 - Summarize the following text delimited by triple backticks.\n",
    "2 - Answer “What was the reason for the student's disappointment with Professor X? ” based on the email content.\n",
    "3 - Identify key details such as incidents, concerns, and requested actions. \n",
    "4 - Translate the following text into French.\n",
    "\n",
    "Separate your answers with line breaks.\n",
    "Text:\n",
    "```{input_text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Execution\n",
    "here we run the model by passing the prompt to a text_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T17:35:22.641474Z",
     "iopub.status.busy": "2025-02-02T17:35:22.641141Z",
     "iopub.status.idle": "2025-02-02T17:36:45.440483Z",
     "shell.execute_reply": "2025-02-02T17:36:45.439605Z",
     "shell.execute_reply.started": "2025-02-02T17:35:22.641448Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0h 1m 22s 791ms\n",
      "\n",
      "Summarized Text: The student, Shaagird, is expressing concerns about Professor X’s conduct in the Introduction to Zoology class. Shaagird mentions that Professor X made dismissive remarks, failed to provide clear feedback, and had inconsistent grading. Shaagird found the experience frustrating and demotivating, affecting their performance. They request the department to investigate and improve the learning environment for future students.\n",
      "\n",
      "\n",
      "## response\n",
      "\n",
      "The student, Shaagird, is writing to Dr. Ustaad to express concerns about Professor X's conduct in the Introduction to Zoology class. Shaagird reports that Professor X made dismissive remarks, failed to provide clear feedback on assignments, and had inconsistent and unfair grading. This experience was frustrating and demotivating for Shaag\n",
      "\n",
      "Answer to the question:  The student was disappointed with Professor X because of dismissive remarks about students’ questions, lack of clear feedback on assignments, inconsistent and unfair grading, and no opportunity for clarification or appeal.\n",
      "\n",
      "Extracted Keywords: \n",
      "\n",
      "Translated Text: Objet: Préoccupations concernant le comportement du Professeur X \n",
      "\n",
      "Cher Dr. Ustaad, \n",
      "J'espère que ce courriel vous trouve en bonne santé. Je vous écris pour exprimer mes préoccupations concernant le comportement du Professeur X lors du cours d'introduction à la zoologie l'année dernière. À plusieurs reprises, le Professeur X a fait des remarques dédaigneuses sur les questions des étudiants et a échoué à fournir des commentaires clairs sur les devoirs. \n",
      "\n",
      "De plus, la notation semblait incohérente et injuste, sans possibilité de clarification ou d'appel. \n",
      "\n",
      "J'ai trouvé cette expérience profondément frustrante et démoralisante, et je crois qu'elle a affecté ma performance dans le cours. J'apprécierais que le département examine cette affaire et assure que les futurs étudiants aient un environnement d'apprentissage plus positif et soutenant. \n",
      "\n",
      "Merci de votre attention à cette affaire. \n",
      "Cordialement, \n",
      "Shaagird\n",
      "\n",
      "English Translation:\n",
      "\n",
      "## Solution: English Text:\n",
      "\n",
      "Subject: Concerns About Professor X’s Conduct\n",
      "\n",
      "Dear Dr. Ustaad,\n",
      "\n",
      "I hope this email finds you well. I am writing to express my concerns about Professor X’s conduct during the Introduction to Zoology class last semester. On multiple occasions, Professor X made dismissive remarks about students’ questions and failed to provide clear feedback on assignments.\n",
      "\n",
      "Additionally, the grading seemed inconsistent and unfair, with no opportunity for clarification or appeal.\n",
      "\n",
      "I found this experience deeply frustrating and demotivating, and I believe it affected my performance in the course. I would appreciate it if the department could look into this\n"
     ]
    }
   ],
   "source": [
    "# start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "summarization_prompt = f\"summarize the following text:\\n\\n{input_text}\\n\\nSummary:\"\n",
    "summarization_response = generator(summarization_prompt, max_new_tokens=150, num_return_sequences=1)\n",
    "summarized_text = summarization_response[0]['generated_text'].split(\"Summary:\")[-1].strip()\n",
    "\n",
    "# 2. Question Answering\n",
    "question = \"What was the reason for the student's disappointment with Professor X?\"\n",
    "question_prompt = f\"Context: {input_text}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n",
    "answer_response = generator(question_prompt, max_new_tokens=200, num_return_sequences=1)\n",
    "answer_extracted = answer_response[0]['generated_text']\n",
    "\n",
    "# 3. Keyword Extraction\n",
    "keyword_prompt = f\"Extract important keywords from the following text:\\n\\n{input_text}\\n\\nKeywords:\"\n",
    "keyword_response = generator(keyword_prompt, max_new_tokens=350, num_return_sequences=1)\n",
    "keyword_extracted = keyword_response[0]['generated_text'].split(\"Keywords:\")[-1].strip()\n",
    "\n",
    "# 4. Translation to French and Back to English\n",
    "translation_prompt = f\"Translate the following English text into French:\\n\\n{input_text}\\n\\nFrench Translation:\\n\\nNow, translate the French text back into English:\\n\\nFrench Text:\"\n",
    "# Use text generation to get the French translation and then translate it back to English\n",
    "translation_response = generator(translation_prompt, max_new_tokens=375, num_return_sequences=1)\n",
    "translated_text = translation_response[0]['generated_text'].split(\"French Translation:\")[-1].split(\"French Text:\")[-1].strip()\n",
    "\n",
    "# end the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# display time taken with output\n",
    "print(compute_time(start_time, end_time))\n",
    "# print(\"Output:\\n\", output[0][\"generated_text\"])#textwrap.fill(output[0][\"generated_text\"], width=80))\n",
    "print(\"\\nSummarized Text:\", summarized_text)\n",
    "print(\"\\nAnswer to the question:\", answer_extracted)\n",
    "print(\"\\nExtracted Keywords:\", keyword_extracted)\n",
    "print(\"\\nTranslated Text:\", translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T17:41:41.687511Z",
     "iopub.status.busy": "2025-02-02T17:41:41.687189Z",
     "iopub.status.idle": "2025-02-02T17:41:41.691291Z",
     "shell.execute_reply": "2025-02-02T17:41:41.690330Z",
     "shell.execute_reply.started": "2025-02-02T17:41:41.687488Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt_back_to_eng = \"\"\"\n",
    "Translate the following text to English:\n",
    "Translated Text: Objet: Préoccupations concernant le comportement du Professeur X \n",
    "\n",
    "Cher Dr. Ustaad, \n",
    "J'espère que ce courriel vous trouve en bonne santé. Je vous écris pour exprimer mes préoccupations concernant le comportement du Professeur X lors du cours d'introduction à la zoologie l'année dernière. À plusieurs reprises, le Professeur X a fait des remarques dédaigneuses sur les questions des étudiants et a échoué à fournir des commentaires clairs sur les devoirs. \n",
    "\n",
    "De plus, la notation semblait incohérente et injuste, sans possibilité de clarification ou d'appel. \n",
    "\n",
    "J'ai trouvé cette expérience profondément frustrante et démoralisante, et je crois qu'elle a affecté ma performance dans le cours. J'apprécierais que le département examine cette affaire et assure que les futurs étudiants aient un environnement d'apprentissage plus positif et soutenant. \n",
    "\n",
    "Merci de votre attention à cette affaire. \n",
    "Cordialement, \n",
    "Shaagird\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T17:41:41.692425Z",
     "iopub.status.busy": "2025-02-02T17:41:41.692229Z",
     "iopub.status.idle": "2025-02-02T17:42:05.572863Z",
     "shell.execute_reply": "2025-02-02T17:42:05.572105Z",
     "shell.execute_reply.started": "2025-02-02T17:41:41.692408Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0h 0m 23s 863ms\n",
      "Output:\n",
      " **Subject: Concerns Regarding Professor X's Behavior**\n",
      "\n",
      "Dear Dr. Ustaad,\n",
      "\n",
      "I hope this email finds you in good health. I am writing to express my concerns about Professor X's behavior during last year's introductory zoology course. On several occasions, Professor X made dismissive remarks about students' questions and failed to provide clear feedback on assignments.\n",
      "\n",
      "Additionally, the grading seemed inconsistent and unfair, with no opportunity for clarification or appeal.\n",
      "\n",
      "I found this experience deeply frustrating and demoralizing, and I believe it affected my performance in the course. I would appreciate it if the department could examine this matter and ensure that future students have a more positive and supportive learning environment.\n",
      "\n",
      "Thank you for your attention to this matter.\n",
      "\n",
      "Sincerely,  \n",
      "Shaagird\n"
     ]
    }
   ],
   "source": [
    "# start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# generate the output\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt_back_to_eng}\n",
    "]\n",
    "output = generator(messages)\n",
    "\n",
    "# end the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# display time taken with output\n",
    "print(compute_time(start_time, end_time))\n",
    "print(\"Output:\\n\", output[0][\"generated_text\"])#textwrap.fill(output[0][\"generated_text\"], width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End\n",
    "here we display the last time this jupyter notebook was run to always remember it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T17:42:05.573998Z",
     "iopub.status.busy": "2025-02-02T17:42:05.573784Z",
     "iopub.status.idle": "2025-02-02T17:42:05.578024Z",
     "shell.execute_reply": "2025-02-02T17:42:05.577303Z",
     "shell.execute_reply.started": "2025-02-02T17:42:05.573980Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-02 17:42:05\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
