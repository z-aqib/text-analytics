{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n",
    "performing all necessary imports here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "# import model related libraries\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# import huggingface login\n",
    "from huggingface_hub import login\n",
    "\n",
    "# import the accesstoken of huggingface, which is saved in .env file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# import time to compute how long it took the model to run\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Inputs\n",
    "here we initailize all inputs given, like the input text, the prompt, and the model name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Text\n",
    "here is the email we will be passing for all models: this is given in the question and stays constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:\n",
      " Subject: Concerns About Professor X’s Conduct \n",
      " \n",
      "Dear Dr. Ustaad, \n",
      "I hope this email finds you well. I am writing to express my concerns about Professor X’s conduct during the Introduction to Zoology class last semester. On multiple occasions, Professor X made dismissive remarks about students’ questions and failed to provide clear feedback on assignments.  \n",
      " \n",
      "Additionally, the grading seemed inconsistent and unfair, with no opportunity for clarification or appeal. \n",
      " \n",
      "I found this experience deeply frustrating and demotivating, and I believe it affected my performance in the course. I would appreciate it if the department could look into this matter and ensure that future students have a more positive and supportive learning environment. \n",
      " \n",
      "Thank you for your attention to this matter. \n",
      "Sincerely, \n",
      "Shaagird\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"Subject: Concerns About Professor X’s Conduct \n",
    " \n",
    "Dear Dr. Ustaad, \n",
    "I hope this email finds you well. I am writing to express my concerns about Professor X’s conduct during the Introduction to Zoology class last semester. On multiple occasions, Professor X made dismissive remarks about students’ questions and failed to provide clear feedback on assignments.  \n",
    " \n",
    "Additionally, the grading seemed inconsistent and unfair, with no opportunity for clarification or appeal. \n",
    " \n",
    "I found this experience deeply frustrating and demotivating, and I believe it affected my performance in the course. I would appreciate it if the department could look into this matter and ensure that future students have a more positive and supportive learning environment. \n",
    " \n",
    "Thank you for your attention to this matter. \n",
    "Sincerely, \n",
    "Shaagird\n",
    "\"\"\"\n",
    "\n",
    "print(\"Input Text:\\n\", input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Params\n",
    "here we intialize the model name, its parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-3B-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login, Authentication\n",
    "here we authenticate the hugging face access token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we load the access token from the .env hidden file (it is a gitignore file)\n",
    "load_dotenv()\n",
    "api_token = os.getenv(\"HUGGINGFACE_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Running\n",
    "here we run the model, but before that we display the model details and also compute the timetaken to run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Details\n",
    "here we display how many parameters and how much memory the model took"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters Computation\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model Total Parameters: {total_params / 1e9:.2f} billion\")\n",
    "\n",
    "# Memory used Computation (in MBs)\n",
    "memory = total_params * 2 / (1024 ** 2)  \n",
    "print(f\"Estimated Memory Footprint: {memory:.2f} MB\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Running\n",
    "here for every task, we will first save the start_time and the end_time and then compute the timetaken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time(start_time, end_time):\n",
    "    \"\"\"This function calculates the time difference between two given times in seconds. For this given problem, this function computes the timetaken by a model to run and perform a task\n",
    "\n",
    "    Args:\n",
    "        start_time (time): start time of the model\n",
    "        end_time (time): end time of the model\n",
    "\n",
    "    Returns:\n",
    "        string: the time taken in hours, minutes, seconds, and mili-seconds\n",
    "    \"\"\"\n",
    "    elapsed_time = end_time - start_time\n",
    "    hours, rem = divmod(elapsed_time, 3600)\n",
    "    minutes, rem = divmod(rem, 60)\n",
    "    seconds, milliseconds = divmod(rem, 1)\n",
    "    milliseconds *= 1000  # Convert seconds fraction to milliseconds\n",
    "    \n",
    "    return f\"Time taken: {int(hours)}h {int(minutes)}m {int(seconds)}s {int(milliseconds)}ms\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization\n",
    "here we summarize the text that we have defined before. we will use 75 words to summarize as this is standard. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "outputs = summarizer(input_text, max_length=75, max_new_tokens=75, clean_up_tokenization_spaces=True)\n",
    "\n",
    "end_time = time.time()\n",
    "print(compute_time(start_time, end_time))\n",
    "print(\"\\nSummary:\\n\", outputs[0]['summary_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
