{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries\n",
    "performing all necessary imports here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import model related libraries\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# import huggingface login\n",
    "from huggingface_hub import login\n",
    "\n",
    "# import the accesstoken of huggingface, which is saved in .env file\n",
    "# from dotenv import load_dotenv # it was not running on kaggle so i have commented it\n",
    "import os\n",
    "\n",
    "# import time to compute how long it took the model to run\n",
    "import time\n",
    "\n",
    "# import date-time to display the date and time of last run\n",
    "from datetime import datetime\n",
    "\n",
    "# import text wrap to make sure its fully displayable\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Inputs\n",
    "here we initailize all inputs given, like the input text, the prompt, and the model name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Text\n",
    "here is the email we will be passing for all models: this is given in the question and stays constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text:\n",
      " Subject: Concerns About Professor X’s Conduct \n",
      " \n",
      "Dear Dr. Ustaad, \n",
      "I hope this email finds you well. I am writing to express my concerns about Professor X’s conduct during the Introduction to Zoology class last semester. On multiple occasions, Professor X made dismissive remarks about students’ questions and failed to provide clear feedback on assignments.  \n",
      " \n",
      "Additionally, the grading seemed inconsistent and unfair, with no opportunity for clarification or appeal. \n",
      " \n",
      "I found this experience deeply frustrating and demotivating, and I believe it affected my performance in the course. I would appreciate it if the department could look into this matter and ensure that future students have a more positive and supportive learning environment. \n",
      " \n",
      "Thank you for your attention to this matter. \n",
      "Sincerely, \n",
      "Shaagird\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"Subject: Concerns About Professor X’s Conduct \n",
    " \n",
    "Dear Dr. Ustaad, \n",
    "I hope this email finds you well. I am writing to express my concerns about Professor X’s conduct during the Introduction to Zoology class last semester. On multiple occasions, Professor X made dismissive remarks about students’ questions and failed to provide clear feedback on assignments.  \n",
    " \n",
    "Additionally, the grading seemed inconsistent and unfair, with no opportunity for clarification or appeal. \n",
    " \n",
    "I found this experience deeply frustrating and demotivating, and I believe it affected my performance in the course. I would appreciate it if the department could look into this matter and ensure that future students have a more positive and supportive learning environment. \n",
    " \n",
    "Thank you for your attention to this matter. \n",
    "Sincerely, \n",
    "Shaagird\n",
    "\"\"\"\n",
    "\n",
    "print(\"Input Text:\\n\", input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Params\n",
    "here we intialize the model name, its parameters, and then the text_generator we will be using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-3B-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=5000,\n",
    "    do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login, Authentication\n",
    "here we authenticate the hugging face access token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we load the access token from the .env hidden file (it is a gitignore file)\n",
    "# load_dotenv() # it was not running on kaggle so i have commentated it\n",
    "# api_token = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    "api_token = # put your api token here, have removed for github security"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(api_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Running\n",
    "here we run the model, but before that we display the model details and also compute the timetaken to run the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Details\n",
    "here we display how many parameters and how much memory the model took"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.dtype)\n",
    "\n",
    "# Parameters Computation\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model Total Parameters: {total_params / 1e9:.2f} billion\")\n",
    "\n",
    "# Memory used Computation (in MBs)\n",
    "memory = total_params * 2 / (1024 ** 2)  \n",
    "print(f\"Estimated Memory Footprint: {memory:.2f} MB\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Running\n",
    "here for every task, we will first save the start_time and the end_time and then compute the timetaken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time(start_time, end_time):\n",
    "    \"\"\"This function calculates the time difference between two given times in seconds. For this given problem, this function computes the timetaken by a model to run and perform a task\n",
    "\n",
    "    Args:\n",
    "        start_time (time): start time of the model\n",
    "        end_time (time): end time of the model\n",
    "\n",
    "    Returns:\n",
    "        string: the time taken in hours, minutes, seconds, and mili-seconds\n",
    "    \"\"\"\n",
    "    elapsed_time = end_time - start_time\n",
    "    hours, rem = divmod(elapsed_time, 3600)\n",
    "    minutes, rem = divmod(rem, 60)\n",
    "    seconds, milliseconds = divmod(rem, 1)\n",
    "    milliseconds *= 1000  # Convert seconds fraction to milliseconds\n",
    "    \n",
    "    return f\"Time taken: {int(hours)}h {int(minutes)}m {int(seconds)}s {int(milliseconds)}ms\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt Making\n",
    "here we define the prompt we will use for all tasks of summarization, question answering, keyword extraction and translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Perform the following actions: \n",
    "1 - Summarize the following text delimited by triple backticks.\n",
    "2 - Answer “What was the reason for the student's disappointment with Professor X? ” based on the email content.\n",
    "3 - Identify key details such as incidents, concerns, and requested actions. \n",
    "4 - Translate the following text into French and then back to English.\n",
    "\n",
    "Separate your answers with line breaks.\n",
    "Text:\n",
    "```{input_text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Execution\n",
    "here we run the model by passing the prompt to a text_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# generate the output\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "output = generator(messages)\n",
    "\n",
    "# end the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# display time taken with output\n",
    "print(compute_time(start_time, end_time))\n",
    "print(\"Output:\\n\", textwrap.fill(output[0][\"generated_text\"], width=80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End\n",
    "here we display the last time this jupyter notebook was run to always remember it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-02 15:13:53\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
