{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgement\n",
    "Name: Zuha Aqib     \n",
    "ERP ID: 26106    \n",
    "Section: 10am Miss Solat     \n",
    "Date: (written on) 18-Feb-25     \n",
    "\n",
    "code has been taken from Miss Solat's code files and written by Zuha themselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "print(\"Last time code executed:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erp = 26106 \n",
    "# will be referenced later on in the code when we use it for random_state\n",
    "erp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "here we import all the necessary libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load the dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# to handle the data and perform numerical operations on it\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# to do natural language processing\n",
    "import nltk\n",
    "\n",
    "# preprocessing: to clean the data\n",
    "import re\n",
    "\n",
    "# preprocessing: words tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# preprocessing: stemmming and lemmatization\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# vectorization: BOW\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# vectorization: LSA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# clustering: k means\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# to plot the graph\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Displayer\n",
    "this code is a function that we will call at multiple instances of the code to see how long it took to run the code to see when it ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get current date and time as a string\n",
    "def get_current_datetime():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset\n",
    "Here we load the dataset from a csv file and then save it into a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset/news_Feb_14.csv\")\n",
    "df = df.iloc[:, 0]  # Select only the headline column\n",
    "documents = df.tolist()  # Convert to list\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_headlines = documents.copy()  # Keep original headlines before preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "here we perform the preprocessing of data like:\n",
    "- lowercase text\n",
    "- stopword removal\n",
    "- stemming or lemmatization\n",
    "- n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase Conversion\n",
    "here we convert our text to lowercase to have accurate DTM/TDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [document.lower() for document in documents]\n",
    "\n",
    "print(\"Finished executing at:\", get_current_datetime())\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "here we clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.encode('ascii', 'ignore').decode()  # Remove non-ASCII characters\n",
    "    text = unicodedata.normalize(\"NFKD\", text)  # Normalize Unicode text\n",
    "    \n",
    "    # Separate numbers attached to words\n",
    "    text = re.sub(r'(?<=\\d)(?=[a-zA-Z])', ' ', text)  # number-word\n",
    "    text = re.sub(r'(?<=[a-zA-Z])(?=\\d)', ' ', text)  # word-number\n",
    "    \n",
    "    text = text.replace(\"-\", \" \") # replace hyphens with spaces to tokenize the numbers and words\n",
    "    text = re.sub(r'[^\\w\\s,]', '', text)  # Remove everything except words, numbers, and commas\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()   # remove extra spaces\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [clean_text(text) for text in documents]\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words Removal\n",
    "here we remove stop words. but we initialize stop words in count vectorizer. we have two other options of:\n",
    "- either declaring all possible stop words and then iteratively checking each word in the document if its a stop word and then adding the non-stop words. However this is not a good practice. \n",
    "- pulling stop words from ```ntlk``` library:\n",
    "```\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "documents_no_stopwords = [\" \".join([word for word in doc.split() if word not in stop_words]) \n",
    "```\n",
    "however after running this code it didnt not remove stop words thus i removed it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word = True\n",
    "stop_word_language = 'english'\n",
    "\n",
    "stop_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction\n",
    "here we change words to a smaller common form instead of the 's. We do this using two ways:\n",
    "- stemming\n",
    "- lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "this reduces words to their singular form however it is not said to be very good. we cannot say this surely as we have not rigourously tested it yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "documents = [\" \".join([stemmer.stem(word) for word in doc.split()]) for doc in documents]\n",
    "\n",
    "print(\"Finished executing at:\", get_current_datetime())\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "this also reduces words to their singular form and is said to be better as it only reduces to words in the dictionary. but again we cannot say this surely as we have not tested it yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_doc(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = [lemmatize_doc(doc) for doc in documents]\n",
    "# documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams\n",
    "here we decide how we want the words to be tokenized, either they are\n",
    "- unigram: singular\n",
    "- bigram: pairs (doubular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_range = (1, 1) # unigram\n",
    "# n_gram_range = (2, 2) # bigram\n",
    "\n",
    "n_gram_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "here we convert the text to numerical format for the model to understand it. we have four ways to do it:\n",
    "- CountVectorizer using term-presence\n",
    "- CountVextorizer using term-frequency\n",
    "- TfidfVectorizer using term-frequency-inverse-document-frequency\n",
    "- LSA using different 'n'\n",
    "\n",
    "in this code we are exploring the third method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "here we construct a DTM (document to term matrix) using term-frequency inverse-data-frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stop_word is True:\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_word_language, ngram_range=n_gram_range)\n",
    "    documents = vectorizer.fit_transform(documents)\n",
    "    print('Stop words removed')\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(ngram_range=n_gram_range)\n",
    "    documents = vectorizer.fit_transform(documents)\n",
    "    print('All words kept')\n",
    "\n",
    "print(\"Finished executing at:\", get_current_datetime())\n",
    "documents.toarray()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names (words) from CountVectorizer\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert sparse matrix to DataFrame\n",
    "df = pd.DataFrame(documents.toarray(), columns=feature_names)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_columns', None)  # Show all columns\n",
    "# print(df.head(1))  # Show first X rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find (column, row) indices where frequency != 0\n",
    "indices = np.argwhere(df.values != 0)\n",
    "\n",
    "# Convert indices to a list of (word, row) pairs\n",
    "word_occurrences = [(df.columns[col], row) for row, col in indices]\n",
    "\n",
    "# Display the result\n",
    "print(word_occurrences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns.tolist())  # Converts Index object to list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Means Clustering\n",
    "here we cluster the documents into similar categories using k means clustering algorithm. we will be testing the algorithm for three values of k: 5, 9 and 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_k_means(k, data):\n",
    "    print(f\"Displaying {k} start time:\", get_current_datetime())\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "    data = svd.fit_transform(data)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=erp)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    \n",
    "    # Convert to DataFrame for visualization\n",
    "    df_viz = pd.DataFrame({'X': data[:, 0], 'Y': data[:, 1], 'Cluster': labels})\n",
    "\n",
    "    # Scatter plot of clusters\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=df_viz, x='X', y='Y', hue='Cluster', palette='tab10', s=100, edgecolor='black')\n",
    "    plt.title(\"K-Means Clustering Visualization (2D Projection)\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.legend(title=\"Cluster\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Finished displaying at:\", get_current_datetime(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_means(k, data):\n",
    "    print(f\"Executing {k} start time:\", get_current_datetime())\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=erp)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    \n",
    "    wss = kmeans.inertia_  # Within-Cluster Sum of Squares\n",
    "    silhouette_avg = silhouette_score(data, labels)  # Silhouette Score\n",
    "    \n",
    "    print(f\"K={k}: Silhouette Score and WSS={silhouette_avg:.4f}\\t{wss:.4f}\")\n",
    "    \n",
    "    values = [[silhouette_avg, wss]]\n",
    "    # Create DataFrame and copy to clipboard\n",
    "    df = pd.DataFrame(values)\n",
    "    df.to_clipboard(index=False, header=False)\n",
    "    print(\"copied to clipboard\")\n",
    "    \n",
    "    # Create a DataFrame to store headlines with their assigned clusters\n",
    "    df_clusters = pd.DataFrame({'Headline': original_headlines, 'Cluster': labels})\n",
    "\n",
    "    # Display sample headlines per cluster\n",
    "    for cluster in range(k):\n",
    "        print(f\"\\nCluster {cluster}:\")\n",
    "        sample_headlines = df_clusters[df_clusters[\"Cluster\"] == cluster].sample(n=min(5, len(df_clusters[df_clusters[\"Cluster\"] == cluster])), random_state=erp)\n",
    "        for idx, row in sample_headlines.iterrows():\n",
    "            print(f\"- {row['Headline']}\")\n",
    "    \n",
    "    display_k_means(k, data)\n",
    "    \n",
    "    print(\"Finished executing at:\", get_current_datetime(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_k_means(5, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_k_means(9, documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_k_means(13, documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
