{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgement\n",
    "Name: Zuha Aqib     \n",
    "ERP ID: 26106    \n",
    "Section: 10am Miss Solat     \n",
    "Date: (written on) 08-Mar-25    \n",
    "\n",
    "code has been taken from Miss Solat's code files and written by Zuha themselves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "print(\"Last time code executed:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erp = 26106 \n",
    "# will be referenced later on in the code when we use it for random_state\n",
    "erp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "here we import all the necessary libraries and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load the dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# to handle the data and perform numerical operations on it\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# to do natural language processing\n",
    "import nltk\n",
    "\n",
    "# preprocessing: to clean the data\n",
    "import re\n",
    "\n",
    "# preprocessing: stop words removal\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "\n",
    "# preprocessing: words tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# preprocessing: stemmming and lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# model running\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# clustering: k means\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# to plot the graph\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Displayer\n",
    "this code is a function that we will call at multiple instances of the code to see how long it took to run the code to see when it ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get current date and time as a string\n",
    "def get_current_datetime():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Dataset\n",
    "Here we load the dataset from a csv file and then save it into a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_dataframe = pd.read_csv(\"dataset/news_Feb_14.csv\")\n",
    "dummy_dataframe = dummy_dataframe.iloc[:, 0]  # Select only the headline column\n",
    "documents = dummy_dataframe.tolist()  # Convert to list\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_headlines = documents.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "here we perform the preprocessing of data like:\n",
    "- lowercase text\n",
    "- stopword removal\n",
    "- stemming or lemmatization\n",
    "- n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercase Conversion\n",
    "here we convert our text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [document.lower() for document in documents]\n",
    "\n",
    "print(\"Finished executing at:\", get_current_datetime())\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "here we clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.encode('ascii', 'ignore').decode()  # Remove non-ASCII characters\n",
    "    text = unicodedata.normalize(\"NFKD\", text)  # Normalize Unicode text\n",
    "    \n",
    "    # Separate numbers attached to words\n",
    "    text = re.sub(r'(?<=\\d)(?=[a-zA-Z])', ' ', text)  # number-word\n",
    "    text = re.sub(r'(?<=[a-zA-Z])(?=\\d)', ' ', text)  # word-number\n",
    "    \n",
    "    text = text.replace(\"-\", \" \") # replace hyphens with spaces to tokenize the numbers and words\n",
    "    text = re.sub(r'[^\\w\\s,]', '', text)  # Remove everything except words, numbers, and commas\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()   # remove extra spaces\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [clean_text(text) for text in documents]\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words Removal\n",
    "here we remove stop words. we have three options of:\n",
    "- either declaring all possible stop words and then iteratively checking each word in the document if its a stop word and then adding the non-stop words. \n",
    "```\n",
    "sw = [\"from\",\"of\",\"and\", \"on\",\"for\", \"the\",\"have\",\"a\",\"be\",\"to\",\"in\",\"will\",\"if\",\"by\",\"into\",\"as\"]\n",
    "docs_list = []\n",
    "for d in document: \n",
    "    d_nlp = nlp(d.lower())\n",
    "    t_list = []\n",
    "    for token in d_nlp:\n",
    "        tok_lem = str(token.lemma_)\n",
    "        if (tok_lem not in sw):\n",
    "            t_list.append(tok_lem)\n",
    "    str_ = ' '.join(t_list) \n",
    "    docs_list.append(str_)\n",
    "docs_list\n",
    "```\n",
    "However this is not a good practice. \n",
    "- pulling stop words from ```ntlk``` library: \n",
    "```\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "documents_no_stopwords = [\" \".join([word for word in doc.split() if word not in stop_words]) \n",
    "```\n",
    "However after running this code it didnt not remove stop words thus i removed it\n",
    "- intializing stop_words in ```CountVectorizer()``` however here we are not using it\n",
    "- using the ```ENGLISH_STOP_WORDS``` in ```sklearn```\n",
    "```\n",
    "tokens = [word for word in tokens if word not in stopwords.words('english') and word not in ENGLISH_STOP_WORDS]  \n",
    "```\n",
    "Lets try it and see it how it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize each document\n",
    "tokenized_docs = [word_tokenize(doc) for doc in documents] \n",
    "\n",
    "# Remove stop words\n",
    "filtered_docs = [[word for word in doc if word not in stopwords.words('english') and word not in ENGLISH_STOP_WORDS] for doc in tokenized_docs]\n",
    "\n",
    "# Convert back to sentences if needed\n",
    "documents = [\" \".join(doc) for doc in filtered_docs]\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction\n",
    "here we change words to a smaller common form instead of the 's. We do this using two ways:\n",
    "- stemming\n",
    "- lemmatization\n",
    "\n",
    "here we perform lemmatization as in the previous assignment, lemmatization had worked much better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "this also reduces words to their singular form and is said to be better as it only reduces to words in the dictionary. after our 48 trials in the previous assignment, lemmatization was much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_doc(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [lemmatize_doc(doc) for doc in documents]\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams\n",
    "here we decide how we want the words to be tokenized, either they are\n",
    "- unigram: singular\n",
    "- bigram: pairs (doubular)\n",
    "\n",
    "for this assignment, we will focus on unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because we want unigrams, we DO NOT have to do anything and we will leave it the way it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "here we convert the text to numerical format for the model to understand it. we have four ways to do it:\n",
    "- word2Vec\n",
    "- doc2vec\n",
    "\n",
    "in this code we are exploring the first method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "in word2vec we have two methods:\n",
    "- cbow architecture (target is predicted by context)\n",
    "- skipgram (target predicts context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sg variable\n",
    "sg = 0 # CBOW\n",
    "# sg = 1 # Skip-Gram\n",
    "\n",
    "if sg == 0:\n",
    "    model_type = 'CBOW'\n",
    "else:\n",
    "    model_type = 'Skipgram'\n",
    "\n",
    "print(f\"sg={sg}, model={model_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables to set\n",
    "here we set variables like vector_size, window_size, count of epochs etc. we define them here so that in the code we dont have to change them. We can easily change them here and the code will change accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one thing to note here is that before starting the assignment i will make a list of each variable and what i want to test. then i will iterate over each variable keeping rest fixed, and write a loop to iterate over that variable and then i will select that as best. then for the next i will use its list but that previous one will be in its best. this will help me run as many things as i want in one code run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_sizes = [50, 100, 150, 200, 250, 300]\n",
    "selected_vector_size = vector_sizes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = [3, 5, 7, 10, 12, 15, 20]\n",
    "selected_window = windows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [50, 100, 150, 200, 250]\n",
    "selected_epoch = epochs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code declaration\n",
    "here we declare our code using the following syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_word2vec(tokens, model, vector_size=300):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]  \n",
    "    return np.mean(vectors, axis=0) if vectors else np.zeros(vector_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_word2vec (docs, vector_size, window_size, epoch):\n",
    "    word2vec_model = Word2Vec(\n",
    "        sentences=docs,                        # Tokenized text\n",
    "        vector_size=vector_size,           # Each word is represented by a x-dimensional vector\n",
    "        window=window_size,                     # Context window size\n",
    "        min_count=1,                                # Ignores words that appear less than 'min_count' times\n",
    "        workers=4,                                  # Number of CPU cores used\n",
    "        sg=sg,                   \n",
    "        epochs=epoch                       # Number of training iterations\n",
    "    )\n",
    "    # print(word2vec_model)\n",
    "    \n",
    "    # Convert the list to a Pandas Series\n",
    "    documents_series = pd.Series(docs)\n",
    "\n",
    "    docs = documents_series.apply(lambda x: get_avg_word2vec(x, word2vec_model))    \n",
    "    docs = np.array(docs.tolist())\n",
    "    \n",
    "    # print(docs)\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Means Clustering\n",
    "here we cluster the documents into similar categories using k means clustering algorithm. we will be testing the algorithm for three values of k: 5, 9 and 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_k_means(k, data):\n",
    "    print(f\"Displaying {k} start time:\", get_current_datetime())\n",
    "    \n",
    "    svd = TruncatedSVD(n_components=2, random_state=42)\n",
    "    data = svd.fit_transform(data)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=erp)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    \n",
    "    # Convert to DataFrame for visualization\n",
    "    df_viz = pd.DataFrame({'X': data[:, 0], 'Y': data[:, 1], 'Cluster': labels})\n",
    "\n",
    "    # Scatter plot of clusters\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=df_viz, x='X', y='Y', hue='Cluster', palette='tab10', s=100, edgecolor='black')\n",
    "    plt.title(\"K-Means Clustering Visualization (2D Projection)\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.legend(title=\"Cluster\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Finished displaying at:\", get_current_datetime(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_means(k, data, results, vector_size, window, epoch):\n",
    "    print(f\"Executing {k} start time:\", get_current_datetime())\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, random_state=erp)\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    \n",
    "    wss = kmeans.inertia_  # Within-Cluster Sum of Squares\n",
    "    silhouette_avg = silhouette_score(data, labels)  # Silhouette Score\n",
    "    \n",
    "    print(f\"K={k}: Silhouette Score and WSS={silhouette_avg:.4f}\\t{wss:.4f}\")\n",
    "    \n",
    "    results.append([k, f'Word2Vec', vector_size, window, epoch, model_type, silhouette_avg, wss])\n",
    "    \n",
    "    # Create a DataFrame to store headlines with their assigned clusters\n",
    "    df_clusters = pd.DataFrame({'Headline': original_headlines, 'Cluster': labels})\n",
    "\n",
    "    # Display sample headlines per cluster\n",
    "    for cluster in range(k):\n",
    "        print(f\"\\nCluster {cluster}:\")\n",
    "        sample_headlines = df_clusters[df_clusters[\"Cluster\"] == cluster].sample(n=min(5, len(df_clusters[df_clusters[\"Cluster\"] == cluster])), random_state=erp)\n",
    "        for idx, row in sample_headlines.iterrows():\n",
    "            print(f\"- {row['Headline']}\")\n",
    "    \n",
    "    display_k_means(k, data)\n",
    "    \n",
    "    print(\"Finished executing at:\", get_current_datetime(), \"\\n\")\n",
    "    \n",
    "    return silhouette_avg, wss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Running\n",
    "here we run the code in a for-loop, because we have many variables we need to test with k-means. thus we test them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_of_docs = documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = selected_window\n",
    "epoch = selected_epoch\n",
    "# vector_size = selected_vector_size\n",
    "# the variable that is commented means that is the variable being tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a output that will save each case WSS and silhoutee so that it can be saved into a csv file later on\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an output that will save each WSS with its k so that we can plot a scree plot later on\n",
    "wss_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a variable that will save the best combination for reference at the end\n",
    "best_combination = []\n",
    "best_WSS = 100000000000000000000000000 # we want lowest wss so we start with large value\n",
    "best_Sil = 0 # we want max sil so we start with small value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_combinations(k):\n",
    "    sum_wss = 0\n",
    "    count_wss = 0\n",
    "\n",
    "    for vector_size in vector_sizes:\n",
    "        print(\"Executed at:\", get_current_datetime(), \"\\n\")\n",
    "        documents = run_word2vec(copy_of_docs, vector_size, window, epoch)\n",
    "        print(f\"sg={sg}, Vector_Size={vector_size}, Window={window}, Epoch={epoch}\")\n",
    "        print(\"Finished displaying at:\", get_current_datetime(), \"\\n\")\n",
    "        sil, wss = run_k_means(k, documents, results, vector_size, window, epoch)\n",
    "        sum_wss += wss\n",
    "        count_wss += 1\n",
    "        if sil > best_Sil and wss < best_WSS:\n",
    "            best_combination = [k, vector_size, window, epoch, sil, wss]\n",
    "            best_WSS = wss\n",
    "            best_Sil = sil\n",
    "        else:\n",
    "            sil_diff = sil - best_Sil\n",
    "            wss_diff = best_WSS - wss\n",
    "            if sil_diff > 0 and sil_diff < 0.1 and wss_diff > 0:\n",
    "                best_combination = [k, vector_size, window, epoch, sil, wss]\n",
    "                best_WSS = wss\n",
    "                best_Sil = sil\n",
    "\n",
    "    avg_wss = sum_wss / count_wss\n",
    "    wss_scores.append([k, avg_wss])\n",
    "    return best_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_combinations(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_combinations(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_combinations(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easy export\n",
    "results_df = pd.DataFrame(results, columns=['k', 'Vectorizer Type', 'vector_size', 'window', 'Epochs Count', 'Vectorizer Name', 'Silhouette Score', 'WSS Score'])\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(\"clustering_results.csv\", index=False)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easy export\n",
    "wss_scores_df = pd.DataFrame(wss_scores, columns=['k', 'Average WSS Score'])\n",
    "\n",
    "wss_scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the elbow plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(wss_scores_df['k'], wss_scores_df['Average WSS Score'], marker='o', linestyle='-')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Average WSS Score\")\n",
    "plt.title(\"Elbow Method for Optimal k\")\n",
    "plt.xticks(wss_scores_df['k'])  # Ensure all k values are shown on x-axis\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_combination # vector size, window size, epoch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
