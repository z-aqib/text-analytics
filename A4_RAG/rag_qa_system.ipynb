{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11345061,"sourceType":"datasetVersion","datasetId":7098504},{"sourceId":11439657,"sourceType":"datasetVersion","datasetId":7166003},{"sourceId":11439662,"sourceType":"datasetVersion","datasetId":7166008}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Acknowledgement\nName: Hamna Inam, Zara Masood, Zuha Aqib     \nERP ID: X, Y, 26106    \nSection: 10am Miss Solat    \nDate: 16-Apr-25   ","metadata":{"id":"j1D1m381Ebv5"}},{"cell_type":"code","source":"from datetime import datetime\nprint(\"Last time code executed:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T16:36:11.379376Z","iopub.execute_input":"2025-04-19T16:36:11.379981Z","iopub.status.idle":"2025-04-19T16:36:11.390145Z","shell.execute_reply.started":"2025-04-19T16:36:11.379950Z","shell.execute_reply":"2025-04-19T16:36:11.389298Z"},"id":"pBYgXkK-EbwD","outputId":"6369c243-79e2-4398-e457-b4b88e418973","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"name":"stdout","text":"Last time code executed: 2025-04-19 16:36:11\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"def print_date_time():\n    return \"\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T16:36:11.391448Z","iopub.execute_input":"2025-04-19T16:36:11.391720Z","iopub.status.idle":"2025-04-19T16:36:11.423362Z","shell.execute_reply.started":"2025-04-19T16:36:11.391698Z","shell.execute_reply":"2025-04-19T16:36:11.422429Z"},"id":"CXAQ2BwVEbwH"},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# Imports\nhere we add all imports and prerequisities like installations, authentications, constant definitions etc","metadata":{"id":"4d8Us0HHEbwJ"}},{"cell_type":"markdown","source":"## Installations\nfirst we need to install related packages","metadata":{"id":"sm79L72bEbwJ"}},{"cell_type":"code","source":"# # adding this because kaggle ke maslay\n# !pip uninstall -y langchain langchain-core langchain-community langchain-openai ragas pydantic -y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T16:36:11.424691Z","iopub.execute_input":"2025-04-19T16:36:11.424986Z","iopub.status.idle":"2025-04-19T16:36:11.434798Z","shell.execute_reply.started":"2025-04-19T16:36:11.424960Z","shell.execute_reply":"2025-04-19T16:36:11.434032Z"},"id":"1wF3_bZqEbwK"},"outputs":[],"execution_count":3},{"cell_type":"code","source":"get_ipython().system('pip install transformers')\nget_ipython().system('pip install sentence-transformers')\nget_ipython().system('pip install pypdf')\nget_ipython().system('pip install pymupdf')\nget_ipython().system('pip install rank_bm25')\nget_ipython().system('pip install datasets')\nget_ipython().system('pip install matplotlib')\nget_ipython().system('pip install faiss')\nget_ipython().system('pip install faiss-cpu')\nget_ipython().system('pip install faiss-gpu')\nget_ipython().system('pip install --upgrade pypdf')\n\n# 2. Then install\n# !pip install \"langchain==0.2.0\"\n# !pip install \"langchain-core==0.2.0\"\n# !pip install \"langchain-community==0.2.0\"\n# !pip install \"langchain-text-splitters==0.2.1\"\n# !pip install \"langchain-openai==0.1.0\"\n# !pip install \"pydantic==2.6.4\"\n# !pip install \"ragas==0.2.14\"\n\n!pip install langchain\n!pip install langchain-core\n!pip install langchain-community\n!pip install langchain-text-splitters\n!pip install langchain-openai\n!pip install pydantic\n!pip install ragas\n\n!pip install --upgrade numpy\n!pip install --upgrade pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T16:36:11.463540Z","iopub.execute_input":"2025-04-19T16:36:11.464067Z"},"id":"RgO_gHo1EbwL","outputId":"fa4d0f38-c3e4-4d9e-e624-1469d46bbea9","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\nRequirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/211.5 MB\u001b[0m \u001b[31m101.3 MB/s\u001b[0m eta \u001b[36m0:00:02\u001b[0m","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade numpy\n!pip install --upgrade scipy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install numpy==1.26.4 scipy==1.12.0 --force-reinstall","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Imports\nhere we import the necessary libraries and modules","metadata":{"id":"RAKtxVv0EbwN"}},{"cell_type":"code","source":"# ===== Environment & Authentication =====\nimport os\nimport time\nimport csv\n# from dotenv import load_dotenv, dotenv_values\nfrom huggingface_hub import login\n\n# ===== Core Python & Data Handling =====\nfrom typing import List, Tuple, Dict\nimport textwrap\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# ===== NLP Preprocessing =====\nimport nltk\nfrom rank_bm25 import BM25Okapi  # BM25 retriever\nnltk.download('punkt')  # Ensure NLTK data is available\n\n# ===== LangChain - Document Loading & Splitting =====\nfrom langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n# from langchain.schema import Document\nfrom langchain_core.documents import Document\n\n# ===== LangChain - Embeddings & Vector Stores =====\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import FAISS\n\n# ===== Hugging Face Models & Pipelines =====\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    pipeline\n)\nfrom langchain.llms import (\n    HuggingFaceHub,\n    HuggingFacePipeline\n)\n\n# ===== RAG Evaluation (RAGAS) =====\nfrom ragas import evaluate\nfrom ragas.metrics import (\n    Faithfulness,\n    AnswerRelevancy,\n    ContextRecall,\n    ContextPrecision,\n    AnswerCorrectness\n)\nfrom datasets import Dataset","metadata":{"trusted":true,"id":"22QiRs-aEbwO","outputId":"e8156eed-7734-44fd-dd53-13af7ed8951c","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# because there was an error in this import, here it is seperatly\nimport torch\nprint(torch.__version__)","metadata":{"trusted":true,"id":"ITlgZo5AEbwO","outputId":"5fb80a8e-862d-4fd4-9af5-a32ee3ff448d","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from packaging import version\n\n# Check torch version (modern alternative)\ndef is_torch_greater_or_equal_than_1_13():\n    return version.parse(torch.__version__) >= version.parse(\"1.13.0\")\n\nprint(f\"Torch version: {torch.__version__}\")\nprint(f\"Is >=1.13.0: {is_torch_greater_or_equal_than_1_13()}\")","metadata":{"trusted":true,"id":"oJxxudGdEbwP","outputId":"db0e5a54-0c21-49bc-da90-1b8786681a88","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Authentication\nhere we authenticate our LLM with hugging face","metadata":{"id":"KRG7BTmKEbwP"}},{"cell_type":"code","source":"# # Load environment variables from .env\n# load_dotenv()\n\n# # Retrieve the token\n# hf_token = os.getenv(\"HUGGING_FACE_TOKEN\")\n\n# print(\"Token loaded:\", hf_token is not None)\n\n# # Log in to Hugging Face Hub\nlogin(token=\"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\")","metadata":{"trusted":true,"id":"F0yPasmAEbwP"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# config = dotenv_values(\".env\")\n# login(token=config[\"HUGGING_FACE_TOKEN\"])","metadata":{"trusted":true,"id":"rqAqPx9_EbwQ"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Constants\nhere we define constants that we will fine tune","metadata":{"id":"mwpZZH72EbwQ"}},{"cell_type":"code","source":"# Constants\nDEFAULT_CHUNK_SIZE = 1000              # Max size of each text chunk\nDEFAULT_CHUNK_OVERLAP = 200            # Overlap between chunks\nDEFAULT_SEARCH_K = 3                   # Top-k results to retrieve\nDEFAULT_SEARCH_TYPE = \"hybrid\"         # Choose from: 'semantic', 'keyword', or 'hybrid'\nDEFAULT_EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # Embedding model for vector search\n#DEFAULT_LLM_MODEL = \"meta-llama/Llama-3.2-1B\"   #worked!\n# DEFAULT_LLM_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # worked!\n\nDEFAULT_LLM_MODEL = \"microsoft/phi-2\"   #out of memory error\n#DEFAULT_LLM_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\" #out of memory error\n#DEFAULT_LLM_MODEL = \"google/gemma-1.1-2b-it\" #out of memory error\n#DEFAULT_LLM_MODEL = \"tiiuae/falcon-7b-instruct\" #out of memory error\n# LLM for generating answers\n# DEFAULT_LLM_MODEL = \"deepseek-ai/DeepSeek-V3-0324\"                         # LLM for generating answers\n# DEFAULT_DOCUMENT_DIR = \"/data/corpus.zip\"\nDEFAULT_DOCUMENT_DIR = \"/kaggle/input/daa-lectures-for-a4/cmu-lecs\"\n# DEFAULT_DOCUMENT_DIR = \"/content/\"  # Changed to the directory path\n\nDEFAULT_LLM_MODEL","metadata":{"trusted":true,"id":"uvpZHpqdEbwQ"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Document Processing Functions","metadata":{"id":"L1E1l1iNxLEF"}},{"cell_type":"code","source":"def load_documents(directory: str, glob_pattern: str = \"**/*.pdf\") -> List[Document]:\n    \"\"\"\n    Loads all PDF files from a given directory.\n\n    Args:\n        directory: path to folder with PDF files\n        glob_pattern: pattern to match files (default = all PDFs)\n\n    Returns:\n        List of LangChain Document objects\n    \"\"\"\n    loader = DirectoryLoader(directory, glob=glob_pattern, loader_cls=PyPDFLoader)\n    return loader.load()","metadata":{"trusted":true,"id":"HV2X-Xd7EbwQ"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def chunk_documents(\n    documents: List[Document],\n    chunk_size: int = DEFAULT_CHUNK_SIZE,\n    chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,\n    separators: List[str] = None\n) -> List[Document]:\n    \"\"\"\n    Splits documents into chunks for better retrieval.\n\n    Args:\n        documents: list of LangChain documents\n        chunk_size: size of each chunk\n        chunk_overlap: how much content overlaps between chunks\n        separators: optional list of separators for better splitting\n\n    Returns:\n        List of chunked Document objects\n    \"\"\"\n    if separators is None:\n        # Default separators: prioritize splitting on paragraphs, then sentences, then words\n        separators = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        separators=separators\n    )\n    return text_splitter.split_documents(documents)","metadata":{"trusted":true,"id":"k1ua7tohEbwR"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_vector_store(\n    chunks: List[Document],\n    embedding_model: str = DEFAULT_EMBEDDING_MODEL,\n    save_path: str = None\n) -> FAISS:\n    \"\"\"\n    Creates a FAISS vector index from document chunks using specified embedding model.\n\n    Args:\n        chunks: list of Document chunks\n        embedding_model: HuggingFace model used for embeddings\n        save_path: optional path to save the index\n\n    Returns:\n        FAISS vector store\n    \"\"\"\n    embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n    vectordb = FAISS.from_documents(chunks, embeddings)\n\n    if save_path:\n        vectordb.save_local(save_path)\n\n    return vectordb","metadata":{"trusted":true,"id":"IYGOIWotEbwR"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_bm25_index(chunks: List[Document]) -> BM25Okapi:\n    \"\"\"\n    Builds a keyword-based index using BM25.\n\n    Args:\n        chunks: list of Document chunks\n\n    Returns:\n        BM25 index\n    \"\"\"\n    texts = [chunk.page_content for chunk in chunks]                  # Get plain text\n    tokenized_texts = [text.split() for text in texts]               # Tokenize by whitespace\n    return BM25Okapi(tokenized_texts)","metadata":{"trusted":true,"id":"1SBM9zbqEbwR"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Search Functions","metadata":{"id":"p-gPSeFBxLEH"}},{"cell_type":"code","source":"def semantic_search(\n    query: str,\n    vectordb: FAISS,\n    k: int = DEFAULT_SEARCH_K,\n    score_threshold: float = None\n) -> List[Tuple[Document, float]]:\n    \"\"\"\n    Perform semantic search using vector similarity from FAISS.\n\n    Args:\n        query: Natural language question\n        vectordb: Vector index (FAISS)\n        k: Number of results to return\n        score_threshold: Filter out low similarity scores (optional)\n\n    Returns:\n        List of (Document, similarity_score) tuples\n    \"\"\"\n    results = vectordb.similarity_search_with_score(query, k=k)\n\n    # Optional thresholding to remove irrelevant results\n    if score_threshold is not None:\n        results = [(doc, score) for doc, score in results if score >= score_threshold]\n\n    return results","metadata":{"trusted":true,"id":"fE1SDy_nEbwR"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def keyword_search(\n    query: str,\n    bm25_index: BM25Okapi,\n    chunks: List[Document],\n    k: int = DEFAULT_SEARCH_K,\n    score_threshold: float = None\n) -> List[Tuple[Document, float]]:\n    \"\"\"\n    Perform lexical search using BM25.\n\n    Args:\n        query: Search query string\n        bm25_index: Pre-built BM25 index\n        chunks: List of document chunks for mapping back\n        k: Top-k documents to retrieve\n        score_threshold: Optional filtering threshold for BM25 scores\n\n    Returns:\n        List of (Document, BM25_score) tuples\n    \"\"\"\n    tokenized_query = query.split()  # Basic whitespace tokenization\n    scores = bm25_index.get_scores(tokenized_query)\n\n    # Get indices of top-k documents\n    top_k_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n\n    results = [(chunks[i], scores[i]) for i in top_k_indices]\n\n    if score_threshold is not None:\n        results = [(doc, score) for doc, score in results if score >= score_threshold]\n\n    return results","metadata":{"trusted":true,"id":"Nhfc2td3EbwS"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def hybrid_search(\n    query: str,\n    vectordb: FAISS,\n    bm25_index: BM25Okapi,\n    chunks: List[Document],\n    k: int = DEFAULT_SEARCH_K,\n    semantic_weight: float = 0.5,\n    keyword_weight: float = 0.5\n) -> List[Tuple[Document, float]]:\n    \"\"\"\n    Combine semantic and keyword search using weighted score fusion.\n\n    Args:\n        query: Natural language query\n        vectordb: FAISS vector database\n        bm25_index: BM25 keyword index\n        chunks: Document chunks (used for mapping back)\n        k: Top-k results to return\n        semantic_weight: Weight for vector similarity\n        keyword_weight: Weight for BM25 relevance\n\n    Returns:\n        List of (Document, combined_score) tuples\n    \"\"\"\n    # Step 1: Run both types of searches with larger k (to capture wider context)\n    semantic_results = semantic_search(query, vectordb, k * 2)\n    semantic_scores = {doc.page_content: score for doc, score in semantic_results}\n\n    keyword_results = keyword_search(query, bm25_index, chunks, k * 2)\n    keyword_scores = {doc.page_content: score for doc, score in keyword_results}\n\n    # Step 2: Normalize BM25 scores (they are not bounded, unlike cosine similarity)\n    max_kw_score = max(keyword_scores.values()) if keyword_scores else 1\n\n    # Step 3: Combine results\n    all_docs = set(semantic_scores.keys()).union(set(keyword_scores.keys()))\n    combined_scores = []\n\n    for doc_content in all_docs:\n        sem_score = semantic_scores.get(doc_content, 0)\n        kw_score = keyword_scores.get(doc_content, 0)\n        norm_kw_score = kw_score / max_kw_score if max_kw_score > 0 else 0\n\n        # Weighted sum of both types of scores\n        combined_score = (semantic_weight * sem_score) + (keyword_weight * norm_kw_score)\n        combined_scores.append((doc_content, combined_score))\n\n    # Step 4: Sort and return top-k\n    combined_scores.sort(key=lambda x: x[1], reverse=True)\n    top_scores = combined_scores[:k]\n\n    # Step 5: Re-map back to full Document objects using content\n    doc_lookup = {chunk.page_content: chunk for chunk in chunks}\n    results = []\n\n    for doc_content, score in top_scores:\n        if doc_content in doc_lookup:\n            results.append((doc_lookup[doc_content], score))\n\n    return results","metadata":{"id":"J3T0XMX0xLEH","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# LLM functions","metadata":{"id":"7Q5NGOdUxLEI"}},{"cell_type":"code","source":"def initialize_llm(\n    model_name: str = DEFAULT_LLM_MODEL,\n    device: str = \"cuda\",  # Use \"cpu\" if not using GPU\n    max_new_tokens: int = 300\n) -> Tuple[pipeline, any]:\n    \"\"\"\n    Loads a language model pipeline for text generation.\n\n    Args:\n        model_name: HuggingFace model repo (must support causal LM)\n        device: \"cuda\" for GPU or \"cpu\"\n        max_new_tokens: Max tokens to generate per response\n\n    Returns:\n        Tuple (generator pipeline, tokenizer)\n    \"\"\"\n    try:\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            device_map=device,\n            torch_dtype=\"auto\",            # Uses GPU acceleration if available\n            trust_remote_code=True         # Allow custom model architectures\n        )\n        print(\"Original used\")\n    except ImportError:\n        # Fallback without device_map\n        model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=\"auto\",\n            trust_remote_code=True\n        ).to(device)\n        print(\"Edited used\")\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    # Format the prompt as a dialogue (user + assistant style)\n    tokenizer.chat_template = (\n        \"{% for message in messages %}\"\n        \"{% if message['role'] == 'user' %}User: {{ message['content'] }}\\n\"\n        \"{% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }}\\n\"\n        \"{% endif %}\"\n        \"{% endfor %}\"\n        \"{% if add_generation_prompt %}Assistant:{% endif %}\"\n    )\n\n    # Create a text-generation pipeline\n    generator = pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        return_full_text=False,        # Only return generated part, not the full prompt\n        max_new_tokens=max_new_tokens,\n        do_sample=True                 # Use sampling (stochastic generation)\n    )\n\n    return generator, tokenizer","metadata":{"trusted":true,"id":"v89jLCD_EbwS"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_response(\n    prompt: str,\n    generator: pipeline,\n    width: int = 80  # For pretty-printing long outputs\n) -> str:\n    \"\"\"\n    Generates a response from the LLM using the prompt.\n\n    Args:\n        prompt: Full RAG-formatted prompt with question + context\n        generator: HF pipeline object\n        width: max characters per printed line (for wrapping)\n\n    Returns:\n        Answer string\n    \"\"\"\n    messages = [{\"role\": \"user\", \"content\": prompt}]        # Wrap prompt in chat message format\n    output = generator(\n        messages,\n        max_new_tokens=256,\n        temperature=0.7,\n        top_p=0.9,\n        do_sample=True\n    )\n                            # Call LLM\n    return textwrap.fill(output[0][\"generated_text\"], width=width)","metadata":{"trusted":true,"id":"kQMhtE8lEbwT"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def format_rag_prompt(\n    question: str,\n    retrieved_docs: List[Document],\n    instruction: str = None\n) -> str:\n    \"\"\"\n    Formats the final input prompt to send to the LLM.\n\n    Args:\n        question: The user's natural language question\n        retrieved_docs: List of document chunks retrieved by search\n        instruction: Optional prompt instructions (system message)\n\n    Returns:\n        Full prompt text string\n    \"\"\"\n    # Default instructions to guide the LLM on how to use retrieved documents\n    if instruction is None:\n        instruction = \"\"\"You are an AI assistant tasked with answering questions based on retrieved knowledge.\n                    - Integrate the key points from all retrieved responses into a cohesive, well-structured answer.\n                    - If the responses are contradictory, mention the different perspectives.\n                    - If none of the retrieved responses contain relevant information, reply:\n                    \"I couldn't find a good response to your query in the database.\"\n                    \"\"\"\n\n    # Truncate each document to 1000 characters if long\n    retrieved_info = \"\\n\\n\".join(\n        f\"{i+1}️⃣ {doc.page_content[:1000]}...\" if len(doc.page_content) > 1000\n        else f\"{i+1}️⃣ {doc.page_content}\"\n        for i, doc in enumerate(retrieved_docs)\n    )\n\n    # Final structured prompt\n    return f\"\"\"\n        {instruction}\n\n        ### Retrieved Information:\n        {retrieved_info}\n\n        ### Question:\n        {question}\n    \"\"\"","metadata":{"id":"9GJIwleTxLEJ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RAG Evaluator","metadata":{"id":"07jDDqiSxLEJ"}},{"cell_type":"code","source":"# WRAPPER TO USE LANGCHAIN LLM IN RAGAS\n\nclass HuggingFaceLLMWrapper:\n    \"\"\"\n    Wrapper for using HuggingFacePipeline with RAGAS evaluation.\n    Includes a dummy 'set_run_config' to avoid errors.\n    \"\"\"\n\n    def __init__(self, pipeline):\n        self.pipeline = pipeline\n\n    def __call__(self, prompt: str) -> str:\n        return self.pipeline(prompt)\n\n    def set_run_config(self, run_config):\n        \"\"\"Dummy method to avoid errors with ragas.\"\"\"\n        pass  # Do nothing, as TextGenerationPipeline doesn't have this method","metadata":{"id":"86nm904ENFqi","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# WRAPPER TO USE LANGCHAIN LLM IN RAGAS\n\nclass HuggingFaceLLM:\n    \"\"\"\n    Simple wrapper for using HuggingFaceHub with RAGAS evaluation.\n    \"\"\"\n    def __init__(self, model_name: str):\n        self.model = HuggingFaceHub(repo_id=model_name)\n\n    def __call__(self, prompt: str) -> str:\n        return self.model(prompt)","metadata":{"trusted":true,"id":"eKsmE4PVEbwU"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# EVALUATOR CLASS\n\nclass RAGEvaluator:\n    \"\"\"\n    Performs automatic evaluation of RAG responses using Ragas metrics.\n    Also supports result visualization and optimization insights.\n    \"\"\"\n    def __init__(self, pipeline, llm, embeddings):\n        self.pipeline = pipeline         # RAGPipeline object\n        self.embeddings = embeddings     # HuggingFaceEmbeddings instance\n\n        # Use passed LLM, or initialize default\n        if isinstance(llm, str):\n            self.llm = HuggingFaceHub(repo_id=llm)\n        else:\n            self.llm = HuggingFaceLLMWrapper(llm) if llm else HuggingFaceLLMWrapper(HuggingFaceHub(repo_id=DEFAULT_LLM_MODEL))\n\n        # Internal result tracking\n        self.results = []\n\n    def evaluate_ragas(self, questions: list, gold_answers: list = None):\n        \"\"\"\n        Run Ragas evaluation across all questions.\n\n        Args:\n            questions: List of input questions\n            gold_answers: Reference answers (optional)\n\n        Returns:\n            DataFrame of results\n        \"\"\"\n        all_rows = []\n\n        for question, gold_answer in zip(questions, gold_answers or [None]*len(questions)):\n            # Run full RAG query\n            answer = self.pipeline.query(question)\n\n            # Get context used in the answer\n            contexts = [doc.page_content for doc in self.pipeline.get_last_retrieved_docs()]\n\n            # Prepare a single sample for evaluation\n            data = {\n                \"question\": [question],\n                \"answer\": [answer],\n                \"contexts\": [contexts]\n            }\n            if gold_answer:\n                data[\"ground_truth\"] = [gold_answer]\n\n            dataset = Dataset.from_dict(data)\n\n            # Select metrics to compute\n            metrics = [Faithfulness(), AnswerRelevancy(), ContextRecall(), ContextPrecision()]\n            if gold_answer:\n                metrics.append(AnswerCorrectness())\n\n            # Run the evaluation\n            result = evaluate(dataset, metrics=metrics, llm=self.llm, embeddings=self.embeddings)\n\n            # Convert to DataFrame and store\n            row = result.to_pandas()\n            row[\"question\"] = question\n            row[\"retrieved_docs\"] = len(contexts)\n            all_rows.append(row)\n\n        # Combine all rows into one DataFrame\n        self.results = pd.concat(all_rows, ignore_index=True)\n        return self.results\n\n    def visualize_metrics(self):\n        \"\"\"\n        Visualize average metric scores and context retrieval stats.\n        \"\"\"\n        if self.results is None or self.results.empty:\n            raise ValueError(\"No evaluation results found. Run evaluate_ragas() first.\")\n\n        # Plot main metrics\n        metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']\n        if 'answer_correctness' in self.results.columns:\n            metrics.append('answer_correctness')\n\n        plt.figure(figsize=(10, 5))\n        self.results[metrics].mean().plot(kind='bar', color='lightblue')\n        plt.title(\"🔍 Average RAG Evaluation Metrics\")\n        plt.ylabel(\"Score (0 to 1)\")\n        plt.xticks(rotation=45)\n        plt.grid(axis='y', linestyle='--', alpha=0.5)\n        plt.show()\n\n        # Plot document retrieval counts\n        plt.figure(figsize=(8, 4))\n        self.results['retrieved_docs'].value_counts().sort_index().plot(kind='bar', color='lightgreen')\n        plt.title(\"📄 Number of Context Chunks Retrieved Per Query\")\n        plt.xlabel(\"Number of Chunks\")\n        plt.ylabel(\"Frequency\")\n        plt.grid(axis='y', linestyle='--', alpha=0.5)\n        plt.show()\n\n    def get_optimization_insights(self):\n        \"\"\"\n        Analyze weak metrics and recommend strategies to improve RAG performance.\n        \"\"\"\n        if self.results is None or self.results.empty:\n            return \"No evaluation results available.\"\n\n        insights = []\n        df = self.results\n\n        # Faithfulness issues (hallucination)\n        if df['faithfulness'].mean() < 0.7:\n            insights.append(\"⚠️ Faithfulness is low — possible hallucinations.\")\n            insights.append(\"🔧 Try increasing chunk overlap or improving retrieval relevance.\")\n\n        # Context recall issues (missing info)\n        if df['context_recall'].mean() < 0.6:\n            insights.append(\"⚠️ Low context recall — relevant info may be missed.\")\n            insights.append(\"🔧 Consider using hybrid retrieval or adjusting chunk size.\")\n\n        # Precision issues (irrelevant info)\n        if df['context_precision'].mean() < 0.6:\n            insights.append(\"⚠️ Low context precision — too much irrelevant context.\")\n            insights.append(\"🔧 Use better embeddings or rerank retrieved chunks.\")\n\n        # Relevance issues (answer not matching question)\n        if df['answer_relevancy'].mean() < 0.7:\n            insights.append(\"⚠️ Low answer relevancy — answers not matching question.\")\n            insights.append(\"🔧 Refine your prompts or improve chunk matching.\")\n\n        # Optional: Correctness based on gold answers\n        if 'answer_correctness' in df.columns and df['answer_correctness'].mean() < 0.7:\n            insights.append(\"⚠️ Low correctness — answers differ from references.\")\n            insights.append(\"🔧 Try different LLMs or use post-editing.\")\n\n        return \"\\n\".join(insights)","metadata":{"id":"DAFXSDHQxLEK","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# RAG Pipeline","metadata":{"id":"r1Lfz6KJxLEL"}},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, util\nimport numpy as np\n\nclass SimpleEvaluator:\n    \"\"\"\n    Evaluates the quality of generated answers by comparing the original question\n    with a regenerated question from the generated answer.\n    \"\"\"\n    def __init__(self, llm, tokenizer, embedding_model: str = DEFAULT_EMBEDDING_MODEL):\n        self.llm = llm\n        self.tokenizer = tokenizer\n        self.embedding_model = SentenceTransformer(embedding_model)\n\n    def generate_question(self, answer: str, max_chars: int = 200) -> str:\n        \"\"\"\n        Generates a single-sentence question from a given answer.\n    \n        Args:\n            answer: The answer text to reverse-generate a question for.\n            max_chars: Maximum character length of the output question.\n    \n        Returns:\n            A short, single question string.\n        \"\"\"\n        prompt = (\n            \"You are a question generation AI.\\n\"\n            \"Generate exactly one concise, clear question (no explanations) that can be answered using the following passage.\\n\\n\"\n            f\"PASSAGE:\\n{answer}\\n\\n\"\n            \"QUESTION (one line only):\"\n        )\n    \n        messages = [{\"role\": \"user\", \"content\": prompt}]\n        output = self.llm(\n            messages,\n            max_new_tokens=64,          # prevent long rambly generations\n            temperature=0.7,\n            do_sample=True,\n            top_p=0.9\n        )\n    \n        # Get the first sentence and truncate if necessary\n        full_output = output[0][\"generated_text\"].strip()\n        single_line = full_output.split(\"\\n\")[0].strip()  # grab first line\n        trimmed = single_line[:max_chars].strip()\n    \n        # Ensure it ends with a question mark\n        if not trimmed.endswith(\"?\"):\n            trimmed += \"?\"\n    \n        return trimmed\n\n    def compute_faithfulness(self, generated_answer: str, retrieved_docs: list) -> float:\n        \"\"\"\n        Computes faithfulness score between answer and retrieved docs.\n        \"\"\"\n        combined_context = \" \".join([doc.page_content for doc in retrieved_docs])\n        emb_ans = self.embedding_model.encode(generated_answer, convert_to_tensor=True)\n        emb_ctx = self.embedding_model.encode(combined_context, convert_to_tensor=True)\n        return float(util.pytorch_cos_sim(emb_ans, emb_ctx).item())\n\n    def compute_similarity(self, q1: str, q2: str) -> float:\n        emb1 = self.embedding_model.encode(q1, convert_to_tensor=True)\n        emb2 = self.embedding_model.encode(q2, convert_to_tensor=True)\n        return float(util.pytorch_cos_sim(emb1, emb2).item())\n\n    def evaluate(self, question: str, generated_answer: str, retrieved_docs: list) -> dict:\n        regenerated_q = self.generate_question(generated_answer)\n        q_similarity = self.compute_similarity(question, regenerated_q)\n        faith_score = self.compute_faithfulness(generated_answer, retrieved_docs)\n        return {\n            \"original_question\": question,\n            \"regenerated_question\": regenerated_q,\n            \"question_similarity\": round(q_similarity, 4),\n            \"faithfulness_score\": round(faith_score, 4)\n        }\n","metadata":{"id":"70ftZAKCSjyr","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# MAIN RAG PIPELINE\n\nclass RAGPipeline:\n    \"\"\"\n    This is the central class that handles:\n    - Loading and chunking documents\n    - Initializing vector and keyword search\n    - Running queries\n    - Generating responses from the LLM\n    - Running full experiment sweeps\n    \"\"\"\n    def __init__(\n        self,\n        document_dir: str,\n        embedding_model: str = DEFAULT_EMBEDDING_MODEL,\n        llm_model: str = DEFAULT_LLM_MODEL,\n        chunk_size: int = DEFAULT_CHUNK_SIZE,\n        chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,\n        device: str = \"cuda\"\n    ):\n        self.document_dir = document_dir\n        self.embedding_model = embedding_model\n        self.llm_model = llm_model\n        self.chunk_size = chunk_size\n        self.chunk_overlap = chunk_overlap\n        self.device = device\n\n        # To store runtime state\n        self.documents = None\n        self.chunks = None\n        self.vectordb = None\n        self.bm25_index = None\n        self.llm = None\n        self.tokenizer = None\n        self.last_retrieved_docs = None  # For evaluation traceability\n\n\n    # ==========================\n    # LOAD & CHUNK DOCUMENTS\n    # ==========================\n    def load_and_process_documents(self):\n        \"\"\"\n        Loads PDF documents and splits them into overlapping chunks.\n        \"\"\"\n        print(\"📄 Loading documents...\")\n        self.documents = load_documents(self.document_dir)\n        print(f\"✅ Loaded {len(self.documents)} document pages.\")\n\n        print(\"🪓 Chunking documents...\")\n        self.chunks = chunk_documents(\n            self.documents,\n            chunk_size=self.chunk_size,\n            chunk_overlap=self.chunk_overlap\n        )\n        print(f\"✅ Created {len(self.chunks)} chunks.\")\n\n        # Add unique IDs to chunks for tracking\n        for i, chunk in enumerate(self.chunks):\n            chunk.metadata[\"chunk_id\"] = i\n\n\n    # ==========================\n    # INITIALIZE RETRIEVAL\n    # ==========================\n    def initialize_retrieval(self):\n        \"\"\"\n        Builds vector store and keyword index for retrieval.\n        \"\"\"\n        if not self.chunks:\n            raise ValueError(\"❌ No chunks found. Run load_and_process_documents() first.\")\n\n        print(\"📦 Creating vector store...\")\n        self.vectordb = create_vector_store(self.chunks, self.embedding_model)\n\n        print(\"🔎 Creating BM25 index...\")\n        self.bm25_index = create_bm25_index(self.chunks)\n\n\n    # ==========================\n    # INITIALIZE LLM\n    # ==========================\n    def initialize_llm(self):\n        \"\"\"\n        Loads the chosen LLM and tokenizer from HuggingFace.\n        \"\"\"\n        print(\"🤖 Loading LLM...\")\n        self.llm, self.tokenizer = initialize_llm(self.llm_model, self.device)\n\n\n    # ==========================\n    # GET LAST RETRIEVED CHUNKS\n    # ==========================\n    def get_last_retrieved_docs(self):\n        \"\"\"\n        Returns the last set of retrieved document chunks (used in evaluation).\n        \"\"\"\n        if self.last_retrieved_docs is None:\n            raise ValueError(\"❌ No retrievals done yet.\")\n        return self.last_retrieved_docs\n\n\n    # ==========================\n    # MAIN QUERY FUNCTION\n    # ==========================\n    def query(\n        self,\n        question: str,\n        search_type: str = DEFAULT_SEARCH_TYPE,  # semantic / keyword / hybrid\n        k: int = DEFAULT_SEARCH_K,\n        semantic_weight: float = 0.5,\n        keyword_weight: float = 0.5,\n        custom_instruction: str = None\n    ) -> str:\n        \"\"\"\n        Executes a full query through the pipeline:\n        - Retrieves chunks\n        - Formats prompt\n        - Calls LLM\n        - Returns answer\n        \"\"\"\n        if not self.vectordb or not self.bm25_index:\n            raise ValueError(\"❌ Retrieval systems not ready. Run initialize_retrieval().\")\n        if not self.llm:\n            raise ValueError(\"❌ LLM not initialized. Run initialize_llm().\")\n\n        # Step 1: Retrieve relevant documents\n        if search_type == \"semantic\":\n            results = semantic_search(question, self.vectordb, k)\n        elif search_type == \"keyword\":\n            results = keyword_search(question, self.bm25_index, self.chunks, k)\n        elif search_type == \"hybrid\":\n            results = hybrid_search(\n                question,\n                self.vectordb,\n                self.bm25_index,\n                self.chunks,\n                k,\n                semantic_weight,\n                keyword_weight\n            )\n        else:\n            raise ValueError(f\"❌ Unknown search type: {search_type}\")\n\n        retrieved_docs = [doc for doc, _ in results]\n        self.last_retrieved_docs = retrieved_docs\n\n        # Step 2: Format prompt\n        prompt = format_rag_prompt(question, retrieved_docs, custom_instruction)\n\n        # Step 3: Generate LLM response\n        return generate_response(prompt, self.llm)\n\n\n    # ==========================\n    # EXPERIMENTATION FUNCTION\n    # ==========================\n    def experiment(\n        self,\n        questions: List[str],\n        gold_answers: List[str],\n        chunk_sizes: List[int],\n        k_values: List[int],\n        search_types: List[str],\n        chunk_overlaps: List[int] = [0, 100, 200]\n    ) -> Dict:\n        \"\"\"\n        Run multiple experiment configurations (varying chunk size, k, search type).\n\n        Args:\n            questions: list of input queries\n            gold_answers: reference answers (used in evaluation)\n            chunk_sizes: different chunk sizes to test\n            k_values: number of results to retrieve\n            search_types: list of retrieval modes\n            chunk_overlaps: amount of content overlap between chunks\n\n        Returns:\n            A dict of results by config\n        \"\"\"\n        results = {}\n\n        for chunk_size in chunk_sizes:\n            for chunk_overlap in chunk_overlaps:\n                self.chunk_size = chunk_size\n                self.chunk_overlap = chunk_overlap\n\n                print(f\"\\n⚙️  Testing: chunk={chunk_size}, overlap={chunk_overlap}\")\n                self.load_and_process_documents()\n                self.initialize_retrieval()\n\n                evaluator = RAGEvaluator(self, self.llm, HuggingFaceEmbeddings(model_name=self.embedding_model))\n\n                for search_type in search_types:\n                    for k in k_values:\n                        config_name = f\"chunk{chunk_size}_overlap{chunk_overlap}_{search_type}_k{k}\"\n                        results[config_name] = {}\n\n                        for question in questions:\n                            try:\n                                answer = self.query(\n                                    question,\n                                    search_type=search_type,\n                                    k=k\n                                )\n                                results[config_name][question] = answer\n\n                                # Run evaluation (with gold answer)\n                                eval_result = evaluator.evaluate_ragas([question], [gold_answers[0]])\n                                print(eval_result[['faithfulness', 'answer_relevancy']])\n\n                                # Visualization + optimization\n                                evaluator.visualize_metrics()\n                                print(\"\\n🧠 Optimization Suggestions:\")\n                                print(evaluator.get_optimization_insights())\n\n                            except Exception as e:\n                                results[config_name][question] = f\"❌ Error: {str(e)}\"\n\n        return results\n\n\n    def grid_search(\n        self,\n        questions: List[str],\n        gold_answers: List[str],\n        chunk_sizes: List[int],\n        k_values: List[int],\n        search_types: List[str],\n        chunk_overlaps: List[int] = [0, 100, 200],\n        semantic_weight: float = 0.5,\n        keyword_weight: float = 0.5,\n        output_csv_path: str = \"rag_grid_search_results.csv\"\n    ) -> Dict:\n        \"\"\"\n        Run grid search over multiple config combinations and log results to CSV.\n\n        Returns:\n            Dictionary of results.\n        \"\"\"\n        results = []\n        header_written = False\n\n        # Ensure output directory exists\n        os.makedirs(os.path.dirname(output_csv_path) or \".\", exist_ok=True)\n\n        with open(output_csv_path, mode='w', newline='', encoding='utf-8') as file:\n            writer = csv.writer(file)\n\n            for chunk_size in chunk_sizes:\n                for chunk_overlap in chunk_overlaps:\n                    if chunk_overlap >= chunk_size:\n                        continue\n                    self.chunk_size = chunk_size\n                    self.chunk_overlap = chunk_overlap\n\n                    self.load_and_process_documents()\n                    self.initialize_retrieval()\n\n                    evaluator = SimpleEvaluator(self.llm, self.tokenizer)\n\n                    for search_type in search_types:\n                        for k in k_values:\n                            config_name = f\"chunk{chunk_size}_overlap{chunk_overlap}_{search_type}_k{k}\"\n                            print(f\"\\n🔬 Running Config: {config_name}\")\n\n                            for i, question in enumerate(questions):\n                                try:\n                                    start_time = time.time()\n\n                                    # Step 1: Query\n                                    retrieval_start = time.time()\n                                    answer = self.query(\n                                        question=question,\n                                        search_type=search_type,\n                                        k=k,\n                                        semantic_weight=semantic_weight,\n                                        keyword_weight=keyword_weight\n                                    )\n                                    retrieval_end = time.time()\n\n                                    # Step 2: Evaluation\n                                    eval_df = evaluator.evaluate([question], [answer], self.last_retrieved_docs)\n                                    # eval_row = eval_df.iloc[0].to_dict()\n\n                                    # Step 3: Timing\n                                    end_time = time.time()\n                                    total_time = end_time - start_time\n                                    retrieval_time = retrieval_end - retrieval_start\n                                    generation_time = total_time - retrieval_time\n\n                                    print(eval_df)\n\n                                    # # Visualization + optimization\n                                    # evaluator.visualize_metrics()\n                                    # print(\"\\n🧠 Optimization Suggestions:\")\n                                    # print(evaluator.get_optimization_insights())\n\n                                    # Step 4: Build result row\n                                    row = {\n                                        \"time run\":  print_date_time(),\n                                        \"config\": config_name,\n                                        \"question\": question,\n                                        \"answer\": answer,\n                                        \"retrieved_documents\": self.last_retrieved_docs,\n                                        \"chunk_size\": chunk_size,\n                                        \"chunk_overlap\": chunk_overlap,\n                                        \"search_type\": search_type,\n                                        \"top_k\": k,\n                                        \"semantic_weight\": semantic_weight,\n                                        \"keyword_weight\": keyword_weight,\n                                        \"LLM\": self.llm_model,\n                                        \"embedding_model\": self.embedding_model,\n                                        \"retrieval_time\": round(retrieval_time, 4),\n                                        \"generation_time\": round(generation_time, 4),\n                                        \"total_time\": round(total_time, 4),\n                                        **eval_df\n                                    }\n\n                                    # Write CSV header if needed\n                                    if not header_written:\n                                        writer.writerow(row.keys())\n                                        header_written = True\n\n                                    writer.writerow(row.values())\n                                    results.append(row)\n\n                                except Exception as e:\n                                    print(f\"⚠️ Error in config {config_name} for question '{question}': {e}\")\n                                    writer.writerow([config_name, question, f\"Error: {str(e)}\"])\n\n        print(f\"\\n✅ All results logged to: {output_csv_path}\")\n        return results","metadata":{"id":"ebUXkzJ5xLEL","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Running\nhere we now run the code","metadata":{"id":"ieAPdGVSEbwW"}},{"cell_type":"code","source":"# STEP 1: CREATE EMBEDDINGS & LLM WRAPPER\n\n# Use the same model you defined as DEFAULT_EMBEDDING_MODEL\nembeddings = HuggingFaceEmbeddings(model_name=DEFAULT_EMBEDDING_MODEL)\n\n# Initialize LLM pipeline\ngenerator, tokenizer = initialize_llm(\n    model_name=DEFAULT_LLM_MODEL,     # <-- Use the same LLM constant from above\n    device=\"cuda\",                    # \"cuda\" or \"cpu\"\n    max_new_tokens=300                # <-- You can increase this if responses are too short\n)\n\n# Wrap your generator for LangChain compatibility\nlocal_llm = HuggingFacePipeline(pipeline=generator)","metadata":{"trusted":true,"id":"C97jGDKVEbwX","outputId":"35c9a6e3-abe3-4a5d-e39d-040b6cb38b0b","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# STEP 2: INITIALIZE RAG PIPELINE\n\nrag = RAGPipeline(\n    document_dir=DEFAULT_DOCUMENT_DIR,       # <-- uses constant\n    embedding_model=DEFAULT_EMBEDDING_MODEL,\n    llm_model=DEFAULT_LLM_MODEL,\n    chunk_size=DEFAULT_CHUNK_SIZE,\n    chunk_overlap=DEFAULT_CHUNK_OVERLAP,\n    device=\"cuda\"\n)\nrag","metadata":{"id":"oZT87eroxLEM","trusted":true,"outputId":"3d0fba0d-c829-417d-fc96-0bed0f599dde","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# STEP 3: LOAD DOCUMENTS & PREPARE SYSTEM\n\nrag.load_and_process_documents()\nrag.initialize_retrieval()\nrag.initialize_llm()  # This will use Llama model defined above","metadata":{"trusted":true,"id":"Z6PZiqMbEbwe","outputId":"b44a957e-47d9-4b0c-e07b-dddf3a9e64d5","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# STEP 4: ASK A SINGLE QUESTION\n\nquestions = [\n    \"What is Dynamic Programming?\"\n    # \"Explain the matrix method in hashing\",\n    # \"What are the key concepts in amortized analysis?\"\n]\n\n# Run query with hybrid search and show result\nanswer = rag.query(\n    questions[0],\n    search_type=DEFAULT_SEARCH_TYPE,\n    k=DEFAULT_SEARCH_K\n)\nprint(f\"\\n🧠 Answer:\\n{answer}\")","metadata":{"trusted":true,"id":"vUgUZ4ArEbwf","outputId":"d3370362-203e-4948-a12d-36bb144135b4","colab":{"base_uri":"https://localhost:8080/","height":517}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# STEP 5: PROVIDE GROUND TRUTH FOR EVALUATION (OPTIONAL)\n\ngold_answers = [\n    \"Dynamic Programming is a technique for solving problems by breaking them into overlapping subproblems, storing intermediate results, and combining them to solve the larger problem efficiently.\"\n    # \"Dynamic Programming is a powerful technique that can be used to solve many combinatorial problems in polynomial time for which a naive approach would take exponential time. Dynamic Programming is a general approach to solving problems, much like “divide-and-conquer”, except that the subproblems will overlap.\"\n]","metadata":{"trusted":true,"id":"aaNOBSlnEbwf"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# STEP 6: RUN AUTOMATIC EVALUATION\n\nevaluator = RAGEvaluator(rag, llm=local_llm, embeddings=embeddings)\nresults_df = evaluator.evaluate_ragas(questions, gold_answers)\nprint(\"\\n📊 Evaluation Results:\\n\", results_df)","metadata":{"trusted":true,"id":"fr32Ich5Ebwf"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize performance\nevaluator.visualize_metrics()","metadata":{"trusted":true,"id":"25CFCtoEEbwf"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Show optimization tips\nprint(\"\\n🛠️ Suggestions to Improve RAG System:\")\nprint(evaluator.get_optimization_insights())","metadata":{"trusted":true,"id":"0tc1zvFKEbwf"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"experiment_results = rag.grid_search(\n    questions=questions,\n    gold_answers=gold_answers,\n    chunk_sizes=[100, 250, 500, 800, 1000],\n    k_values=[3, 4, 5],\n    search_types=[\"semantic\", \"hybrid\", \"keyword\"],\n    chunk_overlaps=[100, 200],\n    # output_csv_path=\"/content/rag_grid_log.csv\"\n    output_csv_path=\"/kaggle/working/rag_grid_log.csv\"\n)","metadata":{"id":"7_CDByMKxLEO","trusted":true},"outputs":[],"execution_count":null}]}