{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 11345061,
          "sourceType": "datasetVersion",
          "datasetId": 7098504
        },
        {
          "sourceId": 11439657,
          "sourceType": "datasetVersion",
          "datasetId": 7166003
        },
        {
          "sourceId": 11439662,
          "sourceType": "datasetVersion",
          "datasetId": 7166008
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Acknowledgement\n",
        "Name: Hamna Inam, Zara Masood, Zuha Aqib     \n",
        "ERP ID: X, Y, 26106    \n",
        "Section: 10am Miss Solat    \n",
        "Date: 16-Apr-25   "
      ],
      "metadata": {
        "id": "j1D1m381Ebv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "print(\"Last time code executed:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-18T17:53:20.035311Z",
          "iopub.execute_input": "2025-04-18T17:53:20.035596Z",
          "iopub.status.idle": "2025-04-18T17:53:20.043438Z",
          "shell.execute_reply.started": "2025-04-18T17:53:20.035566Z",
          "shell.execute_reply": "2025-04-18T17:53:20.042574Z"
        },
        "id": "pBYgXkK-EbwD",
        "outputId": "6369c243-79e2-4398-e457-b4b88e418973",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last time code executed: 2025-04-18 19:11:07\n"
          ]
        }
      ],
      "execution_count": 144
    },
    {
      "cell_type": "code",
      "source": [
        "def print_date_time():\n",
        "    return \"\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-18T17:53:20.044281Z",
          "iopub.execute_input": "2025-04-18T17:53:20.044532Z",
          "iopub.status.idle": "2025-04-18T17:53:20.059792Z",
          "shell.execute_reply.started": "2025-04-18T17:53:20.044507Z",
          "shell.execute_reply": "2025-04-18T17:53:20.059250Z"
        },
        "id": "CXAQ2BwVEbwH"
      },
      "outputs": [],
      "execution_count": 145
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n",
        "here we add all imports and prerequisities like installations, authentications, constant definitions etc"
      ],
      "metadata": {
        "id": "4d8Us0HHEbwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations\n",
        "first we need to install related packages"
      ],
      "metadata": {
        "id": "sm79L72bEbwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # adding this because kaggle ke maslay\n",
        "# !pip uninstall -y langchain langchain-core langchain-community langchain-openai ragas pydantic -y"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-18T17:53:20.061393Z",
          "iopub.execute_input": "2025-04-18T17:53:20.061583Z",
          "iopub.status.idle": "2025-04-18T17:53:27.621985Z",
          "shell.execute_reply.started": "2025-04-18T17:53:20.061567Z",
          "shell.execute_reply": "2025-04-18T17:53:27.621276Z"
        },
        "id": "1wF3_bZqEbwK"
      },
      "outputs": [],
      "execution_count": 146
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system('pip install transformers')\n",
        "get_ipython().system('pip install sentence-transformers')\n",
        "get_ipython().system('pip install pypdf')\n",
        "get_ipython().system('pip install pymupdf')\n",
        "get_ipython().system('pip install rank_bm25')\n",
        "get_ipython().system('pip install datasets')\n",
        "get_ipython().system('pip install matplotlib')\n",
        "get_ipython().system('pip install faiss')\n",
        "get_ipython().system('pip install faiss-cpu')\n",
        "get_ipython().system('pip install faiss-gpu')\n",
        "get_ipython().system('pip install --upgrade pypdf')\n",
        "\n",
        "# 2. Then install\n",
        "# !pip install \"langchain==0.2.0\"\n",
        "# !pip install \"langchain-core==0.2.0\"\n",
        "# !pip install \"langchain-community==0.2.0\"\n",
        "# !pip install \"langchain-text-splitters==0.2.1\"\n",
        "# !pip install \"langchain-openai==0.1.0\"\n",
        "# !pip install \"pydantic==2.6.4\"\n",
        "# !pip install \"ragas==0.2.14\"\n",
        "\n",
        "!pip install langchain\n",
        "!pip install langchain-core\n",
        "!pip install langchain-community\n",
        "!pip install langchain-text-splitters\n",
        "!pip install langchain-openai\n",
        "!pip install pydantic\n",
        "!pip install ragas\n",
        "\n",
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade pandas"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-18T17:53:27.623072Z",
          "iopub.execute_input": "2025-04-18T17:53:27.623409Z",
          "iopub.status.idle": "2025-04-18T17:55:56.234933Z",
          "shell.execute_reply.started": "2025-04-18T17:53:27.623376Z",
          "shell.execute_reply": "2025-04-18T17:55:56.234196Z"
        },
        "id": "RgO_gHo1EbwL",
        "outputId": "fa4d0f38-c3e4-4d9e-e624-1469d46bbea9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.2.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.11/dist-packages (1.25.5)\n",
            "Requirement already satisfied: rank_bm25 in /usr/local/lib/python3.11/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank_bm25) (2.2.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.2.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.2.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (23.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.54)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.1.147)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (0.3.54)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (0.1.147)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core) (2.11.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (3.10.16)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core) (1.3.1)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.54)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.1.147)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.2.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.19.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain-community) (2.33.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.11/dist-packages (0.3.8)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain-text-splitters) (0.3.54)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.1.147)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (2.11.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (3.10.16)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-text-splitters) (1.3.1)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.14)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.53 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.3.54)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.75.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.1.147)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain-openai) (2.11.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.53->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain-openai) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain-openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain-openai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.11/dist-packages (2.11.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic) (0.4.0)\n",
            "Requirement already satisfied: ragas in /usr/local/lib/python3.11/dist-packages (0.2.14)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ragas) (2.2.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from ragas) (3.5.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from ragas) (0.9.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (from ragas) (0.3.23)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (from ragas) (0.3.54)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (from ragas) (0.3.21)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.11/dist-packages (from ragas) (0.3.14)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ragas) (1.6.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.11/dist-packages (from ragas) (1.4.4)\n",
            "Requirement already satisfied: pydantic>=2 in /usr/local/lib/python3.11/dist-packages (from ragas) (2.11.3)\n",
            "Requirement already satisfied: openai>1 in /usr/local/lib/python3.11/dist-packages (from ragas) (1.75.0)\n",
            "Requirement already satisfied: diskcache>=5.6.3 in /usr/local/lib/python3.11/dist-packages (from ragas) (5.6.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>1->ragas) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>1->ragas) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>1->ragas) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>1->ragas) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>1->ragas) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>1->ragas) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai>1->ragas) (4.13.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->ragas) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->ragas) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->ragas) (0.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->ragas) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->ragas) (6.0.2)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain->ragas) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain->ragas) (0.1.147)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain->ragas) (2.0.40)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core->ragas) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core->ragas) (1.33)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community->ragas) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community->ragas) (2.9.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community->ragas) (0.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->ragas) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas) (1.19.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai>1->ragas) (3.10)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>1->ragas) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->ragas) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain->ragas) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->ragas) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->ragas) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->ragas) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->ragas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->ragas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->ragas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.17.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.2.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "execution_count": 147
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "here we import the necessary libraries and modules"
      ],
      "metadata": {
        "id": "RAKtxVv0EbwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Environment & Authentication =====\n",
        "import os\n",
        "import time\n",
        "import csv\n",
        "# from dotenv import load_dotenv, dotenv_values\n",
        "from huggingface_hub import login\n",
        "\n",
        "# ===== Core Python & Data Handling =====\n",
        "from typing import List, Tuple, Dict\n",
        "import textwrap\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ===== NLP Preprocessing =====\n",
        "import nltk\n",
        "from rank_bm25 import BM25Okapi  # BM25 retriever\n",
        "nltk.download('punkt')  # Ensure NLTK data is available\n",
        "\n",
        "# ===== LangChain - Document Loading & Splitting =====\n",
        "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "# from langchain.schema import Document\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# ===== LangChain - Embeddings & Vector Stores =====\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# ===== Hugging Face Models & Pipelines =====\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    pipeline\n",
        ")\n",
        "from langchain.llms import (\n",
        "    HuggingFaceHub,\n",
        "    HuggingFacePipeline\n",
        ")\n",
        "\n",
        "# ===== RAG Evaluation (RAGAS) =====\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    Faithfulness,\n",
        "    AnswerRelevancy,\n",
        "    ContextRecall,\n",
        "    ContextPrecision,\n",
        "    AnswerCorrectness\n",
        ")\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-18T17:56:42.738972Z",
          "iopub.execute_input": "2025-04-18T17:56:42.739182Z",
          "iopub.status.idle": "2025-04-18T17:57:23.363956Z",
          "shell.execute_reply.started": "2025-04-18T17:56:42.739162Z",
          "shell.execute_reply": "2025-04-18T17:57:23.363300Z"
        },
        "id": "22QiRs-aEbwO",
        "outputId": "e8156eed-7734-44fd-dd53-13af7ed8951c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "execution_count": 148
    },
    {
      "cell_type": "code",
      "source": [
        "# because there was an error in this import, here it is seperatly\n",
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-18T17:57:23.364693Z",
          "iopub.execute_input": "2025-04-18T17:57:23.365314Z",
          "iopub.status.idle": "2025-04-18T17:57:23.370006Z",
          "shell.execute_reply.started": "2025-04-18T17:57:23.365287Z",
          "shell.execute_reply": "2025-04-18T17:57:23.369143Z"
        },
        "id": "ITlgZo5AEbwO",
        "outputId": "5fb80a8e-862d-4fd4-9af5-a32ee3ff448d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0+cu124\n"
          ]
        }
      ],
      "execution_count": 149
    },
    {
      "cell_type": "code",
      "source": [
        "from packaging import version\n",
        "\n",
        "# Check torch version (modern alternative)\n",
        "def is_torch_greater_or_equal_than_1_13():\n",
        "    return version.parse(torch.__version__) >= version.parse(\"1.13.0\")\n",
        "\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "print(f\"Is >=1.13.0: {is_torch_greater_or_equal_than_1_13()}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-18T17:57:23.372449Z",
          "iopub.execute_input": "2025-04-18T17:57:23.372676Z",
          "iopub.status.idle": "2025-04-18T17:57:23.395458Z",
          "shell.execute_reply.started": "2025-04-18T17:57:23.372654Z",
          "shell.execute_reply": "2025-04-18T17:57:23.394701Z"
        },
        "id": "oJxxudGdEbwP",
        "outputId": "db0e5a54-0c21-49bc-da90-1b8786681a88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.6.0+cu124\n",
            "Is >=1.13.0: True\n"
          ]
        }
      ],
      "execution_count": 150
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Authentication\n",
        "here we authenticate our LLM with hugging face"
      ],
      "metadata": {
        "id": "KRG7BTmKEbwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load environment variables from .env\n",
        "# load_dotenv()\n",
        "\n",
        "# # Retrieve the token\n",
        "# hf_token = os.getenv(\"HUGGING_FACE_TOKEN\")\n",
        "\n",
        "# print(\"Token loaded:\", hf_token is not None)\n",
        "\n",
        "# # Log in to Hugging Face Hub\n",
        "login(token=\"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\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-18T17:57:23.396300Z",
          "iopub.execute_input": "2025-04-18T17:57:23.396541Z",
          "iopub.status.idle": "2025-04-18T17:57:23.539520Z",
          "shell.execute_reply.started": "2025-04-18T17:57:23.396524Z",
          "shell.execute_reply": "2025-04-18T17:57:23.538735Z"
        },
        "id": "F0yPasmAEbwP"
      },
      "outputs": [],
      "execution_count": 151
    },
    {
      "cell_type": "code",
      "source": [
        "# config = dotenv_values(\".env\")\n",
        "# login(token=config[\"HUGGING_FACE_TOKEN\"])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-18T17:57:23.540413Z",
          "iopub.execute_input": "2025-04-18T17:57:23.540700Z",
          "iopub.status.idle": "2025-04-18T17:57:23.544032Z",
          "shell.execute_reply.started": "2025-04-18T17:57:23.540678Z",
          "shell.execute_reply": "2025-04-18T17:57:23.543398Z"
        },
        "id": "rqAqPx9_EbwQ"
      },
      "outputs": [],
      "execution_count": 152
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants\n",
        "here we define constants that we will fine tune"
      ],
      "metadata": {
        "id": "mwpZZH72EbwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "DEFAULT_CHUNK_SIZE = 1000              # Max size of each text chunk\n",
        "DEFAULT_CHUNK_OVERLAP = 200            # Overlap between chunks\n",
        "DEFAULT_SEARCH_K = 3                   # Top-k results to retrieve\n",
        "DEFAULT_SEARCH_TYPE = \"hybrid\"         # Choose from: 'semantic', 'keyword', or 'hybrid'\n",
        "DEFAULT_EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # Embedding model for vector search\n",
        "DEFAULT_LLM_MODEL = \"meta-llama/Llama-3.2-1B\"                         # LLM for generating answers\n",
        "# DEFAULT_LLM_MODEL = \"deepseek-ai/DeepSeek-V3-0324\"                         # LLM for generating answers\n",
        "# DEFAULT_DOCUMENT_DIR = \"/data/corpus.zip\"\n",
        "# DEFAULT_DOCUMENT_DIR = \"/kaggle/input/daa-lectures-for-a4/cmu-lecs\"\n",
        "DEFAULT_DOCUMENT_DIR = \"/content/\"  # Changed to the directory path"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.002Z"
        },
        "id": "uvpZHpqdEbwQ"
      },
      "outputs": [],
      "execution_count": 153
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Processing Functions"
      ],
      "metadata": {
        "id": "L1E1l1iNxLEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_documents(directory: str, glob_pattern: str = \"**/*.pdf\") -> List[Document]:\n",
        "    \"\"\"\n",
        "    Loads all PDF files from a given directory.\n",
        "\n",
        "    Args:\n",
        "        directory: path to folder with PDF files\n",
        "        glob_pattern: pattern to match files (default = all PDFs)\n",
        "\n",
        "    Returns:\n",
        "        List of LangChain Document objects\n",
        "    \"\"\"\n",
        "    loader = DirectoryLoader(directory, glob=glob_pattern, loader_cls=PyPDFLoader)\n",
        "    return loader.load()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.005Z"
        },
        "id": "HV2X-Xd7EbwQ"
      },
      "outputs": [],
      "execution_count": 154
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_documents(\n",
        "    documents: List[Document],\n",
        "    chunk_size: int = DEFAULT_CHUNK_SIZE,\n",
        "    chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,\n",
        "    separators: List[str] = None\n",
        ") -> List[Document]:\n",
        "    \"\"\"\n",
        "    Splits documents into chunks for better retrieval.\n",
        "\n",
        "    Args:\n",
        "        documents: list of LangChain documents\n",
        "        chunk_size: size of each chunk\n",
        "        chunk_overlap: how much content overlaps between chunks\n",
        "        separators: optional list of separators for better splitting\n",
        "\n",
        "    Returns:\n",
        "        List of chunked Document objects\n",
        "    \"\"\"\n",
        "    if separators is None:\n",
        "        # Default separators: prioritize splitting on paragraphs, then sentences, then words\n",
        "        separators = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        separators=separators\n",
        "    )\n",
        "    return text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.005Z"
        },
        "id": "k1ua7tohEbwR"
      },
      "outputs": [],
      "execution_count": 155
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_store(\n",
        "    chunks: List[Document],\n",
        "    embedding_model: str = DEFAULT_EMBEDDING_MODEL,\n",
        "    save_path: str = None\n",
        ") -> FAISS:\n",
        "    \"\"\"\n",
        "    Creates a FAISS vector index from document chunks using specified embedding model.\n",
        "\n",
        "    Args:\n",
        "        chunks: list of Document chunks\n",
        "        embedding_model: HuggingFace model used for embeddings\n",
        "        save_path: optional path to save the index\n",
        "\n",
        "    Returns:\n",
        "        FAISS vector store\n",
        "    \"\"\"\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
        "    vectordb = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "    if save_path:\n",
        "        vectordb.save_local(save_path)\n",
        "\n",
        "    return vectordb"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.006Z"
        },
        "id": "IYGOIWotEbwR"
      },
      "outputs": [],
      "execution_count": 156
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bm25_index(chunks: List[Document]) -> BM25Okapi:\n",
        "    \"\"\"\n",
        "    Builds a keyword-based index using BM25.\n",
        "\n",
        "    Args:\n",
        "        chunks: list of Document chunks\n",
        "\n",
        "    Returns:\n",
        "        BM25 index\n",
        "    \"\"\"\n",
        "    texts = [chunk.page_content for chunk in chunks]                  # Get plain text\n",
        "    tokenized_texts = [text.split() for text in texts]               # Tokenize by whitespace\n",
        "    return BM25Okapi(tokenized_texts)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.006Z"
        },
        "id": "1SBM9zbqEbwR"
      },
      "outputs": [],
      "execution_count": 157
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Search Functions"
      ],
      "metadata": {
        "id": "p-gPSeFBxLEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search(\n",
        "    query: str,\n",
        "    vectordb: FAISS,\n",
        "    k: int = DEFAULT_SEARCH_K,\n",
        "    score_threshold: float = None\n",
        ") -> List[Tuple[Document, float]]:\n",
        "    \"\"\"\n",
        "    Perform semantic search using vector similarity from FAISS.\n",
        "\n",
        "    Args:\n",
        "        query: Natural language question\n",
        "        vectordb: Vector index (FAISS)\n",
        "        k: Number of results to return\n",
        "        score_threshold: Filter out low similarity scores (optional)\n",
        "\n",
        "    Returns:\n",
        "        List of (Document, similarity_score) tuples\n",
        "    \"\"\"\n",
        "    results = vectordb.similarity_search_with_score(query, k=k)\n",
        "\n",
        "    # Optional thresholding to remove irrelevant results\n",
        "    if score_threshold is not None:\n",
        "        results = [(doc, score) for doc, score in results if score >= score_threshold]\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.007Z"
        },
        "id": "fE1SDy_nEbwR"
      },
      "outputs": [],
      "execution_count": 158
    },
    {
      "cell_type": "code",
      "source": [
        "def keyword_search(\n",
        "    query: str,\n",
        "    bm25_index: BM25Okapi,\n",
        "    chunks: List[Document],\n",
        "    k: int = DEFAULT_SEARCH_K,\n",
        "    score_threshold: float = None\n",
        ") -> List[Tuple[Document, float]]:\n",
        "    \"\"\"\n",
        "    Perform lexical search using BM25.\n",
        "\n",
        "    Args:\n",
        "        query: Search query string\n",
        "        bm25_index: Pre-built BM25 index\n",
        "        chunks: List of document chunks for mapping back\n",
        "        k: Top-k documents to retrieve\n",
        "        score_threshold: Optional filtering threshold for BM25 scores\n",
        "\n",
        "    Returns:\n",
        "        List of (Document, BM25_score) tuples\n",
        "    \"\"\"\n",
        "    tokenized_query = query.split()  # Basic whitespace tokenization\n",
        "    scores = bm25_index.get_scores(tokenized_query)\n",
        "\n",
        "    # Get indices of top-k documents\n",
        "    top_k_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
        "\n",
        "    results = [(chunks[i], scores[i]) for i in top_k_indices]\n",
        "\n",
        "    if score_threshold is not None:\n",
        "        results = [(doc, score) for doc, score in results if score >= score_threshold]\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.007Z"
        },
        "id": "Nhfc2td3EbwS"
      },
      "outputs": [],
      "execution_count": 159
    },
    {
      "cell_type": "code",
      "source": [
        "def hybrid_search(\n",
        "    query: str,\n",
        "    vectordb: FAISS,\n",
        "    bm25_index: BM25Okapi,\n",
        "    chunks: List[Document],\n",
        "    k: int = DEFAULT_SEARCH_K,\n",
        "    semantic_weight: float = 0.5,\n",
        "    keyword_weight: float = 0.5\n",
        ") -> List[Tuple[Document, float]]:\n",
        "    \"\"\"\n",
        "    Combine semantic and keyword search using weighted score fusion.\n",
        "\n",
        "    Args:\n",
        "        query: Natural language query\n",
        "        vectordb: FAISS vector database\n",
        "        bm25_index: BM25 keyword index\n",
        "        chunks: Document chunks (used for mapping back)\n",
        "        k: Top-k results to return\n",
        "        semantic_weight: Weight for vector similarity\n",
        "        keyword_weight: Weight for BM25 relevance\n",
        "\n",
        "    Returns:\n",
        "        List of (Document, combined_score) tuples\n",
        "    \"\"\"\n",
        "    # Step 1: Run both types of searches with larger k (to capture wider context)\n",
        "    semantic_results = semantic_search(query, vectordb, k * 2)\n",
        "    semantic_scores = {doc.page_content: score for doc, score in semantic_results}\n",
        "\n",
        "    keyword_results = keyword_search(query, bm25_index, chunks, k * 2)\n",
        "    keyword_scores = {doc.page_content: score for doc, score in keyword_results}\n",
        "\n",
        "    # Step 2: Normalize BM25 scores (they are not bounded, unlike cosine similarity)\n",
        "    max_kw_score = max(keyword_scores.values()) if keyword_scores else 1\n",
        "\n",
        "    # Step 3: Combine results\n",
        "    all_docs = set(semantic_scores.keys()).union(set(keyword_scores.keys()))\n",
        "    combined_scores = []\n",
        "\n",
        "    for doc_content in all_docs:\n",
        "        sem_score = semantic_scores.get(doc_content, 0)\n",
        "        kw_score = keyword_scores.get(doc_content, 0)\n",
        "        norm_kw_score = kw_score / max_kw_score if max_kw_score > 0 else 0\n",
        "\n",
        "        # Weighted sum of both types of scores\n",
        "        combined_score = (semantic_weight * sem_score) + (keyword_weight * norm_kw_score)\n",
        "        combined_scores.append((doc_content, combined_score))\n",
        "\n",
        "    # Step 4: Sort and return top-k\n",
        "    combined_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    top_scores = combined_scores[:k]\n",
        "\n",
        "    # Step 5: Re-map back to full Document objects using content\n",
        "    doc_lookup = {chunk.page_content: chunk for chunk in chunks}\n",
        "    results = []\n",
        "\n",
        "    for doc_content, score in top_scores:\n",
        "        if doc_content in doc_lookup:\n",
        "            results.append((doc_lookup[doc_content], score))\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.007Z"
        },
        "id": "J3T0XMX0xLEH",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 160
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM functions"
      ],
      "metadata": {
        "id": "7Q5NGOdUxLEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_llm(\n",
        "    model_name: str = DEFAULT_LLM_MODEL,\n",
        "    device: str = \"cuda\",  # Use \"cpu\" if not using GPU\n",
        "    max_new_tokens: int = 300\n",
        ") -> Tuple[pipeline, any]:\n",
        "    \"\"\"\n",
        "    Loads a language model pipeline for text generation.\n",
        "\n",
        "    Args:\n",
        "        model_name: HuggingFace model repo (must support causal LM)\n",
        "        device: \"cuda\" for GPU or \"cpu\"\n",
        "        max_new_tokens: Max tokens to generate per response\n",
        "\n",
        "    Returns:\n",
        "        Tuple (generator pipeline, tokenizer)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=device,\n",
        "            torch_dtype=\"auto\",            # Uses GPU acceleration if available\n",
        "            trust_remote_code=True         # Allow custom model architectures\n",
        "        )\n",
        "        print(\"Original used\")\n",
        "    except ImportError:\n",
        "        # Fallback without device_map\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        ).to(device)\n",
        "        print(\"Edited used\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Format the prompt as a dialogue (user + assistant style)\n",
        "    tokenizer.chat_template = (\n",
        "        \"{% for message in messages %}\"\n",
        "        \"{% if message['role'] == 'user' %}User: {{ message['content'] }}\\n\"\n",
        "        \"{% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }}\\n\"\n",
        "        \"{% endif %}\"\n",
        "        \"{% endfor %}\"\n",
        "        \"{% if add_generation_prompt %}Assistant:{% endif %}\"\n",
        "    )\n",
        "\n",
        "    # Create a text-generation pipeline\n",
        "    generator = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        return_full_text=False,        # Only return generated part, not the full prompt\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True                 # Use sampling (stochastic generation)\n",
        "    )\n",
        "\n",
        "    return generator, tokenizer"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.008Z"
        },
        "id": "v89jLCD_EbwS"
      },
      "outputs": [],
      "execution_count": 161
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(\n",
        "    prompt: str,\n",
        "    generator: pipeline,\n",
        "    width: int = 80  # For pretty-printing long outputs\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generates a response from the LLM using the prompt.\n",
        "\n",
        "    Args:\n",
        "        prompt: Full RAG-formatted prompt with question + context\n",
        "        generator: HF pipeline object\n",
        "        width: max characters per printed line (for wrapping)\n",
        "\n",
        "    Returns:\n",
        "        Answer string\n",
        "    \"\"\"\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]        # Wrap prompt in chat message format\n",
        "    output = generator(\n",
        "        messages,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True\n",
        "    )\n",
        "                            # Call LLM\n",
        "    return textwrap.fill(output[0][\"generated_text\"], width=width)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.008Z"
        },
        "id": "kQMhtE8lEbwT"
      },
      "outputs": [],
      "execution_count": 162
    },
    {
      "cell_type": "code",
      "source": [
        "def format_rag_prompt(\n",
        "    question: str,\n",
        "    retrieved_docs: List[Document],\n",
        "    instruction: str = None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Formats the final input prompt to send to the LLM.\n",
        "\n",
        "    Args:\n",
        "        question: The user's natural language question\n",
        "        retrieved_docs: List of document chunks retrieved by search\n",
        "        instruction: Optional prompt instructions (system message)\n",
        "\n",
        "    Returns:\n",
        "        Full prompt text string\n",
        "    \"\"\"\n",
        "    # Default instructions to guide the LLM on how to use retrieved documents\n",
        "    if instruction is None:\n",
        "        instruction = \"\"\"You are an AI assistant tasked with answering questions based on retrieved knowledge.\n",
        "                    - Integrate the key points from all retrieved responses into a cohesive, well-structured answer.\n",
        "                    - If the responses are contradictory, mention the different perspectives.\n",
        "                    - If none of the retrieved responses contain relevant information, reply:\n",
        "                    \"I couldn't find a good response to your query in the database.\"\n",
        "                    \"\"\"\n",
        "\n",
        "    # Truncate each document to 1000 characters if long\n",
        "    retrieved_info = \"\\n\\n\".join(\n",
        "        f\"{i+1} {doc.page_content[:1000]}...\" if len(doc.page_content) > 1000\n",
        "        else f\"{i+1} {doc.page_content}\"\n",
        "        for i, doc in enumerate(retrieved_docs)\n",
        "    )\n",
        "\n",
        "    # Final structured prompt\n",
        "    return f\"\"\"\n",
        "        {instruction}\n",
        "\n",
        "        ### Retrieved Information:\n",
        "        {retrieved_info}\n",
        "\n",
        "        ### Question:\n",
        "        {question}\n",
        "    \"\"\""
      ],
      "metadata": {
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.008Z"
        },
        "id": "9GJIwleTxLEJ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 163
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Evaluator"
      ],
      "metadata": {
        "id": "07jDDqiSxLEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WRAPPER TO USE LANGCHAIN LLM IN RAGAS\n",
        "\n",
        "class HuggingFaceLLMWrapper:\n",
        "    \"\"\"\n",
        "    Wrapper for using HuggingFacePipeline with RAGAS evaluation.\n",
        "    Includes a dummy 'set_run_config' to avoid errors.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pipeline):\n",
        "        self.pipeline = pipeline\n",
        "\n",
        "    def __call__(self, prompt: str) -> str:\n",
        "        return self.pipeline(prompt)\n",
        "\n",
        "    def set_run_config(self, run_config):\n",
        "        \"\"\"Dummy method to avoid errors with ragas.\"\"\"\n",
        "        pass  # Do nothing, as TextGenerationPipeline doesn't have this method"
      ],
      "metadata": {
        "id": "86nm904ENFqi"
      },
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WRAPPER TO USE LANGCHAIN LLM IN RAGAS\n",
        "\n",
        "class HuggingFaceLLM:\n",
        "    \"\"\"\n",
        "    Simple wrapper for using HuggingFaceHub with RAGAS evaluation.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model = HuggingFaceHub(repo_id=model_name)\n",
        "\n",
        "    def __call__(self, prompt: str) -> str:\n",
        "        return self.model(prompt)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.009Z"
        },
        "id": "eKsmE4PVEbwU"
      },
      "outputs": [],
      "execution_count": 165
    },
    {
      "cell_type": "code",
      "source": [
        "# EVALUATOR CLASS\n",
        "\n",
        "class RAGEvaluator:\n",
        "    \"\"\"\n",
        "    Performs automatic evaluation of RAG responses using Ragas metrics.\n",
        "    Also supports result visualization and optimization insights.\n",
        "    \"\"\"\n",
        "    def __init__(self, pipeline, llm, embeddings):\n",
        "        self.pipeline = pipeline         # RAGPipeline object\n",
        "        self.embeddings = embeddings     # HuggingFaceEmbeddings instance\n",
        "\n",
        "        # Use passed LLM, or initialize default\n",
        "        if isinstance(llm, str):\n",
        "            self.llm = HuggingFaceHub(repo_id=llm)\n",
        "        else:\n",
        "            self.llm = HuggingFaceLLMWrapper(llm) if llm else HuggingFaceLLMWrapper(HuggingFaceHub(repo_id=DEFAULT_LLM_MODEL))\n",
        "\n",
        "        # Internal result tracking\n",
        "        self.results = []\n",
        "\n",
        "    def evaluate_ragas(self, questions: list, gold_answers: list = None):\n",
        "        \"\"\"\n",
        "        Run Ragas evaluation across all questions.\n",
        "\n",
        "        Args:\n",
        "            questions: List of input questions\n",
        "            gold_answers: Reference answers (optional)\n",
        "\n",
        "        Returns:\n",
        "            DataFrame of results\n",
        "        \"\"\"\n",
        "        all_rows = []\n",
        "\n",
        "        for question, gold_answer in zip(questions, gold_answers or [None]*len(questions)):\n",
        "            # Run full RAG query\n",
        "            answer = self.pipeline.query(question)\n",
        "\n",
        "            # Get context used in the answer\n",
        "            contexts = [doc.page_content for doc in self.pipeline.get_last_retrieved_docs()]\n",
        "\n",
        "            # Prepare a single sample for evaluation\n",
        "            data = {\n",
        "                \"question\": [question],\n",
        "                \"answer\": [answer],\n",
        "                \"contexts\": [contexts]\n",
        "            }\n",
        "            if gold_answer:\n",
        "                data[\"ground_truth\"] = [gold_answer]\n",
        "\n",
        "            dataset = Dataset.from_dict(data)\n",
        "\n",
        "            # Select metrics to compute\n",
        "            metrics = [Faithfulness(), AnswerRelevancy(), ContextRecall(), ContextPrecision()]\n",
        "            if gold_answer:\n",
        "                metrics.append(AnswerCorrectness())\n",
        "\n",
        "            # Run the evaluation\n",
        "            result = evaluate(dataset, metrics=metrics, llm=self.llm, embeddings=self.embeddings)\n",
        "\n",
        "            # Convert to DataFrame and store\n",
        "            row = result.to_pandas()\n",
        "            row[\"question\"] = question\n",
        "            row[\"retrieved_docs\"] = len(contexts)\n",
        "            all_rows.append(row)\n",
        "\n",
        "        # Combine all rows into one DataFrame\n",
        "        self.results = pd.concat(all_rows, ignore_index=True)\n",
        "        return self.results\n",
        "\n",
        "    def visualize_metrics(self):\n",
        "        \"\"\"\n",
        "        Visualize average metric scores and context retrieval stats.\n",
        "        \"\"\"\n",
        "        if self.results is None or self.results.empty:\n",
        "            raise ValueError(\"No evaluation results found. Run evaluate_ragas() first.\")\n",
        "\n",
        "        # Plot main metrics\n",
        "        metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']\n",
        "        if 'answer_correctness' in self.results.columns:\n",
        "            metrics.append('answer_correctness')\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        self.results[metrics].mean().plot(kind='bar', color='lightblue')\n",
        "        plt.title(\" Average RAG Evaluation Metrics\")\n",
        "        plt.ylabel(\"Score (0 to 1)\")\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "        plt.show()\n",
        "\n",
        "        # Plot document retrieval counts\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        self.results['retrieved_docs'].value_counts().sort_index().plot(kind='bar', color='lightgreen')\n",
        "        plt.title(\" Number of Context Chunks Retrieved Per Query\")\n",
        "        plt.xlabel(\"Number of Chunks\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "        plt.show()\n",
        "\n",
        "    def get_optimization_insights(self):\n",
        "        \"\"\"\n",
        "        Analyze weak metrics and recommend strategies to improve RAG performance.\n",
        "        \"\"\"\n",
        "        if self.results is None or self.results.empty:\n",
        "            return \"No evaluation results available.\"\n",
        "\n",
        "        insights = []\n",
        "        df = self.results\n",
        "\n",
        "        # Faithfulness issues (hallucination)\n",
        "        if df['faithfulness'].mean() < 0.7:\n",
        "            insights.append(\" Faithfulness is low  possible hallucinations.\")\n",
        "            insights.append(\" Try increasing chunk overlap or improving retrieval relevance.\")\n",
        "\n",
        "        # Context recall issues (missing info)\n",
        "        if df['context_recall'].mean() < 0.6:\n",
        "            insights.append(\" Low context recall  relevant info may be missed.\")\n",
        "            insights.append(\" Consider using hybrid retrieval or adjusting chunk size.\")\n",
        "\n",
        "        # Precision issues (irrelevant info)\n",
        "        if df['context_precision'].mean() < 0.6:\n",
        "            insights.append(\" Low context precision  too much irrelevant context.\")\n",
        "            insights.append(\" Use better embeddings or rerank retrieved chunks.\")\n",
        "\n",
        "        # Relevance issues (answer not matching question)\n",
        "        if df['answer_relevancy'].mean() < 0.7:\n",
        "            insights.append(\" Low answer relevancy  answers not matching question.\")\n",
        "            insights.append(\" Refine your prompts or improve chunk matching.\")\n",
        "\n",
        "        # Optional: Correctness based on gold answers\n",
        "        if 'answer_correctness' in df.columns and df['answer_correctness'].mean() < 0.7:\n",
        "            insights.append(\" Low correctness  answers differ from references.\")\n",
        "            insights.append(\" Try different LLMs or use post-editing.\")\n",
        "\n",
        "        return \"\\n\".join(insights)"
      ],
      "metadata": {
        "id": "DAFXSDHQxLEK",
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.010Z"
        }
      },
      "outputs": [],
      "execution_count": 166
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Pipeline"
      ],
      "metadata": {
        "id": "r1Lfz6KJxLEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "\n",
        "class SimpleEvaluator:\n",
        "    \"\"\"\n",
        "    Evaluates the quality of generated answers by comparing the original question\n",
        "    with a regenerated question from the generated answer.\n",
        "    \"\"\"\n",
        "    def __init__(self, llm, tokenizer, embedding_model: str = \"all-MiniLM-L6-v2\"):\n",
        "        self.llm = llm\n",
        "        self.tokenizer = tokenizer\n",
        "        self.embedding_model = SentenceTransformer(embedding_model)\n",
        "\n",
        "    def generate_question(self, answer: str) -> str:\n",
        "        prompt = f\"Generate a question that could be answered with the following passage:\\n\\n{answer}\\n\\nQuestion:\"\n",
        "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "        output = self.llm(messages)\n",
        "        return output[0][\"generated_text\"].strip()\n",
        "\n",
        "    def compute_similarity(self, q1: str, q2: str) -> float:\n",
        "        emb1 = self.embedding_model.encode(q1, convert_to_tensor=True)\n",
        "        emb2 = self.embedding_model.encode(q2, convert_to_tensor=True)\n",
        "        return float(util.pytorch_cos_sim(emb1, emb2).item())\n",
        "\n",
        "    def evaluate(self, question: str, generated_answer: str) -> dict:\n",
        "        regenerated_question = self.generate_question(generated_answer)\n",
        "        similarity = self.compute_similarity(question, regenerated_question)\n",
        "        return {\n",
        "            \"original_question\": question,\n",
        "            \"regenerated_question\": regenerated_question,\n",
        "            \"similarity_score\": round(similarity, 4)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "70ftZAKCSjyr"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN RAG PIPELINE\n",
        "\n",
        "class RAGPipeline:\n",
        "    \"\"\"\n",
        "    This is the central class that handles:\n",
        "    - Loading and chunking documents\n",
        "    - Initializing vector and keyword search\n",
        "    - Running queries\n",
        "    - Generating responses from the LLM\n",
        "    - Running full experiment sweeps\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        document_dir: str,\n",
        "        embedding_model: str = DEFAULT_EMBEDDING_MODEL,\n",
        "        llm_model: str = DEFAULT_LLM_MODEL,\n",
        "        chunk_size: int = DEFAULT_CHUNK_SIZE,\n",
        "        chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,\n",
        "        device: str = \"cuda\"\n",
        "    ):\n",
        "        self.document_dir = document_dir\n",
        "        self.embedding_model = embedding_model\n",
        "        self.llm_model = llm_model\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.device = device\n",
        "\n",
        "        # To store runtime state\n",
        "        self.documents = None\n",
        "        self.chunks = None\n",
        "        self.vectordb = None\n",
        "        self.bm25_index = None\n",
        "        self.llm = None\n",
        "        self.tokenizer = None\n",
        "        self.last_retrieved_docs = None  # For evaluation traceability\n",
        "\n",
        "\n",
        "    # ==========================\n",
        "    # LOAD & CHUNK DOCUMENTS\n",
        "    # ==========================\n",
        "    def load_and_process_documents(self):\n",
        "        \"\"\"\n",
        "        Loads PDF documents and splits them into overlapping chunks.\n",
        "        \"\"\"\n",
        "        print(\" Loading documents...\")\n",
        "        self.documents = load_documents(self.document_dir)\n",
        "        print(f\" Loaded {len(self.documents)} document pages.\")\n",
        "\n",
        "        print(\" Chunking documents...\")\n",
        "        self.chunks = chunk_documents(\n",
        "            self.documents,\n",
        "            chunk_size=self.chunk_size,\n",
        "            chunk_overlap=self.chunk_overlap\n",
        "        )\n",
        "        print(f\" Created {len(self.chunks)} chunks.\")\n",
        "\n",
        "        # Add unique IDs to chunks for tracking\n",
        "        for i, chunk in enumerate(self.chunks):\n",
        "            chunk.metadata[\"chunk_id\"] = i\n",
        "\n",
        "\n",
        "    # ==========================\n",
        "    # INITIALIZE RETRIEVAL\n",
        "    # ==========================\n",
        "    def initialize_retrieval(self):\n",
        "        \"\"\"\n",
        "        Builds vector store and keyword index for retrieval.\n",
        "        \"\"\"\n",
        "        if not self.chunks:\n",
        "            raise ValueError(\" No chunks found. Run load_and_process_documents() first.\")\n",
        "\n",
        "        print(\" Creating vector store...\")\n",
        "        self.vectordb = create_vector_store(self.chunks, self.embedding_model)\n",
        "\n",
        "        print(\" Creating BM25 index...\")\n",
        "        self.bm25_index = create_bm25_index(self.chunks)\n",
        "\n",
        "\n",
        "    # ==========================\n",
        "    # INITIALIZE LLM\n",
        "    # ==========================\n",
        "    def initialize_llm(self):\n",
        "        \"\"\"\n",
        "        Loads the chosen LLM and tokenizer from HuggingFace.\n",
        "        \"\"\"\n",
        "        print(\" Loading LLM...\")\n",
        "        self.llm, self.tokenizer = initialize_llm(self.llm_model, self.device)\n",
        "\n",
        "\n",
        "    # ==========================\n",
        "    # GET LAST RETRIEVED CHUNKS\n",
        "    # ==========================\n",
        "    def get_last_retrieved_docs(self):\n",
        "        \"\"\"\n",
        "        Returns the last set of retrieved document chunks (used in evaluation).\n",
        "        \"\"\"\n",
        "        if self.last_retrieved_docs is None:\n",
        "            raise ValueError(\" No retrievals done yet.\")\n",
        "        return self.last_retrieved_docs\n",
        "\n",
        "\n",
        "    # ==========================\n",
        "    # MAIN QUERY FUNCTION\n",
        "    # ==========================\n",
        "    def query(\n",
        "        self,\n",
        "        question: str,\n",
        "        search_type: str = DEFAULT_SEARCH_TYPE,  # semantic / keyword / hybrid\n",
        "        k: int = DEFAULT_SEARCH_K,\n",
        "        semantic_weight: float = 0.5,\n",
        "        keyword_weight: float = 0.5,\n",
        "        custom_instruction: str = None\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Executes a full query through the pipeline:\n",
        "        - Retrieves chunks\n",
        "        - Formats prompt\n",
        "        - Calls LLM\n",
        "        - Returns answer\n",
        "        \"\"\"\n",
        "        if not self.vectordb or not self.bm25_index:\n",
        "            raise ValueError(\" Retrieval systems not ready. Run initialize_retrieval().\")\n",
        "        if not self.llm:\n",
        "            raise ValueError(\" LLM not initialized. Run initialize_llm().\")\n",
        "\n",
        "        # Step 1: Retrieve relevant documents\n",
        "        if search_type == \"semantic\":\n",
        "            results = semantic_search(question, self.vectordb, k)\n",
        "        elif search_type == \"keyword\":\n",
        "            results = keyword_search(question, self.bm25_index, self.chunks, k)\n",
        "        elif search_type == \"hybrid\":\n",
        "            results = hybrid_search(\n",
        "                question,\n",
        "                self.vectordb,\n",
        "                self.bm25_index,\n",
        "                self.chunks,\n",
        "                k,\n",
        "                semantic_weight,\n",
        "                keyword_weight\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\" Unknown search type: {search_type}\")\n",
        "\n",
        "        retrieved_docs = [doc for doc, _ in results]\n",
        "        self.last_retrieved_docs = retrieved_docs\n",
        "\n",
        "        # Step 2: Format prompt\n",
        "        prompt = format_rag_prompt(question, retrieved_docs, custom_instruction)\n",
        "\n",
        "        # Step 3: Generate LLM response\n",
        "        return generate_response(prompt, self.llm)\n",
        "\n",
        "\n",
        "    # ==========================\n",
        "    # EXPERIMENTATION FUNCTION\n",
        "    # ==========================\n",
        "    def experiment(\n",
        "        self,\n",
        "        questions: List[str],\n",
        "        gold_answers: List[str],\n",
        "        chunk_sizes: List[int],\n",
        "        k_values: List[int],\n",
        "        search_types: List[str],\n",
        "        chunk_overlaps: List[int] = [0, 100, 200]\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Run multiple experiment configurations (varying chunk size, k, search type).\n",
        "\n",
        "        Args:\n",
        "            questions: list of input queries\n",
        "            gold_answers: reference answers (used in evaluation)\n",
        "            chunk_sizes: different chunk sizes to test\n",
        "            k_values: number of results to retrieve\n",
        "            search_types: list of retrieval modes\n",
        "            chunk_overlaps: amount of content overlap between chunks\n",
        "\n",
        "        Returns:\n",
        "            A dict of results by config\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "\n",
        "        for chunk_size in chunk_sizes:\n",
        "            for chunk_overlap in chunk_overlaps:\n",
        "                self.chunk_size = chunk_size\n",
        "                self.chunk_overlap = chunk_overlap\n",
        "\n",
        "                print(f\"\\n  Testing: chunk={chunk_size}, overlap={chunk_overlap}\")\n",
        "                self.load_and_process_documents()\n",
        "                self.initialize_retrieval()\n",
        "\n",
        "                evaluator = RAGEvaluator(self, self.llm, HuggingFaceEmbeddings(model_name=self.embedding_model))\n",
        "\n",
        "                for search_type in search_types:\n",
        "                    for k in k_values:\n",
        "                        config_name = f\"chunk{chunk_size}_overlap{chunk_overlap}_{search_type}_k{k}\"\n",
        "                        results[config_name] = {}\n",
        "\n",
        "                        for question in questions:\n",
        "                            try:\n",
        "                                answer = self.query(\n",
        "                                    question,\n",
        "                                    search_type=search_type,\n",
        "                                    k=k\n",
        "                                )\n",
        "                                results[config_name][question] = answer\n",
        "\n",
        "                                # Run evaluation (with gold answer)\n",
        "                                eval_result = evaluator.evaluate_ragas([question], [gold_answers[0]])\n",
        "                                print(eval_result[['faithfulness', 'answer_relevancy']])\n",
        "\n",
        "                                # Visualization + optimization\n",
        "                                evaluator.visualize_metrics()\n",
        "                                print(\"\\n Optimization Suggestions:\")\n",
        "                                print(evaluator.get_optimization_insights())\n",
        "\n",
        "                            except Exception as e:\n",
        "                                results[config_name][question] = f\" Error: {str(e)}\"\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "    def grid_search(\n",
        "        self,\n",
        "        questions: List[str],\n",
        "        gold_answers: List[str],\n",
        "        chunk_sizes: List[int],\n",
        "        k_values: List[int],\n",
        "        search_types: List[str],\n",
        "        chunk_overlaps: List[int] = [0, 100, 200],\n",
        "        semantic_weight: float = 0.5,\n",
        "        keyword_weight: float = 0.5,\n",
        "        output_csv_path: str = \"rag_grid_search_results.csv\"\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Run grid search over multiple config combinations and log results to CSV.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of results.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        header_written = False\n",
        "\n",
        "        # Ensure output directory exists\n",
        "        os.makedirs(os.path.dirname(output_csv_path) or \".\", exist_ok=True)\n",
        "\n",
        "        with open(output_csv_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "            writer = csv.writer(file)\n",
        "\n",
        "            for chunk_size in chunk_sizes:\n",
        "                for chunk_overlap in chunk_overlaps:\n",
        "                    if chunk_overlap >= chunk_size:\n",
        "                        continue\n",
        "                    self.chunk_size = chunk_size\n",
        "                    self.chunk_overlap = chunk_overlap\n",
        "\n",
        "                    self.load_and_process_documents()\n",
        "                    self.initialize_retrieval()\n",
        "\n",
        "                    evaluator = SimpleEvaluator(self.llm, self.tokenizer)\n",
        "\n",
        "                    for search_type in search_types:\n",
        "                        for k in k_values:\n",
        "                            config_name = f\"chunk{chunk_size}_overlap{chunk_overlap}_{search_type}_k{k}\"\n",
        "                            print(f\"\\n Running Config: {config_name}\")\n",
        "\n",
        "                            for i, question in enumerate(questions):\n",
        "                                try:\n",
        "                                    start_time = time.time()\n",
        "\n",
        "                                    # Step 1: Query\n",
        "                                    retrieval_start = time.time()\n",
        "                                    answer = self.query(\n",
        "                                        question=question,\n",
        "                                        search_type=search_type,\n",
        "                                        k=k,\n",
        "                                        semantic_weight=semantic_weight,\n",
        "                                        keyword_weight=keyword_weight\n",
        "                                    )\n",
        "                                    retrieval_end = time.time()\n",
        "\n",
        "                                    # Step 2: Evaluation\n",
        "                                    eval_df = evaluator.evaluate_ragas([question], [gold_answers[i]])\n",
        "                                    eval_row = eval_df.iloc[0].to_dict()\n",
        "\n",
        "                                    # Step 3: Timing\n",
        "                                    end_time = time.time()\n",
        "                                    total_time = end_time - start_time\n",
        "                                    retrieval_time = retrieval_end - retrieval_start\n",
        "                                    generation_time = total_time - retrieval_time\n",
        "\n",
        "                                    print(eval_result[['faithfulness', 'answer_relevancy']])\n",
        "\n",
        "                                    # Visualization + optimization\n",
        "                                    evaluator.visualize_metrics()\n",
        "                                    print(\"\\n Optimization Suggestions:\")\n",
        "                                    print(evaluator.get_optimization_insights())\n",
        "\n",
        "                                    # Step 4: Build result row\n",
        "                                    row = {\n",
        "                                        \"config\": config_name,\n",
        "                                        \"question\": question,\n",
        "                                        \"answer\": answer,\n",
        "                                        \"chunk_size\": chunk_size,\n",
        "                                        \"chunk_overlap\": chunk_overlap,\n",
        "                                        \"search_type\": search_type,\n",
        "                                        \"top_k\": k,\n",
        "                                        \"semantic_weight\": semantic_weight,\n",
        "                                        \"keyword_weight\": keyword_weight,\n",
        "                                        \"retrieval_time\": round(retrieval_time, 4),\n",
        "                                        \"generation_time\": round(generation_time, 4),\n",
        "                                        \"total_time\": round(total_time, 4),\n",
        "                                        **eval_row\n",
        "                                    }\n",
        "\n",
        "                                    # Write CSV header if needed\n",
        "                                    if not header_written:\n",
        "                                        writer.writerow(row.keys())\n",
        "                                        header_written = True\n",
        "\n",
        "                                    writer.writerow(row.values())\n",
        "                                    results.append(row)\n",
        "\n",
        "                                except Exception as e:\n",
        "                                    print(f\" Error in config {config_name} for question '{question}': {e}\")\n",
        "                                    writer.writerow([config_name, question, f\"Error: {str(e)}\"])\n",
        "\n",
        "        print(f\"\\n All results logged to: {output_csv_path}\")\n",
        "        return results"
      ],
      "metadata": {
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.010Z"
        },
        "id": "ebUXkzJ5xLEL",
        "trusted": true
      },
      "outputs": [],
      "execution_count": 168
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vz0NPXGcQqQ4"
      },
      "execution_count": 168,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running\n",
        "here we now run the code"
      ],
      "metadata": {
        "id": "ieAPdGVSEbwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: CREATE EMBEDDINGS & LLM WRAPPER\n",
        "\n",
        "# Use the same model you defined as DEFAULT_EMBEDDING_MODEL\n",
        "embeddings = HuggingFaceEmbeddings(model_name=DEFAULT_EMBEDDING_MODEL)\n",
        "\n",
        "# Initialize LLM pipeline\n",
        "generator, tokenizer = initialize_llm(\n",
        "    model_name=DEFAULT_LLM_MODEL,     # <-- Use the same LLM constant from above\n",
        "    device=\"cuda\",                    # \"cuda\" or \"cpu\"\n",
        "    max_new_tokens=300                # <-- You can increase this if responses are too short\n",
        ")\n",
        "\n",
        "# Wrap your generator for LangChain compatibility\n",
        "local_llm = HuggingFacePipeline(pipeline=generator)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.011Z"
        },
        "id": "C97jGDKVEbwX",
        "outputId": "35c9a6e3-abe3-4a5d-e39d-040b6cb38b0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original used\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        }
      ],
      "execution_count": 169
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: INITIALIZE RAG PIPELINE\n",
        "\n",
        "rag = RAGPipeline(\n",
        "    document_dir=DEFAULT_DOCUMENT_DIR,       # <-- uses constant\n",
        "    embedding_model=DEFAULT_EMBEDDING_MODEL,\n",
        "    llm_model=DEFAULT_LLM_MODEL,\n",
        "    chunk_size=DEFAULT_CHUNK_SIZE,\n",
        "    chunk_overlap=DEFAULT_CHUNK_OVERLAP,\n",
        "    device=\"cuda\"\n",
        ")\n",
        "rag"
      ],
      "metadata": {
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.011Z"
        },
        "id": "oZT87eroxLEM",
        "trusted": true,
        "outputId": "3d0fba0d-c829-417d-fc96-0bed0f599dde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.RAGPipeline at 0x7b4f5c3b43d0>"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ],
      "execution_count": 170
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: LOAD DOCUMENTS & PREPARE SYSTEM\n",
        "\n",
        "rag.load_and_process_documents()\n",
        "rag.initialize_retrieval()\n",
        "rag.initialize_llm()  # This will use Llama model defined above"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.012Z"
        },
        "id": "Z6PZiqMbEbwe",
        "outputId": "b44a957e-47d9-4b0c-e07b-dddf3a9e64d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loading documents...\n",
            " Loaded 101 document pages.\n",
            " Chunking documents...\n",
            " Created 256 chunks.\n",
            " Creating vector store...\n",
            " Creating BM25 index...\n",
            " Loading LLM...\n",
            "Original used\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        }
      ],
      "execution_count": 171
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: ASK A SINGLE QUESTION\n",
        "\n",
        "questions = [\n",
        "    \"What is Dynamic Programming?\"\n",
        "    # \"Explain the matrix method in hashing\",\n",
        "    # \"What are the key concepts in amortized analysis?\"\n",
        "]\n",
        "\n",
        "# Run query with hybrid search and show result\n",
        "answer = rag.query(\n",
        "    questions[0],\n",
        "    search_type=DEFAULT_SEARCH_TYPE,\n",
        "    k=DEFAULT_SEARCH_K\n",
        ")\n",
        "print(f\"\\n Answer:\\n{answer}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.012Z"
        },
        "id": "vUgUZ4ArEbwf",
        "outputId": "d3370362-203e-4948-a12d-36bb144135b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 55911 has 14.72 GiB memory in use. Of the allocated memory 14.51 GiB is allocated by PyTorch, and 43.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-172-2d9014fa3c48>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Run query with hybrid search and show result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m answer = rag.query(\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mquestions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0msearch_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEFAULT_SEARCH_TYPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-168-d0612855eb14>\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, question, search_type, k, semantic_weight, keyword_weight, custom_instruction)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Step 3: Generate LLM response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgenerate_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-162-5b5df5003975>\u001b[0m in \u001b[0;36mgenerate_response\u001b[0;34m(prompt, generator, width)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \"\"\"\n\u001b[1;32m     17\u001b[0m     \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m        \u001b[0;31m# Wrap prompt in chat message format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     output = generator(\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;31m# We have one or more prompts in list-of-dicts format, so this is chat mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_item\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mChat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m                     \u001b[0mchats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mChat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_inputs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#   \u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1377\u001b[0m             )\n\u001b[1;32m   1378\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1379\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1384\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1386\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1284\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1286\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1287\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generation_config\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModelOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2464\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2465\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2466\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3430\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3431\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3432\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    822\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m                 )\n\u001b[1;32m    570\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    572\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcausal_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    319\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0mattention_interface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALL_ATTENTION_FUNCTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         attn_output, attn_weights = attention_interface(\n\u001b[0m\u001b[1;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/sdpa_attention.py\u001b[0m in \u001b[0;36msdpa_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mis_causal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     attn_output = torch.nn.functional.scaled_dot_product_attention(\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 55911 has 14.72 GiB memory in use. Of the allocated memory 14.51 GiB is allocated by PyTorch, and 43.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "execution_count": 172
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: PROVIDE GROUND TRUTH FOR EVALUATION (OPTIONAL)\n",
        "\n",
        "gold_answers = [\n",
        "    \"Dynamic Programming is a technique for solving problems by breaking them into overlapping subproblems, storing intermediate results, and combining them to solve the larger problem efficiently.\"\n",
        "    # \"Dynamic Programming is a powerful technique that can be used to solve many combinatorial problems in polynomial time for which a naive approach would take exponential time. Dynamic Programming is a general approach to solving problems, much like divide-and-conquer, except that the subproblems will overlap.\"\n",
        "]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.013Z"
        },
        "id": "aaNOBSlnEbwf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: RUN AUTOMATIC EVALUATION\n",
        "\n",
        "evaluator = RAGEvaluator(rag, llm=local_llm, embeddings=embeddings)\n",
        "results_df = evaluator.evaluate_ragas(questions, gold_answers)\n",
        "print(\"\\n Evaluation Results:\\n\", results_df)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.013Z"
        },
        "id": "fr32Ich5Ebwf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize performance\n",
        "evaluator.visualize_metrics()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.014Z"
        },
        "id": "25CFCtoEEbwf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Show optimization tips\n",
        "print(\"\\n Suggestions to Improve RAG System:\")\n",
        "print(evaluator.get_optimization_insights())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.018Z"
        },
        "id": "0tc1zvFKEbwf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_results = rag.grid_search(\n",
        "    questions=questions,\n",
        "    gold_answers=gold_answers,\n",
        "    chunk_sizes=[100, 250, 500, 800, 1000],\n",
        "    k_values=[3, 4, 5],\n",
        "    search_types=[\"semantic\", \"hybrid\", \"keyword\"],\n",
        "    chunk_overlaps=[100, 200],\n",
        "    output_csv_path=\"/content/rag_grid_log.csv\"\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.018Z"
        },
        "id": "7_CDByMKxLEO",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}