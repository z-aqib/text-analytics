{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Acknowledgement\n",
        "Name: Hamna Inam, Zara Masood, Zuha Aqib     \n",
        "ERP ID: X, Y, 26106    \n",
        "Section: 10am Miss Solat    \n",
        "Date: 16-Apr-25   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Last time code executed: 2025-04-16 23:45:32\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "print(\"Last time code executed:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_date_time():\n",
        "    return \"\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports\n",
        "here we add all imports and prerequisities likeinstallations, authentications, constant definitions etc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installations\n",
        "first we need to install related packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: transformers in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 1)) (4.48.1)\n",
            "Requirement already satisfied: sentence-transformers in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 2)) (4.1.0)\n",
            "Requirement already satisfied: ragas in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 3)) (0.2.14)\n",
            "Requirement already satisfied: pypdf in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 4)) (5.4.0)\n",
            "Requirement already satisfied: langchain-community in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 5)) (0.3.21)\n",
            "Requirement already satisfied: rank_bm25 in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 6)) (0.2.2)\n",
            "Requirement already satisfied: datasets in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 7)) (3.5.0)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 8)) (3.9.2)\n",
            "Requirement already satisfied: faiss-cpu in c:\\users\\dell\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from -r requirements.txt (line 9)) (1.10.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\n",
            "ERROR: No matching distribution found for faiss-gpu\n"
          ]
        }
      ],
      "source": [
        "get_ipython().system('pip install -r requirements.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n",
        "here we import the necessary libraries and modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "# ===== Environment & Authentication =====\n",
        "import os\n",
        "from dotenv import load_dotenv, dotenv_values\n",
        "from huggingface_hub import login\n",
        "\n",
        "# ===== Core Python & Data Handling =====\n",
        "from typing import List, Tuple, Dict\n",
        "import textwrap\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ===== NLP Preprocessing =====\n",
        "import nltk\n",
        "from rank_bm25 import BM25Okapi  # BM25 retriever\n",
        "nltk.download('punkt')  # Ensure NLTK data is available\n",
        "\n",
        "# ===== LangChain - Document Loading & Splitting =====\n",
        "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "\n",
        "# ===== LangChain - Embeddings & Vector Stores =====\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# ===== Hugging Face Models & Pipelines =====\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, \n",
        "    AutoTokenizer, \n",
        "    pipeline\n",
        ")\n",
        "from langchain.llms import (\n",
        "    HuggingFaceHub, \n",
        "    HuggingFacePipeline\n",
        ")\n",
        "\n",
        "# ===== RAG Evaluation (RAGAS) =====\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    Faithfulness,\n",
        "    AnswerRelevancy,\n",
        "    ContextRecall,\n",
        "    ContextPrecision,\n",
        "    AnswerCorrectness\n",
        ")\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5.1+cpu\n"
          ]
        }
      ],
      "source": [
        "# because there was an error in this import, here it is seperatly\n",
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch version: 2.5.1+cpu\n",
            "Is >=1.13.0: True\n"
          ]
        }
      ],
      "source": [
        "from packaging import version\n",
        "\n",
        "# Check torch version (modern alternative)\n",
        "def is_torch_greater_or_equal_than_1_13():\n",
        "    return version.parse(torch.__version__) >= version.parse(\"1.13.0\")\n",
        "\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "print(f\"Is >=1.13.0: {is_torch_greater_or_equal_than_1_13()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Authentication\n",
        "here we authenticate our LLM with hugging face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token loaded: True\n"
          ]
        }
      ],
      "source": [
        "# Load environment variables from .env\n",
        "load_dotenv()\n",
        "\n",
        "# Retrieve the token\n",
        "hf_token = os.getenv(\"HUGGING_FACE_TOKEN\")\n",
        "\n",
        "print(\"Token loaded:\", hf_token is not None)\n",
        "\n",
        "# Log in to Hugging Face Hub\n",
        "login(token=hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = dotenv_values(\".env\")\n",
        "login(token=config[\"HUGGING_FACE_TOKEN\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Constants\n",
        "here we define constants that we will fine tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Constants\n",
        "DEFAULT_CHUNK_SIZE = 1000              # Max size of each text chunk\n",
        "DEFAULT_CHUNK_OVERLAP = 200            # Overlap between chunks\n",
        "DEFAULT_SEARCH_K = 3                   # Top-k results to retrieve\n",
        "DEFAULT_SEARCH_TYPE = \"hybrid\"         # Choose from: 'semantic', 'keyword', or 'hybrid'\n",
        "DEFAULT_EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # Embedding model for vector search\n",
        "DEFAULT_LLM_MODEL = \"meta-llama/Llama-3.2-1B\"                         # LLM for generating answers\n",
        "# DEFAULT_LLM_MODEL = \"deepseek-ai/DeepSeek-V3-0324\"                         # LLM for generating answers\n",
        "DEFAULT_DOCUMENT_DIR = \"/data/corpus.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1E1l1iNxLEF"
      },
      "source": [
        "# Document Processing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_documents(directory: str, glob_pattern: str = \"**/*.pdf\") -> List[Document]:\n",
        "    \"\"\"\n",
        "    Loads all PDF files from a given directory.\n",
        "    \n",
        "    Args:\n",
        "        directory: path to folder with PDF files\n",
        "        glob_pattern: pattern to match files (default = all PDFs)\n",
        "    \n",
        "    Returns:\n",
        "        List of LangChain Document objects\n",
        "    \"\"\"\n",
        "    loader = DirectoryLoader(directory, glob=glob_pattern, loader_cls=PyPDFLoader)\n",
        "    return loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chunk_documents(\n",
        "    documents: List[Document],\n",
        "    chunk_size: int = DEFAULT_CHUNK_SIZE,\n",
        "    chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,\n",
        "    separators: List[str] = None\n",
        ") -> List[Document]:\n",
        "    \"\"\"\n",
        "    Splits documents into chunks for better retrieval.\n",
        "    \n",
        "    Args:\n",
        "        documents: list of LangChain documents\n",
        "        chunk_size: size of each chunk\n",
        "        chunk_overlap: how much content overlaps between chunks\n",
        "        separators: optional list of separators for better splitting\n",
        "    \n",
        "    Returns:\n",
        "        List of chunked Document objects\n",
        "    \"\"\"\n",
        "    if separators is None:\n",
        "        # Default separators: prioritize splitting on paragraphs, then sentences, then words\n",
        "        separators = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        separators=separators\n",
        "    )\n",
        "    return text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_vector_store(\n",
        "    chunks: List[Document],\n",
        "    embedding_model: str = DEFAULT_EMBEDDING_MODEL,\n",
        "    save_path: str = None\n",
        ") -> FAISS:\n",
        "    \"\"\"\n",
        "    Creates a FAISS vector index from document chunks using specified embedding model.\n",
        "    \n",
        "    Args:\n",
        "        chunks: list of Document chunks\n",
        "        embedding_model: HuggingFace model used for embeddings\n",
        "        save_path: optional path to save the index\n",
        "    \n",
        "    Returns:\n",
        "        FAISS vector store\n",
        "    \"\"\"\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
        "    vectordb = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "    if save_path:\n",
        "        vectordb.save_local(save_path)\n",
        "\n",
        "    return vectordb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_bm25_index(chunks: List[Document]) -> BM25Okapi:\n",
        "    \"\"\"\n",
        "    Builds a keyword-based index using BM25.\n",
        "    \n",
        "    Args:\n",
        "        chunks: list of Document chunks\n",
        "    \n",
        "    Returns:\n",
        "        BM25 index\n",
        "    \"\"\"\n",
        "    texts = [chunk.page_content for chunk in chunks]                  # Get plain text\n",
        "    tokenized_texts = [text.split() for text in texts]               # Tokenize by whitespace\n",
        "    return BM25Okapi(tokenized_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-gPSeFBxLEH"
      },
      "source": [
        "# Search Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "def semantic_search(\n",
        "    query: str,\n",
        "    vectordb: FAISS,\n",
        "    k: int = DEFAULT_SEARCH_K,\n",
        "    score_threshold: float = None\n",
        ") -> List[Tuple[Document, float]]:\n",
        "    \"\"\"\n",
        "    Perform semantic search using vector similarity from FAISS.\n",
        "\n",
        "    Args:\n",
        "        query: Natural language question\n",
        "        vectordb: Vector index (FAISS)\n",
        "        k: Number of results to return\n",
        "        score_threshold: Filter out low similarity scores (optional)\n",
        "\n",
        "    Returns:\n",
        "        List of (Document, similarity_score) tuples\n",
        "    \"\"\"\n",
        "    results = vectordb.similarity_search_with_score(query, k=k)\n",
        "\n",
        "    # Optional thresholding to remove irrelevant results\n",
        "    if score_threshold is not None:\n",
        "        results = [(doc, score) for doc, score in results if score >= score_threshold]\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "def keyword_search(\n",
        "    query: str,\n",
        "    bm25_index: BM25Okapi,\n",
        "    chunks: List[Document],\n",
        "    k: int = DEFAULT_SEARCH_K,\n",
        "    score_threshold: float = None\n",
        ") -> List[Tuple[Document, float]]:\n",
        "    \"\"\"\n",
        "    Perform lexical search using BM25.\n",
        "\n",
        "    Args:\n",
        "        query: Search query string\n",
        "        bm25_index: Pre-built BM25 index\n",
        "        chunks: List of document chunks for mapping back\n",
        "        k: Top-k documents to retrieve\n",
        "        score_threshold: Optional filtering threshold for BM25 scores\n",
        "\n",
        "    Returns:\n",
        "        List of (Document, BM25_score) tuples\n",
        "    \"\"\"\n",
        "    tokenized_query = query.split()  # Basic whitespace tokenization\n",
        "    scores = bm25_index.get_scores(tokenized_query)\n",
        "\n",
        "    # Get indices of top-k documents\n",
        "    top_k_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
        "\n",
        "    results = [(chunks[i], scores[i]) for i in top_k_indices]\n",
        "\n",
        "    if score_threshold is not None:\n",
        "        results = [(doc, score) for doc, score in results if score >= score_threshold]\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-12T13:51:51.873188Z",
          "iopub.status.busy": "2025-04-12T13:51:51.872900Z",
          "iopub.status.idle": "2025-04-12T13:51:51.883518Z",
          "shell.execute_reply": "2025-04-12T13:51:51.882545Z",
          "shell.execute_reply.started": "2025-04-12T13:51:51.873168Z"
        },
        "id": "J3T0XMX0xLEH",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def hybrid_search(\n",
        "    query: str,\n",
        "    vectordb: FAISS,\n",
        "    bm25_index: BM25Okapi,\n",
        "    chunks: List[Document],\n",
        "    k: int = DEFAULT_SEARCH_K,\n",
        "    semantic_weight: float = 0.5,\n",
        "    keyword_weight: float = 0.5\n",
        ") -> List[Tuple[Document, float]]:\n",
        "    \"\"\"\n",
        "    Combine semantic and keyword search using weighted score fusion.\n",
        "\n",
        "    Args:\n",
        "        query: Natural language query\n",
        "        vectordb: FAISS vector database\n",
        "        bm25_index: BM25 keyword index\n",
        "        chunks: Document chunks (used for mapping back)\n",
        "        k: Top-k results to return\n",
        "        semantic_weight: Weight for vector similarity\n",
        "        keyword_weight: Weight for BM25 relevance\n",
        "\n",
        "    Returns:\n",
        "        List of (Document, combined_score) tuples\n",
        "    \"\"\"\n",
        "    # Step 1: Run both types of searches with larger k (to capture wider context)\n",
        "    semantic_results = semantic_search(query, vectordb, k * 2)\n",
        "    semantic_scores = {doc.page_content: score for doc, score in semantic_results}\n",
        "\n",
        "    keyword_results = keyword_search(query, bm25_index, chunks, k * 2)\n",
        "    keyword_scores = {doc.page_content: score for doc, score in keyword_results}\n",
        "\n",
        "    # Step 2: Normalize BM25 scores (they are not bounded, unlike cosine similarity)\n",
        "    max_kw_score = max(keyword_scores.values()) if keyword_scores else 1\n",
        "\n",
        "    # Step 3: Combine results\n",
        "    all_docs = set(semantic_scores.keys()).union(set(keyword_scores.keys()))\n",
        "    combined_scores = []\n",
        "\n",
        "    for doc_content in all_docs:\n",
        "        sem_score = semantic_scores.get(doc_content, 0)\n",
        "        kw_score = keyword_scores.get(doc_content, 0)\n",
        "        norm_kw_score = kw_score / max_kw_score if max_kw_score > 0 else 0\n",
        "\n",
        "        # Weighted sum of both types of scores\n",
        "        combined_score = (semantic_weight * sem_score) + (keyword_weight * norm_kw_score)\n",
        "        combined_scores.append((doc_content, combined_score))\n",
        "\n",
        "    # Step 4: Sort and return top-k\n",
        "    combined_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    top_scores = combined_scores[:k]\n",
        "\n",
        "    # Step 5: Re-map back to full Document objects using content\n",
        "    doc_lookup = {chunk.page_content: chunk for chunk in chunks}\n",
        "    results = []\n",
        "\n",
        "    for doc_content, score in top_scores:\n",
        "        if doc_content in doc_lookup:\n",
        "            results.append((doc_lookup[doc_content], score))\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q5NGOdUxLEI"
      },
      "source": [
        "# LLM functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {},
      "outputs": [],
      "source": [
        "def initialize_llm(\n",
        "    model_name: str = DEFAULT_LLM_MODEL,\n",
        "    device: str = \"cuda\",  # Use \"cpu\" if not using GPU\n",
        "    max_new_tokens: int = 300\n",
        ") -> Tuple[pipeline, any]:\n",
        "    \"\"\"\n",
        "    Loads a language model pipeline for text generation.\n",
        "\n",
        "    Args:\n",
        "        model_name: HuggingFace model repo (must support causal LM)\n",
        "        device: \"cuda\" for GPU or \"cpu\"\n",
        "        max_new_tokens: Max tokens to generate per response\n",
        "\n",
        "    Returns:\n",
        "        Tuple (generator pipeline, tokenizer)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=device,\n",
        "            torch_dtype=\"auto\",            # Uses GPU acceleration if available\n",
        "            trust_remote_code=True         # Allow custom model architectures\n",
        "        )\n",
        "        print(\"Original used\")\n",
        "    except ImportError:\n",
        "        # Fallback without device_map\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        ).to(device)\n",
        "        print(\"Edited used\")\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Format the prompt as a dialogue (user + assistant style)\n",
        "    tokenizer.chat_template = (\n",
        "        \"{% for message in messages %}\"\n",
        "        \"{% if message['role'] == 'user' %}User: {{ message['content'] }}\\n\"\n",
        "        \"{% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }}\\n\"\n",
        "        \"{% endif %}\"\n",
        "        \"{% endfor %}\"\n",
        "        \"{% if add_generation_prompt %}Assistant:{% endif %}\"\n",
        "    )\n",
        "\n",
        "    # Create a text-generation pipeline\n",
        "    generator = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        return_full_text=False,        # Only return generated part, not the full prompt\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True                 # Use sampling (stochastic generation)\n",
        "    )\n",
        "\n",
        "    return generator, tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_response(\n",
        "    prompt: str,\n",
        "    generator: pipeline,\n",
        "    width: int = 80  # For pretty-printing long outputs\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generates a response from the LLM using the prompt.\n",
        "\n",
        "    Args:\n",
        "        prompt: Full RAG-formatted prompt with question + context\n",
        "        generator: HF pipeline object\n",
        "        width: max characters per printed line (for wrapping)\n",
        "\n",
        "    Returns:\n",
        "        Answer string\n",
        "    \"\"\"\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]        # Wrap prompt in chat message format\n",
        "    output = generator(messages)                            # Call LLM\n",
        "    return textwrap.fill(output[0][\"generated_text\"], width=width)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-12T13:55:55.015353Z",
          "iopub.status.busy": "2025-04-12T13:55:55.015007Z",
          "iopub.status.idle": "2025-04-12T13:55:55.023782Z",
          "shell.execute_reply": "2025-04-12T13:55:55.022725Z",
          "shell.execute_reply.started": "2025-04-12T13:55:55.015327Z"
        },
        "id": "9GJIwleTxLEJ",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def format_rag_prompt(\n",
        "    question: str,\n",
        "    retrieved_docs: List[Document],\n",
        "    instruction: str = None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Formats the final input prompt to send to the LLM.\n",
        "\n",
        "    Args:\n",
        "        question: The user's natural language question\n",
        "        retrieved_docs: List of document chunks retrieved by search\n",
        "        instruction: Optional prompt instructions (system message)\n",
        "\n",
        "    Returns:\n",
        "        Full prompt text string\n",
        "    \"\"\"\n",
        "    # Default instructions to guide the LLM on how to use retrieved documents\n",
        "    if instruction is None:\n",
        "        instruction = \"\"\"You are an AI assistant tasked with answering questions based on retrieved knowledge.\n",
        "                    - Integrate the key points from all retrieved responses into a cohesive, well-structured answer.\n",
        "                    - If the responses are contradictory, mention the different perspectives.\n",
        "                    - If none of the retrieved responses contain relevant information, reply:\n",
        "                    \"I couldn't find a good response to your query in the database.\"\n",
        "                    \"\"\"\n",
        "\n",
        "    # Truncate each document to 1000 characters if long\n",
        "    retrieved_info = \"\\n\\n\".join(\n",
        "        f\"{i+1}Ô∏è‚É£ {doc.page_content[:1000]}...\" if len(doc.page_content) > 1000\n",
        "        else f\"{i+1}Ô∏è‚É£ {doc.page_content}\"\n",
        "        for i, doc in enumerate(retrieved_docs)\n",
        "    )\n",
        "\n",
        "    # Final structured prompt\n",
        "    return f\"\"\"\n",
        "        {instruction}\n",
        "\n",
        "        ### Retrieved Information:\n",
        "        {retrieved_info}\n",
        "\n",
        "        ### Question:\n",
        "        {question}\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07jDDqiSxLEJ"
      },
      "source": [
        "# RAG Evaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WRAPPER TO USE LANGCHAIN LLM IN RAGAS\n",
        "\n",
        "class HuggingFaceLLM:\n",
        "    \"\"\"\n",
        "    Simple wrapper for using HuggingFaceHub with RAGAS evaluation.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model = HuggingFaceHub(repo_id=model_name)\n",
        "\n",
        "    def __call__(self, prompt: str) -> str:\n",
        "        return self.model(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAFXSDHQxLEK",
        "outputId": "b3c7855e-689a-4381-ebdb-06ff925eb982"
      },
      "outputs": [],
      "source": [
        "# EVALUATOR CLASS\n",
        "\n",
        "class RAGEvaluator:\n",
        "    \"\"\"\n",
        "    Performs automatic evaluation of RAG responses using Ragas metrics.\n",
        "    Also supports result visualization and optimization insights.\n",
        "    \"\"\"\n",
        "    def __init__(self, pipeline, llm, embeddings):\n",
        "        self.pipeline = pipeline         # RAGPipeline object\n",
        "        self.embeddings = embeddings     # HuggingFaceEmbeddings instance\n",
        "\n",
        "        # Use passed LLM, or initialize default\n",
        "        if isinstance(llm, str):\n",
        "            self.llm = HuggingFaceHub(repo_id=llm)\n",
        "        else:\n",
        "            self.llm = llm or HuggingFaceHub(repo_id=DEFAULT_LLM_MODEL)\n",
        "\n",
        "        # Internal result tracking\n",
        "        self.results = []\n",
        "\n",
        "    def evaluate_ragas(self, questions: list, gold_answers: list = None):\n",
        "        \"\"\"\n",
        "        Run Ragas evaluation across all questions.\n",
        "\n",
        "        Args:\n",
        "            questions: List of input questions\n",
        "            gold_answers: Reference answers (optional)\n",
        "\n",
        "        Returns:\n",
        "            DataFrame of results\n",
        "        \"\"\"\n",
        "        all_rows = []\n",
        "\n",
        "        for question, gold_answer in zip(questions, gold_answers or [None]*len(questions)):\n",
        "            # Run full RAG query\n",
        "            answer = self.pipeline.query(question)\n",
        "\n",
        "            # Get context used in the answer\n",
        "            contexts = [doc.page_content for doc in self.pipeline.get_last_retrieved_docs()]\n",
        "\n",
        "            # Prepare a single sample for evaluation\n",
        "            data = {\n",
        "                \"question\": [question],\n",
        "                \"answer\": [answer],\n",
        "                \"contexts\": [contexts]\n",
        "            }\n",
        "            if gold_answer:\n",
        "                data[\"ground_truth\"] = [gold_answer]\n",
        "\n",
        "            dataset = Dataset.from_dict(data)\n",
        "\n",
        "            # Select metrics to compute\n",
        "            metrics = [Faithfulness(), AnswerRelevancy(), ContextRecall(), ContextPrecision()]\n",
        "            if gold_answer:\n",
        "                metrics.append(AnswerCorrectness())\n",
        "\n",
        "            # Run the evaluation\n",
        "            result = evaluate(dataset, metrics=metrics, llm=self.llm, embeddings=self.embeddings)\n",
        "\n",
        "            # Convert to DataFrame and store\n",
        "            row = result.to_pandas()\n",
        "            row[\"question\"] = question\n",
        "            row[\"retrieved_docs\"] = len(contexts)\n",
        "            all_rows.append(row)\n",
        "\n",
        "        # Combine all rows into one DataFrame\n",
        "        self.results = pd.concat(all_rows, ignore_index=True)\n",
        "        return self.results\n",
        "\n",
        "    def visualize_metrics(self):\n",
        "        \"\"\"\n",
        "        Visualize average metric scores and context retrieval stats.\n",
        "        \"\"\"\n",
        "        if self.results is None or self.results.empty:\n",
        "            raise ValueError(\"No evaluation results found. Run evaluate_ragas() first.\")\n",
        "\n",
        "        # Plot main metrics\n",
        "        metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']\n",
        "        if 'answer_correctness' in self.results.columns:\n",
        "            metrics.append('answer_correctness')\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        self.results[metrics].mean().plot(kind='bar', color='lightblue')\n",
        "        plt.title(\"üîç Average RAG Evaluation Metrics\")\n",
        "        plt.ylabel(\"Score (0 to 1)\")\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "        plt.show()\n",
        "\n",
        "        # Plot document retrieval counts\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        self.results['retrieved_docs'].value_counts().sort_index().plot(kind='bar', color='lightgreen')\n",
        "        plt.title(\"üìÑ Number of Context Chunks Retrieved Per Query\")\n",
        "        plt.xlabel(\"Number of Chunks\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "        plt.show()\n",
        "\n",
        "    def get_optimization_insights(self):\n",
        "        \"\"\"\n",
        "        Analyze weak metrics and recommend strategies to improve RAG performance.\n",
        "        \"\"\"\n",
        "        if self.results is None or self.results.empty:\n",
        "            return \"No evaluation results available.\"\n",
        "\n",
        "        insights = []\n",
        "        df = self.results\n",
        "\n",
        "        # Faithfulness issues (hallucination)\n",
        "        if df['faithfulness'].mean() < 0.7:\n",
        "            insights.append(\"‚ö†Ô∏è Faithfulness is low ‚Äî possible hallucinations.\")\n",
        "            insights.append(\"üîß Try increasing chunk overlap or improving retrieval relevance.\")\n",
        "\n",
        "        # Context recall issues (missing info)\n",
        "        if df['context_recall'].mean() < 0.6:\n",
        "            insights.append(\"‚ö†Ô∏è Low context recall ‚Äî relevant info may be missed.\")\n",
        "            insights.append(\"üîß Consider using hybrid retrieval or adjusting chunk size.\")\n",
        "\n",
        "        # Precision issues (irrelevant info)\n",
        "        if df['context_precision'].mean() < 0.6:\n",
        "            insights.append(\"‚ö†Ô∏è Low context precision ‚Äî too much irrelevant context.\")\n",
        "            insights.append(\"üîß Use better embeddings or rerank retrieved chunks.\")\n",
        "\n",
        "        # Relevance issues (answer not matching question)\n",
        "        if df['answer_relevancy'].mean() < 0.7:\n",
        "            insights.append(\"‚ö†Ô∏è Low answer relevancy ‚Äî answers not matching question.\")\n",
        "            insights.append(\"üîß Refine your prompts or improve chunk matching.\")\n",
        "\n",
        "        # Optional: Correctness based on gold answers\n",
        "        if 'answer_correctness' in df.columns and df['answer_correctness'].mean() < 0.7:\n",
        "            insights.append(\"‚ö†Ô∏è Low correctness ‚Äî answers differ from references.\")\n",
        "            insights.append(\"üîß Try different LLMs or use post-editing.\")\n",
        "\n",
        "        return \"\\n\".join(insights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1Lfz6KJxLEL"
      },
      "source": [
        "# RAG Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-12T13:55:59.987276Z",
          "iopub.status.busy": "2025-04-12T13:55:59.986917Z",
          "iopub.status.idle": "2025-04-12T13:56:00.001551Z",
          "shell.execute_reply": "2025-04-12T13:56:00.000552Z",
          "shell.execute_reply.started": "2025-04-12T13:55:59.987248Z"
        },
        "id": "ebUXkzJ5xLEL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# MAIN RAG PIPELINE\n",
        "\n",
        "class RAGPipeline:\n",
        "    \"\"\"\n",
        "    This is the central class that handles:\n",
        "    - Loading and chunking documents\n",
        "    - Initializing vector and keyword search\n",
        "    - Running queries\n",
        "    - Generating responses from the LLM\n",
        "    - Running full experiment sweeps\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        document_dir: str,\n",
        "        embedding_model: str = DEFAULT_EMBEDDING_MODEL,\n",
        "        llm_model: str = DEFAULT_LLM_MODEL,\n",
        "        chunk_size: int = DEFAULT_CHUNK_SIZE,\n",
        "        chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,\n",
        "        device: str = \"cuda\"\n",
        "    ):\n",
        "        self.document_dir = document_dir\n",
        "        self.embedding_model = embedding_model\n",
        "        self.llm_model = llm_model\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.device = device\n",
        "\n",
        "        # To store runtime state\n",
        "        self.documents = None\n",
        "        self.chunks = None\n",
        "        self.vectordb = None\n",
        "        self.bm25_index = None\n",
        "        self.llm = None\n",
        "        self.tokenizer = None\n",
        "        self.last_retrieved_docs = None  # For evaluation traceability\n",
        "\n",
        "\n",
        "    # ==========================\n",
        "    # LOAD & CHUNK DOCUMENTS\n",
        "    # ==========================\n",
        "    def load_and_process_documents(self):\n",
        "        \"\"\"\n",
        "        Loads PDF documents and splits them into overlapping chunks.\n",
        "        \"\"\"\n",
        "        print(\"üìÑ Loading documents...\")\n",
        "        self.documents = load_documents(self.document_dir)\n",
        "        print(f\"‚úÖ Loaded {len(self.documents)} document pages.\")\n",
        "\n",
        "        print(\"ü™ì Chunking documents...\")\n",
        "        self.chunks = chunk_documents(\n",
        "            self.documents,\n",
        "            chunk_size=self.chunk_size,\n",
        "            chunk_overlap=self.chunk_overlap\n",
        "        )\n",
        "        print(f\"‚úÖ Created {len(self.chunks)} chunks.\")\n",
        "\n",
        "        # Add unique IDs to chunks for tracking\n",
        "        for i, chunk in enumerate(self.chunks):\n",
        "            chunk.metadata[\"chunk_id\"] = i\n",
        "\n",
        "\n",
        "    # ==========================\n",
        "    # INITIALIZE RETRIEVAL\n",
        "    # ==========================\n",
        "    def initialize_retrieval(self):\n",
        "        \"\"\"\n",
        "        Builds vector store and keyword index for retrieval.\n",
        "        \"\"\"\n",
        "        if not self.chunks:\n",
        "            raise ValueError(\"‚ùå No chunks found. Run load_and_process_documents() first.\")\n",
        "\n",
        "        print(\"üì¶ Creating vector store...\")\n",
        "        self.vectordb = create_vector_store(self.chunks, self.embedding_model)\n",
        "\n",
        "        print(\"üîé Creating BM25 index...\")\n",
        "        self.bm25_index = create_bm25_index(self.chunks)\n",
        "\n",
        "\n",
        "    # ==========================\n",
        "    # INITIALIZE LLM\n",
        "    # ==========================\n",
        "    def initialize_llm(self):\n",
        "        \"\"\"\n",
        "        Loads the chosen LLM and tokenizer from HuggingFace.\n",
        "        \"\"\"\n",
        "        print(\"ü§ñ Loading LLM...\")\n",
        "        self.llm, self.tokenizer = initialize_llm(self.llm_model, self.device)\n",
        "\n",
        "\n",
        "    # ==========================\n",
        "    # GET LAST RETRIEVED CHUNKS\n",
        "    # ==========================\n",
        "    def get_last_retrieved_docs(self):\n",
        "        \"\"\"\n",
        "        Returns the last set of retrieved document chunks (used in evaluation).\n",
        "        \"\"\"\n",
        "        if self.last_retrieved_docs is None:\n",
        "            raise ValueError(\"‚ùå No retrievals done yet.\")\n",
        "        return self.last_retrieved_docs\n",
        "\n",
        "\n",
        "    # ==========================\n",
        "    # MAIN QUERY FUNCTION\n",
        "    # ==========================\n",
        "    def query(\n",
        "        self,\n",
        "        question: str,\n",
        "        search_type: str = DEFAULT_SEARCH_TYPE,  # semantic / keyword / hybrid\n",
        "        k: int = DEFAULT_SEARCH_K,\n",
        "        semantic_weight: float = 0.5,\n",
        "        keyword_weight: float = 0.5,\n",
        "        custom_instruction: str = None\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Executes a full query through the pipeline:\n",
        "        - Retrieves chunks\n",
        "        - Formats prompt\n",
        "        - Calls LLM\n",
        "        - Returns answer\n",
        "        \"\"\"\n",
        "        if not self.vectordb or not self.bm25_index:\n",
        "            raise ValueError(\"‚ùå Retrieval systems not ready. Run initialize_retrieval().\")\n",
        "        if not self.llm:\n",
        "            raise ValueError(\"‚ùå LLM not initialized. Run initialize_llm().\")\n",
        "\n",
        "        # Step 1: Retrieve relevant documents\n",
        "        if search_type == \"semantic\":\n",
        "            results = semantic_search(question, self.vectordb, k)\n",
        "        elif search_type == \"keyword\":\n",
        "            results = keyword_search(question, self.bm25_index, self.chunks, k)\n",
        "        elif search_type == \"hybrid\":\n",
        "            results = hybrid_search(\n",
        "                question,\n",
        "                self.vectordb,\n",
        "                self.bm25_index,\n",
        "                self.chunks,\n",
        "                k,\n",
        "                semantic_weight,\n",
        "                keyword_weight\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"‚ùå Unknown search type: {search_type}\")\n",
        "\n",
        "        retrieved_docs = [doc for doc, _ in results]\n",
        "        self.last_retrieved_docs = retrieved_docs\n",
        "\n",
        "        # Step 2: Format prompt\n",
        "        prompt = format_rag_prompt(question, retrieved_docs, custom_instruction)\n",
        "\n",
        "        # Step 3: Generate LLM response\n",
        "        return generate_response(prompt, self.llm)\n",
        "\n",
        "\n",
        "    # ==========================\n",
        "    # EXPERIMENTATION FUNCTION\n",
        "    # ==========================\n",
        "    def experiment(\n",
        "        self,\n",
        "        questions: List[str],\n",
        "        gold_answers: List[str],\n",
        "        chunk_sizes: List[int],\n",
        "        k_values: List[int],\n",
        "        search_types: List[str],\n",
        "        chunk_overlaps: List[int] = [0, 100, 200]\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Run multiple experiment configurations (varying chunk size, k, search type).\n",
        "\n",
        "        Args:\n",
        "            questions: list of input queries\n",
        "            gold_answers: reference answers (used in evaluation)\n",
        "            chunk_sizes: different chunk sizes to test\n",
        "            k_values: number of results to retrieve\n",
        "            search_types: list of retrieval modes\n",
        "            chunk_overlaps: amount of content overlap between chunks\n",
        "\n",
        "        Returns:\n",
        "            A dict of results by config\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "\n",
        "        for chunk_size in chunk_sizes:\n",
        "            for chunk_overlap in chunk_overlaps:\n",
        "                self.chunk_size = chunk_size\n",
        "                self.chunk_overlap = chunk_overlap\n",
        "\n",
        "                print(f\"\\n‚öôÔ∏è  Testing: chunk={chunk_size}, overlap={chunk_overlap}\")\n",
        "                self.load_and_process_documents()\n",
        "                self.initialize_retrieval()\n",
        "\n",
        "                evaluator = RAGEvaluator(self, self.llm, HuggingFaceEmbeddings(model_name=self.embedding_model))\n",
        "\n",
        "                for search_type in search_types:\n",
        "                    for k in k_values:\n",
        "                        config_name = f\"chunk{chunk_size}_overlap{chunk_overlap}_{search_type}_k{k}\"\n",
        "                        results[config_name] = {}\n",
        "\n",
        "                        for question in questions:\n",
        "                            try:\n",
        "                                answer = self.query(\n",
        "                                    question,\n",
        "                                    search_type=search_type,\n",
        "                                    k=k\n",
        "                                )\n",
        "                                results[config_name][question] = answer\n",
        "\n",
        "                                # Run evaluation (with gold answer)\n",
        "                                eval_result = evaluator.evaluate_ragas([question], [gold_answers[0]])\n",
        "                                print(eval_result[['faithfulness', 'answer_relevancy']])\n",
        "\n",
        "                                # Visualization + optimization\n",
        "                                evaluator.visualize_metrics()\n",
        "                                print(\"\\nüß† Optimization Suggestions:\")\n",
        "                                print(evaluator.get_optimization_insights())\n",
        "\n",
        "                            except Exception as e:\n",
        "                                results[config_name][question] = f\"‚ùå Error: {str(e)}\"\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Running\n",
        "here we now run the code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef87e54a91b64669ac787ce7e359542f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   2%|1         | 41.9M/2.47G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# STEP 1: CREATE EMBEDDINGS & LLM WRAPPER\n",
        "\n",
        "# Use the same model you defined as DEFAULT_EMBEDDING_MODEL\n",
        "embeddings = HuggingFaceEmbeddings(model_name=DEFAULT_EMBEDDING_MODEL)\n",
        "\n",
        "# Initialize LLM pipeline\n",
        "generator, tokenizer = initialize_llm(\n",
        "    model_name=DEFAULT_LLM_MODEL,     # <-- Use the same LLM constant from above\n",
        "    device=\"cuda\",                    # \"cuda\" or \"cpu\"\n",
        "    max_new_tokens=300                # <-- You can increase this if responses are too short\n",
        ")\n",
        "\n",
        "# Wrap your generator for LangChain compatibility\n",
        "local_llm = HuggingFacePipeline(pipeline=generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-12T13:52:22.282784Z",
          "iopub.status.busy": "2025-04-12T13:52:22.282485Z",
          "iopub.status.idle": "2025-04-12T13:52:27.038649Z",
          "shell.execute_reply": "2025-04-12T13:52:27.037747Z",
          "shell.execute_reply.started": "2025-04-12T13:52:22.282763Z"
        },
        "id": "oZT87eroxLEM",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# STEP 2: INITIALIZE RAG PIPELINE\n",
        "\n",
        "rag = RAGPipeline(\n",
        "    document_dir=DEFAULT_DOCUMENT_DIR,       # <-- uses constant\n",
        "    embedding_model=DEFAULT_EMBEDDING_MODEL,\n",
        "    llm_model=DEFAULT_LLM_MODEL,\n",
        "    chunk_size=DEFAULT_CHUNK_SIZE,\n",
        "    chunk_overlap=DEFAULT_CHUNK_OVERLAP,\n",
        "    device=\"cuda\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 3: LOAD DOCUMENTS & PREPARE SYSTEM\n",
        "\n",
        "rag.load_and_process_documents()\n",
        "rag.initialize_retrieval()\n",
        "rag.initialize_llm()  # This will use Llama model defined above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 4: ASK A SINGLE QUESTION\n",
        "\n",
        "questions = [\n",
        "    \"What is Dynamic Programming?\"\n",
        "    # \"Explain the matrix method in hashing\",\n",
        "    # \"What are the key concepts in amortized analysis?\"\n",
        "]\n",
        "\n",
        "# Run query with hybrid search and show result\n",
        "answer = rag.query(\n",
        "    questions[0],\n",
        "    search_type=DEFAULT_SEARCH_TYPE,\n",
        "    k=DEFAULT_SEARCH_K\n",
        ")\n",
        "print(f\"\\nüß† Answer:\\n{answer}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 5: PROVIDE GROUND TRUTH FOR EVALUATION (OPTIONAL)\n",
        "\n",
        "gold_answers = [\n",
        "    \"Dynamic Programming is a technique for solving problems by breaking them into overlapping subproblems, storing intermediate results, and combining them to solve the larger problem efficiently.\"\n",
        "    # \"Dynamic Programming is a powerful technique that can be used to solve many combinatorial problems in polynomial time for which a naive approach would take exponential time. Dynamic Programming is a general approach to solving problems, much like ‚Äúdivide-and-conquer‚Äù, except that the subproblems will overlap.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STEP 6: RUN AUTOMATIC EVALUATION\n",
        "\n",
        "evaluator = RAGEvaluator(rag, llm=local_llm, embeddings=embeddings)\n",
        "results_df = evaluator.evaluate_ragas(questions, gold_answers)\n",
        "print(\"\\nüìä Evaluation Results:\\n\", results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize performance\n",
        "evaluator.visualize_metrics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Show optimization tips\n",
        "print(\"\\nüõ†Ô∏è Suggestions to Improve RAG System:\")\n",
        "print(evaluator.get_optimization_insights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-04-09T19:14:52.605849Z",
          "iopub.status.busy": "2025-04-09T19:14:52.605533Z"
        },
        "id": "7_CDByMKxLEO",
        "outputId": "603c57f0-e91b-4827-c259-90a7b7735bf7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# experiment_results = rag.experiment(\n",
        "#     questions=questions,\n",
        "#     gold_answers=gold_answers,\n",
        "#     chunk_sizes=[800, 1000],\n",
        "#     chunk_overlaps=[100, 200],\n",
        "#     search_types=[\"semantic\", \"hybrid\"],\n",
        "#     k_values=[3, 5] )\n",
        "\n",
        "# # Print experiment results\n",
        "# for config, answers in experiment_results.items():\n",
        "#     print(f\"\\nConfiguration: {config}\")\n",
        "#     for question, answer in answers.items():\n",
        "#         print(f\"\\nQ: {question}\")\n",
        "#         print(f\"A: {answer[:200]}...\")  # Print first 200 chars of answer\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 7098504,
          "sourceId": 11345061,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30919,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0efffe46e4c643f6b5c4d802819036ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2fe1553615e4464b8fa20e2c21bdf2fc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41fd2a4bf25949aaba088d8c56fdc3f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42df8db8a14642058d9b2255b0484497",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_49082566a2ac43a0b8e446809ce5f89b",
            "value": "Evaluating:‚Äá100%"
          }
        },
        "42df8db8a14642058d9b2255b0484497": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49082566a2ac43a0b8e446809ce5f89b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b3fc3dddba44c07b2c3be73cb3034a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e5948865f464a0aa72222fc4776c5fd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6809c0a0878d421ba3715ab62985bf6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fe1553615e4464b8fa20e2c21bdf2fc",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fccd27c6902b403cb6dca4fb3732851c",
            "value": "‚Äá5/5‚Äá[01:48&lt;00:00,‚Äá12.67s/it]"
          }
        },
        "dfd48c084331419091adcd5d85872a82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b3fc3dddba44c07b2c3be73cb3034a0",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0efffe46e4c643f6b5c4d802819036ac",
            "value": 5
          }
        },
        "fccd27c6902b403cb6dca4fb3732851c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd5bf3101ceb462998b0bc310efe1bf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_41fd2a4bf25949aaba088d8c56fdc3f0",
              "IPY_MODEL_dfd48c084331419091adcd5d85872a82",
              "IPY_MODEL_6809c0a0878d421ba3715ab62985bf6d"
            ],
            "layout": "IPY_MODEL_5e5948865f464a0aa72222fc4776c5fd"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
