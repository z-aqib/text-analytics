{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 11345061,
          "sourceType": "datasetVersion",
          "datasetId": 7098504
        },
        {
          "sourceId": 11439657,
          "sourceType": "datasetVersion",
          "datasetId": 7166003
        },
        {
          "sourceId": 11439662,
          "sourceType": "datasetVersion",
          "datasetId": 7166008
        }
      ],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Acknowledgement\n",
        "Name: Hamna Inam, Zara Masood, Zuha Aqib     \n",
        "ERP ID: X, Y, 26106    \n",
        "Section: 10am Miss Solat    \n",
        "Date: 16-Apr-25   "
      ],
      "metadata": {
        "id": "j1D1m381Ebv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "print(\"Last time code executed:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-18T17:53:20.035311Z",
          "iopub.execute_input": "2025-04-18T17:53:20.035596Z",
          "iopub.status.idle": "2025-04-18T17:53:20.043438Z",
          "shell.execute_reply.started": "2025-04-18T17:53:20.035566Z",
          "shell.execute_reply": "2025-04-18T17:53:20.042574Z"
        },
        "id": "pBYgXkK-EbwD",
        "outputId": "0a272df7-d9ee-4a86-ea6b-2b275c33531f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last time code executed: 2025-04-18 18:07:26\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "def print_date_time():\n",
        "    return \"\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-18T17:53:20.044281Z",
          "iopub.execute_input": "2025-04-18T17:53:20.044532Z",
          "iopub.status.idle": "2025-04-18T17:53:20.059792Z",
          "shell.execute_reply.started": "2025-04-18T17:53:20.044507Z",
          "shell.execute_reply": "2025-04-18T17:53:20.059250Z"
        },
        "id": "CXAQ2BwVEbwH"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n",
        "here we add all imports and prerequisities like installations, authentications, constant definitions etc"
      ],
      "metadata": {
        "id": "4d8Us0HHEbwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations\n",
        "first we need to install related packages"
      ],
      "metadata": {
        "id": "sm79L72bEbwJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adding this because kaggle ke maslay\n",
        "!pip uninstall -y langchain langchain-core langchain-community langchain-openai ragas pydantic -y"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-18T17:53:20.061393Z",
          "iopub.execute_input": "2025-04-18T17:53:20.061583Z",
          "iopub.status.idle": "2025-04-18T17:53:27.621985Z",
          "shell.execute_reply.started": "2025-04-18T17:53:20.061567Z",
          "shell.execute_reply": "2025-04-18T17:53:27.621276Z"
        },
        "id": "1wF3_bZqEbwK",
        "outputId": "e5475eb4-e6d3-4a0c-ba82-8092e28c95bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: langchain 0.3.23\n",
            "Uninstalling langchain-0.3.23:\n",
            "  Successfully uninstalled langchain-0.3.23\n",
            "Found existing installation: langchain-core 0.3.52\n",
            "Uninstalling langchain-core-0.3.52:\n",
            "  Successfully uninstalled langchain-core-0.3.52\n",
            "\u001b[33mWARNING: Skipping langchain-community as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping langchain-openai as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping ragas as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: pydantic 2.11.3\n",
            "Uninstalling pydantic-2.11.3:\n",
            "  Successfully uninstalled pydantic-2.11.3\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "get_ipython().system('pip install transformers')\n",
        "get_ipython().system('pip install sentence-transformers')\n",
        "get_ipython().system('pip install pypdf')\n",
        "get_ipython().system('pip install pymupdf')\n",
        "get_ipython().system('pip install rank_bm25')\n",
        "get_ipython().system('pip install datasets')\n",
        "get_ipython().system('pip install matplotlib')\n",
        "get_ipython().system('pip install faiss')\n",
        "get_ipython().system('pip install faiss-cpu')\n",
        "get_ipython().system('pip install faiss-gpu')\n",
        "get_ipython().system('pip install --upgrade pypdf')\n",
        "\n",
        "# 2. Then install\n",
        "!pip install \"langchain==0.2.0\"\n",
        "!pip install \"langchain-core==0.2.0\"\n",
        "!pip install \"langchain-community==0.2.0\"\n",
        "!pip install \"langchain-text-splitters==0.2.1\"\n",
        "!pip install \"langchain-openai==0.1.0\"\n",
        "!pip install \"pydantic==2.6.4\"\n",
        "!pip install \"ragas==0.2.14\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-18T17:53:27.623072Z",
          "iopub.execute_input": "2025-04-18T17:53:27.623409Z",
          "iopub.status.idle": "2025-04-18T17:55:56.234933Z",
          "shell.execute_reply.started": "2025-04-18T17:53:27.623376Z",
          "shell.execute_reply": "2025-04-18T17:55:56.234196Z"
        },
        "id": "RgO_gHo1EbwL",
        "outputId": "01047e15-5656-4aba-975e-a2b0c17b4530",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.14.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.4.0\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.5\n",
            "Collecting rank_bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank_bm25) (2.0.2)\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Installing collected packages: rank_bm25\n",
            "Successfully installed rank_bm25-0.2.2\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-gpu (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-gpu\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Collecting langchain==0.2.0\n",
            "  Downloading langchain-0.2.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (2.0.40)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (3.11.15)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.2.0)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain==0.2.0)\n",
            "  Downloading langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain==0.2.0)\n",
            "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.2.0)\n",
            "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting numpy<2,>=1 (from langchain==0.2.0)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<3,>=1 (from langchain==0.2.0)\n",
            "  Downloading pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain==0.2.0) (2.32.3)\n",
            "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.2.0)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.0) (1.19.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.2.0)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.2.0)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain==0.2.0) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain==0.2.0) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain==0.2.0) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.2.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.2.0) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain==0.2.0) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain==0.2.0) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.2.0) (3.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain==0.2.0) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.2.0)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain==0.2.0) (1.3.1)\n",
            "Downloading langchain-0.2.0-py3-none-any.whl (973 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m973.7/973.7 kB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_core-0.2.43-py3-none-any.whl (397 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.1/397.1 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.8/311.8 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m116.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.6/443.6 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: tenacity, numpy, mypy-extensions, marshmallow, typing-inspect, pydantic, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.1.2\n",
            "    Uninstalling tenacity-9.1.2:\n",
            "      Successfully uninstalled tenacity-9.1.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: langsmith\n",
            "    Found existing installation: langsmith 0.3.31\n",
            "    Uninstalling langsmith-0.3.31:\n",
            "      Successfully uninstalled langsmith-0.3.31\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.3.8\n",
            "    Uninstalling langchain-text-splitters-0.3.8:\n",
            "      Successfully uninstalled langchain-text-splitters-0.3.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dataclasses-json-0.6.7 langchain-0.2.0 langchain-core-0.2.43 langchain-text-splitters-0.2.4 langsmith-0.1.147 marshmallow-3.26.1 mypy-extensions-1.0.0 numpy-1.26.4 pydantic-2.11.3 tenacity-8.5.0 typing-inspect-0.9.0\n",
            "Collecting langchain-core==0.2.0\n",
            "  Downloading langchain_core-0.2.0-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.2.0) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.2.0) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.2.0) (0.1.147)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core==0.2.0)\n",
            "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.2.0) (2.11.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core==0.2.0) (8.5.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core==0.2.0) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core==0.2.0) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core==0.2.0) (3.10.16)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core==0.2.0) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core==0.2.0) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core==0.2.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core==0.2.0) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core==0.2.0) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core==0.2.0) (0.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core==0.2.0) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core==0.2.0) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core==0.2.0) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core==0.2.0) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core==0.2.0) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core==0.2.0) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core==0.2.0) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-core==0.2.0) (1.3.1)\n",
            "Downloading langchain_core-0.2.0-py3-none-any.whl (307 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.9/307.9 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: packaging, langchain-core\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.2.43\n",
            "    Uninstalling langchain-core-0.2.43:\n",
            "      Successfully uninstalled langchain-core-0.2.43\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-text-splitters 0.2.4 requires langchain-core<0.3.0,>=0.2.38, but you have langchain-core 0.2.0 which is incompatible.\n",
            "google-cloud-bigquery 3.31.0 requires packaging>=24.2.0, but you have packaging 23.2 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-core-0.2.0 packaging-23.2\n",
            "Collecting langchain-community==0.2.0\n",
            "  Downloading langchain_community-0.2.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (2.0.40)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (3.11.15)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (0.2.0)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (0.2.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (0.1.147)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (2.32.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community==0.2.0) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community==0.2.0) (1.19.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.0) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.0) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community==0.2.0) (0.2.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain<0.3.0,>=0.2.0->langchain-community==0.2.0) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.0) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.0) (23.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.0) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.0) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community==0.2.0) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.2.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.2.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.2.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community==0.2.0) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.2.0) (3.2.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community==0.2.0) (4.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.0) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.0) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.0) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-community==0.2.0) (3.0.0)\n",
            "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain-community==0.2.0)\n",
            "  Using cached langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community==0.2.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community==0.2.0) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain-community==0.2.0) (0.4.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.0) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain-community==0.2.0) (1.3.1)\n",
            "Downloading langchain_community-0.2.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached langchain_core-0.2.43-py3-none-any.whl (397 kB)\n",
            "Installing collected packages: langchain-core, langchain-community\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.2.0\n",
            "    Uninstalling langchain-core-0.2.0:\n",
            "      Successfully uninstalled langchain-core-0.2.0\n",
            "Successfully installed langchain-community-0.2.0 langchain-core-0.2.43\n",
            "Collecting langchain-text-splitters==0.2.1\n",
            "  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain-text-splitters==0.2.1) (0.2.43)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-text-splitters==0.2.1) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-text-splitters==0.2.1) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-text-splitters==0.2.1) (0.1.147)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-text-splitters==0.2.1) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-text-splitters==0.2.1) (2.11.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-text-splitters==0.2.1) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain-text-splitters==0.2.1) (4.13.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters==0.2.1) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters==0.2.1) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters==0.2.1) (3.10.16)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters==0.2.1) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.112->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters==0.2.1) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters==0.2.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters==0.2.1) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters==0.2.1) (0.4.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters==0.2.1) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters==0.2.1) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters==0.2.1) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters==0.2.1) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters==0.2.1) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.112->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters==0.2.1) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.112->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters==0.2.1) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain-core<0.3.0,>=0.2.0->langchain-text-splitters==0.2.1) (1.3.1)\n",
            "Downloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: langchain-text-splitters\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.2.4\n",
            "    Uninstalling langchain-text-splitters-0.2.4:\n",
            "      Successfully uninstalled langchain-text-splitters-0.2.4\n",
            "Successfully installed langchain-text-splitters-0.2.1\n",
            "Collecting langchain-openai==0.1.0\n",
            "  Downloading langchain_openai-0.1.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting langchain-core<0.2.0,>=0.1.33 (from langchain-openai==0.1.0)\n",
            "  Downloading langchain_core-0.1.53-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai==0.1.0) (1.75.0)\n",
            "Collecting tiktoken<1,>=0.5.2 (from langchain-openai==0.1.0)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (0.1.147)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (23.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (2.11.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (8.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (4.13.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.5.2->langchain-openai==0.1.0) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.5.2->langchain-openai==0.1.0) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.10.0->langchain-openai==0.1.0) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (1.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.33->langchain-openai==0.1.0) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai==0.1.0) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.5.2->langchain-openai==0.1.0) (2.3.0)\n",
            "\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'langchain-openai' candidate (version 0.1.0 at https://files.pythonhosted.org/packages/6d/f4/bea64066e93a4980f0d8352af733f950ff0eea98cca5000a4ca1ff2ae2b8/langchain_openai-0.1.0-py3-none-any.whl (from https://pypi.org/simple/langchain-openai/) (requires-python:<4.0,>=3.8.1))\n",
            "Reason for being yanked: Contained a regression that prevented passing ToolMessage in the input to ChatOpenAI, fixed in 0.1.1\u001b[0m\u001b[33m\n",
            "\u001b[0mDownloading langchain_openai-0.1.0-py3-none-any.whl (32 kB)\n",
            "Downloading langchain_core-0.1.53-py3-none-any.whl (303 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.1/303.1 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain-core, langchain-openai\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.2.43\n",
            "    Uninstalling langchain-core-0.2.43:\n",
            "      Successfully uninstalled langchain-core-0.2.43\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.2.0 requires langchain-core<0.3.0,>=0.2.0, but you have langchain-core 0.1.53 which is incompatible.\n",
            "langchain-text-splitters 0.2.1 requires langchain-core<0.3.0,>=0.2.0, but you have langchain-core 0.1.53 which is incompatible.\n",
            "langchain-community 0.2.0 requires langchain-core<0.3.0,>=0.2.0, but you have langchain-core 0.1.53 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed langchain-core-0.1.53 langchain-openai-0.1.0 tiktoken-0.9.0\n",
            "Collecting pydantic==2.6.4\n",
            "  Downloading pydantic-2.6.4-py3-none-any.whl.metadata (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.1/85.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.6.4) (0.7.0)\n",
            "Collecting pydantic-core==2.16.3 (from pydantic==2.6.4)\n",
            "  Downloading pydantic_core-2.16.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.11/dist-packages (from pydantic==2.6.4) (4.13.2)\n",
            "Downloading pydantic-2.6.4-py3-none-any.whl (394 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.9/394.9 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.16.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydantic-core, pydantic\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.33.1\n",
            "    Uninstalling pydantic_core-2.33.1:\n",
            "      Successfully uninstalled pydantic_core-2.33.1\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.3\n",
            "    Uninstalling pydantic-2.11.3:\n",
            "      Successfully uninstalled pydantic-2.11.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain 0.2.0 requires langchain-core<0.3.0,>=0.2.0, but you have langchain-core 0.1.53 which is incompatible.\n",
            "langchain-text-splitters 0.2.1 requires langchain-core<0.3.0,>=0.2.0, but you have langchain-core 0.1.53 which is incompatible.\n",
            "langchain-community 0.2.0 requires langchain-core<0.3.0,>=0.2.0, but you have langchain-core 0.1.53 which is incompatible.\n",
            "albumentations 2.0.5 requires pydantic>=2.9.2, but you have pydantic 2.6.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pydantic-2.6.4 pydantic-core-2.16.3\n",
            "Collecting ragas==0.2.14\n",
            "  Downloading ragas-0.2.14-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ragas==0.2.14) (1.26.4)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from ragas==0.2.14) (3.5.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from ragas==0.2.14) (0.9.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (from ragas==0.2.14) (0.2.0)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.11/dist-packages (from ragas==0.2.14) (0.1.53)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (from ragas==0.2.14) (0.2.0)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.11/dist-packages (from ragas==0.2.14) (0.1.0)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ragas==0.2.14) (1.6.0)\n",
            "Collecting appdirs (from ragas==0.2.14)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: pydantic>=2 in /usr/local/lib/python3.11/dist-packages (from ragas==0.2.14) (2.6.4)\n",
            "Requirement already satisfied: openai>1 in /usr/local/lib/python3.11/dist-packages (from ragas==0.2.14) (1.75.0)\n",
            "Collecting diskcache>=5.6.3 (from ragas==0.2.14)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai>1->ragas==0.2.14) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai>1->ragas==0.2.14) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai>1->ragas==0.2.14) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai>1->ragas==0.2.14) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai>1->ragas==0.2.14) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai>1->ragas==0.2.14) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai>1->ragas==0.2.14) (4.13.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->ragas==0.2.14) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2->ragas==0.2.14) (2.16.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets->ragas==0.2.14) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->ragas==0.2.14) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->ragas==0.2.14) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->ragas==0.2.14) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets->ragas==0.2.14) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->ragas==0.2.14) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->ragas==0.2.14) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->ragas==0.2.14) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->ragas==0.2.14) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets->ragas==0.2.14) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->ragas==0.2.14) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->ragas==0.2.14) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain->ragas==0.2.14) (2.0.40)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain->ragas==0.2.14) (0.6.7)\n",
            "Collecting langchain-core (from ragas==0.2.14)\n",
            "  Using cached langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from langchain->ragas==0.2.14) (0.2.1)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain->ragas==0.2.14) (0.1.147)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain->ragas==0.2.14) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core->ragas==0.2.14) (1.33)\n",
            "INFO: pip is looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting langchain_openai (from ragas==0.2.14)\n",
            "  Downloading langchain_openai-0.3.14-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading langchain_openai-0.3.13-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading langchain_openai-0.3.12-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading langchain_openai-0.3.11-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading langchain_openai-0.3.10-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading langchain_openai-0.3.9-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading langchain_openai-0.3.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "INFO: pip is still looking at multiple versions of langchain-openai to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading langchain_openai-0.3.7-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading langchain_openai-0.3.6-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading langchain_openai-0.3.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading langchain_openai-0.3.4-py3-none-any.whl.metadata (2.3 kB)\n",
            "  Downloading langchain_openai-0.3.3-py3-none-any.whl.metadata (2.7 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading langchain_openai-0.3.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading langchain_openai-0.3.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading langchain_openai-0.3.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading langchain_openai-0.2.14-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading langchain_openai-0.2.13-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading langchain_openai-0.2.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading langchain_openai-0.2.11-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading langchain_openai-0.2.10-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading langchain_openai-0.2.9-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading langchain_openai-0.2.8-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading langchain_openai-0.2.7-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading langchain_openai-0.2.6-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading langchain_openai-0.2.5-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading langchain_openai-0.2.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading langchain_openai-0.2.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading langchain_openai-0.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading langchain_openai-0.2.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading langchain_openai-0.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading langchain_openai-0.1.25-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->ragas==0.2.14) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas==0.2.14) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas==0.2.14) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas==0.2.14) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas==0.2.14) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas==0.2.14) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas==0.2.14) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->ragas==0.2.14) (1.19.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai>1->ragas==0.2.14) (3.10)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain->ragas==0.2.14) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain->ragas==0.2.14) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>1->ragas==0.2.14) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai>1->ragas==0.2.14) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas==0.2.14) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas==0.2.14) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragas==0.2.14) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain->ragas==0.2.14) (1.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->ragas==0.2.14) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->ragas==0.2.14) (2.3.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain->ragas==0.2.14) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->ragas==0.2.14) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->ragas==0.2.14) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->ragas==0.2.14) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas==0.2.14) (1.17.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->ragas==0.2.14) (1.0.0)\n",
            "Downloading ragas-0.2.14-py3-none-any.whl (187 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Using cached langchain_core-0.2.43-py3-none-any.whl (397 kB)\n",
            "Downloading langchain_openai-0.1.25-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, diskcache, langchain-core, langchain_openai, ragas\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.1.53\n",
            "    Uninstalling langchain-core-0.1.53:\n",
            "      Successfully uninstalled langchain-core-0.1.53\n",
            "  Attempting uninstall: langchain_openai\n",
            "    Found existing installation: langchain-openai 0.1.0\n",
            "    Uninstalling langchain-openai-0.1.0:\n",
            "      Successfully uninstalled langchain-openai-0.1.0\n",
            "Successfully installed appdirs-1.4.4 diskcache-5.6.3 langchain-core-0.2.43 langchain_openai-0.1.25 ragas-0.2.14\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n",
        "here we import the necessary libraries and modules"
      ],
      "metadata": {
        "id": "RAKtxVv0EbwN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== Environment & Authentication =====\n",
        "import os\n",
        "import time\n",
        "import csv\n",
        "# from dotenv import load_dotenv, dotenv_values\n",
        "from huggingface_hub import login\n",
        "\n",
        "# ===== Core Python & Data Handling =====\n",
        "from typing import List, Tuple, Dict\n",
        "import textwrap\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ===== NLP Preprocessing =====\n",
        "import nltk\n",
        "from rank_bm25 import BM25Okapi  # BM25 retriever\n",
        "nltk.download('punkt')  # Ensure NLTK data is available\n",
        "\n",
        "# ===== LangChain - Document Loading & Splitting =====\n",
        "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
        "# from langchain.schema import Document\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# ===== LangChain - Embeddings & Vector Stores =====\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# ===== Hugging Face Models & Pipelines =====\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    pipeline\n",
        ")\n",
        "from langchain.llms import (\n",
        "    HuggingFaceHub,\n",
        "    HuggingFacePipeline\n",
        ")\n",
        "\n",
        "# ===== RAG Evaluation (RAGAS) =====\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    Faithfulness,\n",
        "    AnswerRelevancy,\n",
        "    ContextRecall,\n",
        "    ContextPrecision,\n",
        "    AnswerCorrectness\n",
        ")\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-18T17:56:42.738972Z",
          "iopub.execute_input": "2025-04-18T17:56:42.739182Z",
          "iopub.status.idle": "2025-04-18T17:57:23.363956Z",
          "shell.execute_reply.started": "2025-04-18T17:56:42.739162Z",
          "shell.execute_reply": "2025-04-18T17:57:23.363300Z"
        },
        "id": "22QiRs-aEbwO",
        "outputId": "7f6039a5-b384-4a27-8faa-804c12609090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-287a7bf24186>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtextwrap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "# because there was an error in this import, here it is seperatly\n",
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-18T17:57:23.364693Z",
          "iopub.execute_input": "2025-04-18T17:57:23.365314Z",
          "iopub.status.idle": "2025-04-18T17:57:23.370006Z",
          "shell.execute_reply.started": "2025-04-18T17:57:23.365287Z",
          "shell.execute_reply": "2025-04-18T17:57:23.369143Z"
        },
        "id": "ITlgZo5AEbwO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from packaging import version\n",
        "\n",
        "# Check torch version (modern alternative)\n",
        "def is_torch_greater_or_equal_than_1_13():\n",
        "    return version.parse(torch.__version__) >= version.parse(\"1.13.0\")\n",
        "\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "print(f\"Is >=1.13.0: {is_torch_greater_or_equal_than_1_13()}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-18T17:57:23.372449Z",
          "iopub.execute_input": "2025-04-18T17:57:23.372676Z",
          "iopub.status.idle": "2025-04-18T17:57:23.395458Z",
          "shell.execute_reply.started": "2025-04-18T17:57:23.372654Z",
          "shell.execute_reply": "2025-04-18T17:57:23.394701Z"
        },
        "id": "oJxxudGdEbwP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Authentication\n",
        "here we authenticate our LLM with hugging face"
      ],
      "metadata": {
        "id": "KRG7BTmKEbwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load environment variables from .env\n",
        "# load_dotenv()\n",
        "\n",
        "# # Retrieve the token\n",
        "# hf_token = os.getenv(\"HUGGING_FACE_TOKEN\")\n",
        "\n",
        "# print(\"Token loaded:\", hf_token is not None)\n",
        "\n",
        "# # Log in to Hugging Face Hub\n",
        "login(token=\"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\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-18T17:57:23.396300Z",
          "iopub.execute_input": "2025-04-18T17:57:23.396541Z",
          "iopub.status.idle": "2025-04-18T17:57:23.539520Z",
          "shell.execute_reply.started": "2025-04-18T17:57:23.396524Z",
          "shell.execute_reply": "2025-04-18T17:57:23.538735Z"
        },
        "id": "F0yPasmAEbwP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# config = dotenv_values(\".env\")\n",
        "# login(token=config[\"HUGGING_FACE_TOKEN\"])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-18T17:57:23.540413Z",
          "iopub.execute_input": "2025-04-18T17:57:23.540700Z",
          "iopub.status.idle": "2025-04-18T17:57:23.544032Z",
          "shell.execute_reply.started": "2025-04-18T17:57:23.540678Z",
          "shell.execute_reply": "2025-04-18T17:57:23.543398Z"
        },
        "id": "rqAqPx9_EbwQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constants\n",
        "here we define constants that we will fine tune"
      ],
      "metadata": {
        "id": "mwpZZH72EbwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "DEFAULT_CHUNK_SIZE = 1000              # Max size of each text chunk\n",
        "DEFAULT_CHUNK_OVERLAP = 200            # Overlap between chunks\n",
        "DEFAULT_SEARCH_K = 3                   # Top-k results to retrieve\n",
        "DEFAULT_SEARCH_TYPE = \"hybrid\"         # Choose from: 'semantic', 'keyword', or 'hybrid'\n",
        "DEFAULT_EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # Embedding model for vector search\n",
        "DEFAULT_LLM_MODEL = \"meta-llama/Llama-3.2-1B\"                         # LLM for generating answers\n",
        "# DEFAULT_LLM_MODEL = \"deepseek-ai/DeepSeek-V3-0324\"                         # LLM for generating answers\n",
        "# DEFAULT_DOCUMENT_DIR = \"/data/corpus.zip\"\n",
        "DEFAULT_DOCUMENT_DIR = \"/kaggle/input/daa-lectures-for-a4/cmu-lecs\""
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.002Z"
        },
        "id": "uvpZHpqdEbwQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Processing Functions"
      ],
      "metadata": {
        "id": "L1E1l1iNxLEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_documents(directory: str, glob_pattern: str = \"**/*.pdf\") -> List[Document]:\n",
        "    \"\"\"\n",
        "    Loads all PDF files from a given directory.\n",
        "\n",
        "    Args:\n",
        "        directory: path to folder with PDF files\n",
        "        glob_pattern: pattern to match files (default = all PDFs)\n",
        "\n",
        "    Returns:\n",
        "        List of LangChain Document objects\n",
        "    \"\"\"\n",
        "    loader = DirectoryLoader(directory, glob=glob_pattern, loader_cls=PyPDFLoader)\n",
        "    return loader.load()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.005Z"
        },
        "id": "HV2X-Xd7EbwQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_documents(\n",
        "    documents: List[Document],\n",
        "    chunk_size: int = DEFAULT_CHUNK_SIZE,\n",
        "    chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,\n",
        "    separators: List[str] = None\n",
        ") -> List[Document]:\n",
        "    \"\"\"\n",
        "    Splits documents into chunks for better retrieval.\n",
        "\n",
        "    Args:\n",
        "        documents: list of LangChain documents\n",
        "        chunk_size: size of each chunk\n",
        "        chunk_overlap: how much content overlaps between chunks\n",
        "        separators: optional list of separators for better splitting\n",
        "\n",
        "    Returns:\n",
        "        List of chunked Document objects\n",
        "    \"\"\"\n",
        "    if separators is None:\n",
        "        # Default separators: prioritize splitting on paragraphs, then sentences, then words\n",
        "        separators = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        separators=separators\n",
        "    )\n",
        "    return text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.005Z"
        },
        "id": "k1ua7tohEbwR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vector_store(\n",
        "    chunks: List[Document],\n",
        "    embedding_model: str = DEFAULT_EMBEDDING_MODEL,\n",
        "    save_path: str = None\n",
        ") -> FAISS:\n",
        "    \"\"\"\n",
        "    Creates a FAISS vector index from document chunks using specified embedding model.\n",
        "\n",
        "    Args:\n",
        "        chunks: list of Document chunks\n",
        "        embedding_model: HuggingFace model used for embeddings\n",
        "        save_path: optional path to save the index\n",
        "\n",
        "    Returns:\n",
        "        FAISS vector store\n",
        "    \"\"\"\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=embedding_model)\n",
        "    vectordb = FAISS.from_documents(chunks, embeddings)\n",
        "\n",
        "    if save_path:\n",
        "        vectordb.save_local(save_path)\n",
        "\n",
        "    return vectordb"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.006Z"
        },
        "id": "IYGOIWotEbwR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bm25_index(chunks: List[Document]) -> BM25Okapi:\n",
        "    \"\"\"\n",
        "    Builds a keyword-based index using BM25.\n",
        "\n",
        "    Args:\n",
        "        chunks: list of Document chunks\n",
        "\n",
        "    Returns:\n",
        "        BM25 index\n",
        "    \"\"\"\n",
        "    texts = [chunk.page_content for chunk in chunks]                  # Get plain text\n",
        "    tokenized_texts = [text.split() for text in texts]               # Tokenize by whitespace\n",
        "    return BM25Okapi(tokenized_texts)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.006Z"
        },
        "id": "1SBM9zbqEbwR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Search Functions"
      ],
      "metadata": {
        "id": "p-gPSeFBxLEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search(\n",
        "    query: str,\n",
        "    vectordb: FAISS,\n",
        "    k: int = DEFAULT_SEARCH_K,\n",
        "    score_threshold: float = None\n",
        ") -> List[Tuple[Document, float]]:\n",
        "    \"\"\"\n",
        "    Perform semantic search using vector similarity from FAISS.\n",
        "\n",
        "    Args:\n",
        "        query: Natural language question\n",
        "        vectordb: Vector index (FAISS)\n",
        "        k: Number of results to return\n",
        "        score_threshold: Filter out low similarity scores (optional)\n",
        "\n",
        "    Returns:\n",
        "        List of (Document, similarity_score) tuples\n",
        "    \"\"\"\n",
        "    results = vectordb.similarity_search_with_score(query, k=k)\n",
        "\n",
        "    # Optional thresholding to remove irrelevant results\n",
        "    if score_threshold is not None:\n",
        "        results = [(doc, score) for doc, score in results if score >= score_threshold]\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.007Z"
        },
        "id": "fE1SDy_nEbwR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def keyword_search(\n",
        "    query: str,\n",
        "    bm25_index: BM25Okapi,\n",
        "    chunks: List[Document],\n",
        "    k: int = DEFAULT_SEARCH_K,\n",
        "    score_threshold: float = None\n",
        ") -> List[Tuple[Document, float]]:\n",
        "    \"\"\"\n",
        "    Perform lexical search using BM25.\n",
        "\n",
        "    Args:\n",
        "        query: Search query string\n",
        "        bm25_index: Pre-built BM25 index\n",
        "        chunks: List of document chunks for mapping back\n",
        "        k: Top-k documents to retrieve\n",
        "        score_threshold: Optional filtering threshold for BM25 scores\n",
        "\n",
        "    Returns:\n",
        "        List of (Document, BM25_score) tuples\n",
        "    \"\"\"\n",
        "    tokenized_query = query.split()  # Basic whitespace tokenization\n",
        "    scores = bm25_index.get_scores(tokenized_query)\n",
        "\n",
        "    # Get indices of top-k documents\n",
        "    top_k_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]\n",
        "\n",
        "    results = [(chunks[i], scores[i]) for i in top_k_indices]\n",
        "\n",
        "    if score_threshold is not None:\n",
        "        results = [(doc, score) for doc, score in results if score >= score_threshold]\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.007Z"
        },
        "id": "Nhfc2td3EbwS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def hybrid_search(\n",
        "    query: str,\n",
        "    vectordb: FAISS,\n",
        "    bm25_index: BM25Okapi,\n",
        "    chunks: List[Document],\n",
        "    k: int = DEFAULT_SEARCH_K,\n",
        "    semantic_weight: float = 0.5,\n",
        "    keyword_weight: float = 0.5\n",
        ") -> List[Tuple[Document, float]]:\n",
        "    \"\"\"\n",
        "    Combine semantic and keyword search using weighted score fusion.\n",
        "\n",
        "    Args:\n",
        "        query: Natural language query\n",
        "        vectordb: FAISS vector database\n",
        "        bm25_index: BM25 keyword index\n",
        "        chunks: Document chunks (used for mapping back)\n",
        "        k: Top-k results to return\n",
        "        semantic_weight: Weight for vector similarity\n",
        "        keyword_weight: Weight for BM25 relevance\n",
        "\n",
        "    Returns:\n",
        "        List of (Document, combined_score) tuples\n",
        "    \"\"\"\n",
        "    # Step 1: Run both types of searches with larger k (to capture wider context)\n",
        "    semantic_results = semantic_search(query, vectordb, k * 2)\n",
        "    semantic_scores = {doc.page_content: score for doc, score in semantic_results}\n",
        "\n",
        "    keyword_results = keyword_search(query, bm25_index, chunks, k * 2)\n",
        "    keyword_scores = {doc.page_content: score for doc, score in keyword_results}\n",
        "\n",
        "    # Step 2: Normalize BM25 scores (they are not bounded, unlike cosine similarity)\n",
        "    max_kw_score = max(keyword_scores.values()) if keyword_scores else 1\n",
        "\n",
        "    # Step 3: Combine results\n",
        "    all_docs = set(semantic_scores.keys()).union(set(keyword_scores.keys()))\n",
        "    combined_scores = []\n",
        "\n",
        "    for doc_content in all_docs:\n",
        "        sem_score = semantic_scores.get(doc_content, 0)\n",
        "        kw_score = keyword_scores.get(doc_content, 0)\n",
        "        norm_kw_score = kw_score / max_kw_score if max_kw_score > 0 else 0\n",
        "\n",
        "        # Weighted sum of both types of scores\n",
        "        combined_score = (semantic_weight * sem_score) + (keyword_weight * norm_kw_score)\n",
        "        combined_scores.append((doc_content, combined_score))\n",
        "\n",
        "    # Step 4: Sort and return top-k\n",
        "    combined_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "    top_scores = combined_scores[:k]\n",
        "\n",
        "    # Step 5: Re-map back to full Document objects using content\n",
        "    doc_lookup = {chunk.page_content: chunk for chunk in chunks}\n",
        "    results = []\n",
        "\n",
        "    for doc_content, score in top_scores:\n",
        "        if doc_content in doc_lookup:\n",
        "            results.append((doc_lookup[doc_content], score))\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.007Z"
        },
        "id": "J3T0XMX0xLEH",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM functions"
      ],
      "metadata": {
        "id": "7Q5NGOdUxLEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_llm(\n",
        "    model_name: str = DEFAULT_LLM_MODEL,\n",
        "    device: str = \"cuda\",  # Use \"cpu\" if not using GPU\n",
        "    max_new_tokens: int = 300\n",
        ") -> Tuple[pipeline, any]:\n",
        "    \"\"\"\n",
        "    Loads a language model pipeline for text generation.\n",
        "\n",
        "    Args:\n",
        "        model_name: HuggingFace model repo (must support causal LM)\n",
        "        device: \"cuda\" for GPU or \"cpu\"\n",
        "        max_new_tokens: Max tokens to generate per response\n",
        "\n",
        "    Returns:\n",
        "        Tuple (generator pipeline, tokenizer)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=device,\n",
        "            torch_dtype=\"auto\",            # Uses GPU acceleration if available\n",
        "            trust_remote_code=True         # Allow custom model architectures\n",
        "        )\n",
        "        print(\"Original used\")\n",
        "    except ImportError:\n",
        "        # Fallback without device_map\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        ).to(device)\n",
        "        print(\"Edited used\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Format the prompt as a dialogue (user + assistant style)\n",
        "    tokenizer.chat_template = (\n",
        "        \"{% for message in messages %}\"\n",
        "        \"{% if message['role'] == 'user' %}User: {{ message['content'] }}\\n\"\n",
        "        \"{% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }}\\n\"\n",
        "        \"{% endif %}\"\n",
        "        \"{% endfor %}\"\n",
        "        \"{% if add_generation_prompt %}Assistant:{% endif %}\"\n",
        "    )\n",
        "\n",
        "    # Create a text-generation pipeline\n",
        "    generator = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        return_full_text=False,        # Only return generated part, not the full prompt\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=True                 # Use sampling (stochastic generation)\n",
        "    )\n",
        "\n",
        "    return generator, tokenizer"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.008Z"
        },
        "id": "v89jLCD_EbwS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(\n",
        "    prompt: str,\n",
        "    generator: pipeline,\n",
        "    width: int = 80  # For pretty-printing long outputs\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generates a response from the LLM using the prompt.\n",
        "\n",
        "    Args:\n",
        "        prompt: Full RAG-formatted prompt with question + context\n",
        "        generator: HF pipeline object\n",
        "        width: max characters per printed line (for wrapping)\n",
        "\n",
        "    Returns:\n",
        "        Answer string\n",
        "    \"\"\"\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]        # Wrap prompt in chat message format\n",
        "    output = generator(\n",
        "        messages,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        do_sample=True\n",
        "    )\n",
        "                            # Call LLM\n",
        "    return textwrap.fill(output[0][\"generated_text\"], width=width)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.008Z"
        },
        "id": "kQMhtE8lEbwT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def format_rag_prompt(\n",
        "    question: str,\n",
        "    retrieved_docs: List[Document],\n",
        "    instruction: str = None\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Formats the final input prompt to send to the LLM.\n",
        "\n",
        "    Args:\n",
        "        question: The user's natural language question\n",
        "        retrieved_docs: List of document chunks retrieved by search\n",
        "        instruction: Optional prompt instructions (system message)\n",
        "\n",
        "    Returns:\n",
        "        Full prompt text string\n",
        "    \"\"\"\n",
        "    # Default instructions to guide the LLM on how to use retrieved documents\n",
        "    if instruction is None:\n",
        "        instruction = \"\"\"You are an AI assistant tasked with answering questions based on retrieved knowledge.\n",
        "                    - Integrate the key points from all retrieved responses into a cohesive, well-structured answer.\n",
        "                    - If the responses are contradictory, mention the different perspectives.\n",
        "                    - If none of the retrieved responses contain relevant information, reply:\n",
        "                    \"I couldn't find a good response to your query in the database.\"\n",
        "                    \"\"\"\n",
        "\n",
        "    # Truncate each document to 1000 characters if long\n",
        "    retrieved_info = \"\\n\\n\".join(\n",
        "        f\"{i+1}️⃣ {doc.page_content[:1000]}...\" if len(doc.page_content) > 1000\n",
        "        else f\"{i+1}️⃣ {doc.page_content}\"\n",
        "        for i, doc in enumerate(retrieved_docs)\n",
        "    )\n",
        "\n",
        "    # Final structured prompt\n",
        "    return f\"\"\"\n",
        "        {instruction}\n",
        "\n",
        "        ### Retrieved Information:\n",
        "        {retrieved_info}\n",
        "\n",
        "        ### Question:\n",
        "        {question}\n",
        "    \"\"\""
      ],
      "metadata": {
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.008Z"
        },
        "id": "9GJIwleTxLEJ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Evaluator"
      ],
      "metadata": {
        "id": "07jDDqiSxLEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WRAPPER TO USE LANGCHAIN LLM IN RAGAS\n",
        "\n",
        "class HuggingFaceLLM:\n",
        "    \"\"\"\n",
        "    Simple wrapper for using HuggingFaceHub with RAGAS evaluation.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model = HuggingFaceHub(repo_id=model_name)\n",
        "\n",
        "    def __call__(self, prompt: str) -> str:\n",
        "        return self.model(prompt)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.009Z"
        },
        "id": "eKsmE4PVEbwU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# EVALUATOR CLASS\n",
        "\n",
        "class RAGEvaluator:\n",
        "    \"\"\"\n",
        "    Performs automatic evaluation of RAG responses using Ragas metrics.\n",
        "    Also supports result visualization and optimization insights.\n",
        "    \"\"\"\n",
        "    def __init__(self, pipeline, llm, embeddings):\n",
        "        self.pipeline = pipeline         # RAGPipeline object\n",
        "        self.embeddings = embeddings     # HuggingFaceEmbeddings instance\n",
        "\n",
        "        # Use passed LLM, or initialize default\n",
        "        if isinstance(llm, str):\n",
        "            self.llm = HuggingFaceHub(repo_id=llm)\n",
        "        else:\n",
        "            self.llm = llm or HuggingFaceHub(repo_id=DEFAULT_LLM_MODEL)\n",
        "\n",
        "        # Internal result tracking\n",
        "        self.results = []\n",
        "\n",
        "    def evaluate_ragas(self, questions: list, gold_answers: list = None):\n",
        "        \"\"\"\n",
        "        Run Ragas evaluation across all questions.\n",
        "\n",
        "        Args:\n",
        "            questions: List of input questions\n",
        "            gold_answers: Reference answers (optional)\n",
        "\n",
        "        Returns:\n",
        "            DataFrame of results\n",
        "        \"\"\"\n",
        "        all_rows = []\n",
        "\n",
        "        for question, gold_answer in zip(questions, gold_answers or [None]*len(questions)):\n",
        "            # Run full RAG query\n",
        "            answer = self.pipeline.query(question)\n",
        "\n",
        "            # Get context used in the answer\n",
        "            contexts = [doc.page_content for doc in self.pipeline.get_last_retrieved_docs()]\n",
        "\n",
        "            # Prepare a single sample for evaluation\n",
        "            data = {\n",
        "                \"question\": [question],\n",
        "                \"answer\": [answer],\n",
        "                \"contexts\": [contexts]\n",
        "            }\n",
        "            if gold_answer:\n",
        "                data[\"ground_truth\"] = [gold_answer]\n",
        "\n",
        "            dataset = Dataset.from_dict(data)\n",
        "\n",
        "            # Select metrics to compute\n",
        "            metrics = [Faithfulness(), AnswerRelevancy(), ContextRecall(), ContextPrecision()]\n",
        "            if gold_answer:\n",
        "                metrics.append(AnswerCorrectness())\n",
        "\n",
        "            # Run the evaluation\n",
        "            result = evaluate(dataset, metrics=metrics, llm=self.llm, embeddings=self.embeddings)\n",
        "\n",
        "            # Convert to DataFrame and store\n",
        "            row = result.to_pandas()\n",
        "            row[\"question\"] = question\n",
        "            row[\"retrieved_docs\"] = len(contexts)\n",
        "            all_rows.append(row)\n",
        "\n",
        "        # Combine all rows into one DataFrame\n",
        "        self.results = pd.concat(all_rows, ignore_index=True)\n",
        "        return self.results\n",
        "\n",
        "    def visualize_metrics(self):\n",
        "        \"\"\"\n",
        "        Visualize average metric scores and context retrieval stats.\n",
        "        \"\"\"\n",
        "        if self.results is None or self.results.empty:\n",
        "            raise ValueError(\"No evaluation results found. Run evaluate_ragas() first.\")\n",
        "\n",
        "        # Plot main metrics\n",
        "        metrics = ['faithfulness', 'answer_relevancy', 'context_recall', 'context_precision']\n",
        "        if 'answer_correctness' in self.results.columns:\n",
        "            metrics.append('answer_correctness')\n",
        "\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        self.results[metrics].mean().plot(kind='bar', color='lightblue')\n",
        "        plt.title(\"🔍 Average RAG Evaluation Metrics\")\n",
        "        plt.ylabel(\"Score (0 to 1)\")\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "        plt.show()\n",
        "\n",
        "        # Plot document retrieval counts\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        self.results['retrieved_docs'].value_counts().sort_index().plot(kind='bar', color='lightgreen')\n",
        "        plt.title(\"📄 Number of Context Chunks Retrieved Per Query\")\n",
        "        plt.xlabel(\"Number of Chunks\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "        plt.show()\n",
        "\n",
        "    def get_optimization_insights(self):\n",
        "        \"\"\"\n",
        "        Analyze weak metrics and recommend strategies to improve RAG performance.\n",
        "        \"\"\"\n",
        "        if self.results is None or self.results.empty:\n",
        "            return \"No evaluation results available.\"\n",
        "\n",
        "        insights = []\n",
        "        df = self.results\n",
        "\n",
        "        # Faithfulness issues (hallucination)\n",
        "        if df['faithfulness'].mean() < 0.7:\n",
        "            insights.append(\"⚠️ Faithfulness is low — possible hallucinations.\")\n",
        "            insights.append(\"🔧 Try increasing chunk overlap or improving retrieval relevance.\")\n",
        "\n",
        "        # Context recall issues (missing info)\n",
        "        if df['context_recall'].mean() < 0.6:\n",
        "            insights.append(\"⚠️ Low context recall — relevant info may be missed.\")\n",
        "            insights.append(\"🔧 Consider using hybrid retrieval or adjusting chunk size.\")\n",
        "\n",
        "        # Precision issues (irrelevant info)\n",
        "        if df['context_precision'].mean() < 0.6:\n",
        "            insights.append(\"⚠️ Low context precision — too much irrelevant context.\")\n",
        "            insights.append(\"🔧 Use better embeddings or rerank retrieved chunks.\")\n",
        "\n",
        "        # Relevance issues (answer not matching question)\n",
        "        if df['answer_relevancy'].mean() < 0.7:\n",
        "            insights.append(\"⚠️ Low answer relevancy — answers not matching question.\")\n",
        "            insights.append(\"🔧 Refine your prompts or improve chunk matching.\")\n",
        "\n",
        "        # Optional: Correctness based on gold answers\n",
        "        if 'answer_correctness' in df.columns and df['answer_correctness'].mean() < 0.7:\n",
        "            insights.append(\"⚠️ Low correctness — answers differ from references.\")\n",
        "            insights.append(\"🔧 Try different LLMs or use post-editing.\")\n",
        "\n",
        "        return \"\\n\".join(insights)"
      ],
      "metadata": {
        "id": "DAFXSDHQxLEK",
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.010Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Pipeline"
      ],
      "metadata": {
        "id": "r1Lfz6KJxLEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN RAG PIPELINE\n",
        "\n",
        "class RAGPipeline:\n",
        "    \"\"\"\n",
        "    This is the central class that handles:\n",
        "    - Loading and chunking documents\n",
        "    - Initializing vector and keyword search\n",
        "    - Running queries\n",
        "    - Generating responses from the LLM\n",
        "    - Running full experiment sweeps\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        document_dir: str,\n",
        "        embedding_model: str = DEFAULT_EMBEDDING_MODEL,\n",
        "        llm_model: str = DEFAULT_LLM_MODEL,\n",
        "        chunk_size: int = DEFAULT_CHUNK_SIZE,\n",
        "        chunk_overlap: int = DEFAULT_CHUNK_OVERLAP,\n",
        "        device: str = \"cuda\"\n",
        "    ):\n",
        "        self.document_dir = document_dir\n",
        "        self.embedding_model = embedding_model\n",
        "        self.llm_model = llm_model\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        self.device = device\n",
        "\n",
        "        # To store runtime state\n",
        "        self.documents = None\n",
        "        self.chunks = None\n",
        "        self.vectordb = None\n",
        "        self.bm25_index = None\n",
        "        self.llm = None\n",
        "        self.tokenizer = None\n",
        "        self.last_retrieved_docs = None  # For evaluation traceability\n",
        "\n",
        "\n",
        "    # ==========================\n",
        "    # LOAD & CHUNK DOCUMENTS\n",
        "    # ==========================\n",
        "    def load_and_process_documents(self):\n",
        "        \"\"\"\n",
        "        Loads PDF documents and splits them into overlapping chunks.\n",
        "        \"\"\"\n",
        "        print(\"📄 Loading documents...\")\n",
        "        self.documents = load_documents(self.document_dir)\n",
        "        print(f\"✅ Loaded {len(self.documents)} document pages.\")\n",
        "\n",
        "        print(\"🪓 Chunking documents...\")\n",
        "        self.chunks = chunk_documents(\n",
        "            self.documents,\n",
        "            chunk_size=self.chunk_size,\n",
        "            chunk_overlap=self.chunk_overlap\n",
        "        )\n",
        "        print(f\"✅ Created {len(self.chunks)} chunks.\")\n",
        "\n",
        "        # Add unique IDs to chunks for tracking\n",
        "        for i, chunk in enumerate(self.chunks):\n",
        "            chunk.metadata[\"chunk_id\"] = i\n",
        "\n",
        "\n",
        "    # ==========================\n",
        "    # INITIALIZE RETRIEVAL\n",
        "    # ==========================\n",
        "    def initialize_retrieval(self):\n",
        "        \"\"\"\n",
        "        Builds vector store and keyword index for retrieval.\n",
        "        \"\"\"\n",
        "        if not self.chunks:\n",
        "            raise ValueError(\"❌ No chunks found. Run load_and_process_documents() first.\")\n",
        "\n",
        "        print(\"📦 Creating vector store...\")\n",
        "        self.vectordb = create_vector_store(self.chunks, self.embedding_model)\n",
        "\n",
        "        print(\"🔎 Creating BM25 index...\")\n",
        "        self.bm25_index = create_bm25_index(self.chunks)\n",
        "\n",
        "\n",
        "    # ==========================\n",
        "    # INITIALIZE LLM\n",
        "    # ==========================\n",
        "    def initialize_llm(self):\n",
        "        \"\"\"\n",
        "        Loads the chosen LLM and tokenizer from HuggingFace.\n",
        "        \"\"\"\n",
        "        print(\"🤖 Loading LLM...\")\n",
        "        self.llm, self.tokenizer = initialize_llm(self.llm_model, self.device)\n",
        "\n",
        "\n",
        "    # ==========================\n",
        "    # GET LAST RETRIEVED CHUNKS\n",
        "    # ==========================\n",
        "    def get_last_retrieved_docs(self):\n",
        "        \"\"\"\n",
        "        Returns the last set of retrieved document chunks (used in evaluation).\n",
        "        \"\"\"\n",
        "        if self.last_retrieved_docs is None:\n",
        "            raise ValueError(\"❌ No retrievals done yet.\")\n",
        "        return self.last_retrieved_docs\n",
        "\n",
        "\n",
        "    # ==========================\n",
        "    # MAIN QUERY FUNCTION\n",
        "    # ==========================\n",
        "    def query(\n",
        "        self,\n",
        "        question: str,\n",
        "        search_type: str = DEFAULT_SEARCH_TYPE,  # semantic / keyword / hybrid\n",
        "        k: int = DEFAULT_SEARCH_K,\n",
        "        semantic_weight: float = 0.5,\n",
        "        keyword_weight: float = 0.5,\n",
        "        custom_instruction: str = None\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Executes a full query through the pipeline:\n",
        "        - Retrieves chunks\n",
        "        - Formats prompt\n",
        "        - Calls LLM\n",
        "        - Returns answer\n",
        "        \"\"\"\n",
        "        if not self.vectordb or not self.bm25_index:\n",
        "            raise ValueError(\"❌ Retrieval systems not ready. Run initialize_retrieval().\")\n",
        "        if not self.llm:\n",
        "            raise ValueError(\"❌ LLM not initialized. Run initialize_llm().\")\n",
        "\n",
        "        # Step 1: Retrieve relevant documents\n",
        "        if search_type == \"semantic\":\n",
        "            results = semantic_search(question, self.vectordb, k)\n",
        "        elif search_type == \"keyword\":\n",
        "            results = keyword_search(question, self.bm25_index, self.chunks, k)\n",
        "        elif search_type == \"hybrid\":\n",
        "            results = hybrid_search(\n",
        "                question,\n",
        "                self.vectordb,\n",
        "                self.bm25_index,\n",
        "                self.chunks,\n",
        "                k,\n",
        "                semantic_weight,\n",
        "                keyword_weight\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"❌ Unknown search type: {search_type}\")\n",
        "\n",
        "        retrieved_docs = [doc for doc, _ in results]\n",
        "        self.last_retrieved_docs = retrieved_docs\n",
        "\n",
        "        # Step 2: Format prompt\n",
        "        prompt = format_rag_prompt(question, retrieved_docs, custom_instruction)\n",
        "\n",
        "        # Step 3: Generate LLM response\n",
        "        return generate_response(prompt, self.llm)\n",
        "\n",
        "\n",
        "    # ==========================\n",
        "    # EXPERIMENTATION FUNCTION\n",
        "    # ==========================\n",
        "    def experiment(\n",
        "        self,\n",
        "        questions: List[str],\n",
        "        gold_answers: List[str],\n",
        "        chunk_sizes: List[int],\n",
        "        k_values: List[int],\n",
        "        search_types: List[str],\n",
        "        chunk_overlaps: List[int] = [0, 100, 200]\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Run multiple experiment configurations (varying chunk size, k, search type).\n",
        "\n",
        "        Args:\n",
        "            questions: list of input queries\n",
        "            gold_answers: reference answers (used in evaluation)\n",
        "            chunk_sizes: different chunk sizes to test\n",
        "            k_values: number of results to retrieve\n",
        "            search_types: list of retrieval modes\n",
        "            chunk_overlaps: amount of content overlap between chunks\n",
        "\n",
        "        Returns:\n",
        "            A dict of results by config\n",
        "        \"\"\"\n",
        "        results = {}\n",
        "\n",
        "        for chunk_size in chunk_sizes:\n",
        "            for chunk_overlap in chunk_overlaps:\n",
        "                self.chunk_size = chunk_size\n",
        "                self.chunk_overlap = chunk_overlap\n",
        "\n",
        "                print(f\"\\n⚙️  Testing: chunk={chunk_size}, overlap={chunk_overlap}\")\n",
        "                self.load_and_process_documents()\n",
        "                self.initialize_retrieval()\n",
        "\n",
        "                evaluator = RAGEvaluator(self, self.llm, HuggingFaceEmbeddings(model_name=self.embedding_model))\n",
        "\n",
        "                for search_type in search_types:\n",
        "                    for k in k_values:\n",
        "                        config_name = f\"chunk{chunk_size}_overlap{chunk_overlap}_{search_type}_k{k}\"\n",
        "                        results[config_name] = {}\n",
        "\n",
        "                        for question in questions:\n",
        "                            try:\n",
        "                                answer = self.query(\n",
        "                                    question,\n",
        "                                    search_type=search_type,\n",
        "                                    k=k\n",
        "                                )\n",
        "                                results[config_name][question] = answer\n",
        "\n",
        "                                # Run evaluation (with gold answer)\n",
        "                                eval_result = evaluator.evaluate_ragas([question], [gold_answers[0]])\n",
        "                                print(eval_result[['faithfulness', 'answer_relevancy']])\n",
        "\n",
        "                                # Visualization + optimization\n",
        "                                evaluator.visualize_metrics()\n",
        "                                print(\"\\n🧠 Optimization Suggestions:\")\n",
        "                                print(evaluator.get_optimization_insights())\n",
        "\n",
        "                            except Exception as e:\n",
        "                                results[config_name][question] = f\"❌ Error: {str(e)}\"\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "    def grid_search(\n",
        "        self,\n",
        "        questions: List[str],\n",
        "        gold_answers: List[str],\n",
        "        chunk_sizes: List[int],\n",
        "        k_values: List[int],\n",
        "        search_types: List[str],\n",
        "        chunk_overlaps: List[int] = [0, 100, 200],\n",
        "        semantic_weight: float = 0.5,\n",
        "        keyword_weight: float = 0.5,\n",
        "        output_csv_path: str = \"rag_grid_search_results.csv\"\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Run grid search over multiple config combinations and log results to CSV.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary of results.\n",
        "        \"\"\"\n",
        "        results = []\n",
        "        header_written = False\n",
        "\n",
        "        # Ensure output directory exists\n",
        "        os.makedirs(os.path.dirname(output_csv_path) or \".\", exist_ok=True)\n",
        "\n",
        "        with open(output_csv_path, mode='w', newline='', encoding='utf-8') as file:\n",
        "            writer = csv.writer(file)\n",
        "\n",
        "            for chunk_size in chunk_sizes:\n",
        "                for chunk_overlap in chunk_overlaps:\n",
        "                    if chunk_overlap >= chunk_size:\n",
        "                        continue\n",
        "                    self.chunk_size = chunk_size\n",
        "                    self.chunk_overlap = chunk_overlap\n",
        "\n",
        "                    self.load_and_process_documents()\n",
        "                    self.initialize_retrieval()\n",
        "\n",
        "                    evaluator = RAGEvaluator(self, self.llm, HuggingFaceEmbeddings(model_name=self.embedding_model))\n",
        "\n",
        "                    for search_type in search_types:\n",
        "                        for k in k_values:\n",
        "                            config_name = f\"chunk{chunk_size}_overlap{chunk_overlap}_{search_type}_k{k}\"\n",
        "                            print(f\"\\n🔬 Running Config: {config_name}\")\n",
        "\n",
        "                            for i, question in enumerate(questions):\n",
        "                                try:\n",
        "                                    start_time = time.time()\n",
        "\n",
        "                                    # Step 1: Query\n",
        "                                    retrieval_start = time.time()\n",
        "                                    answer = self.query(\n",
        "                                        question=question,\n",
        "                                        search_type=search_type,\n",
        "                                        k=k,\n",
        "                                        semantic_weight=semantic_weight,\n",
        "                                        keyword_weight=keyword_weight\n",
        "                                    )\n",
        "                                    retrieval_end = time.time()\n",
        "\n",
        "                                    # Step 2: Evaluation\n",
        "                                    eval_df = evaluator.evaluate_ragas([question], [gold_answers[i]])\n",
        "                                    eval_row = eval_df.iloc[0].to_dict()\n",
        "\n",
        "                                    # Step 3: Timing\n",
        "                                    end_time = time.time()\n",
        "                                    total_time = end_time - start_time\n",
        "                                    retrieval_time = retrieval_end - retrieval_start\n",
        "                                    generation_time = total_time - retrieval_time\n",
        "\n",
        "                                    print(eval_result[['faithfulness', 'answer_relevancy']])\n",
        "\n",
        "                                    # Visualization + optimization\n",
        "                                    evaluator.visualize_metrics()\n",
        "                                    print(\"\\n🧠 Optimization Suggestions:\")\n",
        "                                    print(evaluator.get_optimization_insights())\n",
        "\n",
        "                                    # Step 4: Build result row\n",
        "                                    row = {\n",
        "                                        \"config\": config_name,\n",
        "                                        \"question\": question,\n",
        "                                        \"answer\": answer,\n",
        "                                        \"chunk_size\": chunk_size,\n",
        "                                        \"chunk_overlap\": chunk_overlap,\n",
        "                                        \"search_type\": search_type,\n",
        "                                        \"top_k\": k,\n",
        "                                        \"semantic_weight\": semantic_weight,\n",
        "                                        \"keyword_weight\": keyword_weight,\n",
        "                                        \"retrieval_time\": round(retrieval_time, 4),\n",
        "                                        \"generation_time\": round(generation_time, 4),\n",
        "                                        \"total_time\": round(total_time, 4),\n",
        "                                        **eval_row\n",
        "                                    }\n",
        "\n",
        "                                    # Write CSV header if needed\n",
        "                                    if not header_written:\n",
        "                                        writer.writerow(row.keys())\n",
        "                                        header_written = True\n",
        "\n",
        "                                    writer.writerow(row.values())\n",
        "                                    results.append(row)\n",
        "\n",
        "                                except Exception as e:\n",
        "                                    print(f\"⚠️ Error in config {config_name} for question '{question}': {e}\")\n",
        "                                    writer.writerow([config_name, question, f\"Error: {str(e)}\"])\n",
        "\n",
        "        print(f\"\\n✅ All results logged to: {output_csv_path}\")\n",
        "        return results"
      ],
      "metadata": {
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.010Z"
        },
        "id": "ebUXkzJ5xLEL",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running\n",
        "here we now run the code"
      ],
      "metadata": {
        "id": "ieAPdGVSEbwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 1: CREATE EMBEDDINGS & LLM WRAPPER\n",
        "\n",
        "# Use the same model you defined as DEFAULT_EMBEDDING_MODEL\n",
        "embeddings = HuggingFaceEmbeddings(model_name=DEFAULT_EMBEDDING_MODEL)\n",
        "\n",
        "# Initialize LLM pipeline\n",
        "generator, tokenizer = initialize_llm(\n",
        "    model_name=DEFAULT_LLM_MODEL,     # <-- Use the same LLM constant from above\n",
        "    device=\"cuda\",                    # \"cuda\" or \"cpu\"\n",
        "    max_new_tokens=300                # <-- You can increase this if responses are too short\n",
        ")\n",
        "\n",
        "# Wrap your generator for LangChain compatibility\n",
        "local_llm = HuggingFacePipeline(pipeline=generator)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.011Z"
        },
        "id": "C97jGDKVEbwX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: INITIALIZE RAG PIPELINE\n",
        "\n",
        "rag = RAGPipeline(\n",
        "    document_dir=DEFAULT_DOCUMENT_DIR,       # <-- uses constant\n",
        "    embedding_model=DEFAULT_EMBEDDING_MODEL,\n",
        "    llm_model=DEFAULT_LLM_MODEL,\n",
        "    chunk_size=DEFAULT_CHUNK_SIZE,\n",
        "    chunk_overlap=DEFAULT_CHUNK_OVERLAP,\n",
        "    device=\"cuda\"\n",
        ")\n",
        "rag"
      ],
      "metadata": {
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.011Z"
        },
        "id": "oZT87eroxLEM",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 3: LOAD DOCUMENTS & PREPARE SYSTEM\n",
        "\n",
        "rag.load_and_process_documents()\n",
        "rag.initialize_retrieval()\n",
        "rag.initialize_llm()  # This will use Llama model defined above"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.012Z"
        },
        "id": "Z6PZiqMbEbwe"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4: ASK A SINGLE QUESTION\n",
        "\n",
        "questions = [\n",
        "    \"What is Dynamic Programming?\"\n",
        "    # \"Explain the matrix method in hashing\",\n",
        "    # \"What are the key concepts in amortized analysis?\"\n",
        "]\n",
        "\n",
        "# Run query with hybrid search and show result\n",
        "answer = rag.query(\n",
        "    questions[0],\n",
        "    search_type=DEFAULT_SEARCH_TYPE,\n",
        "    k=DEFAULT_SEARCH_K\n",
        ")\n",
        "print(f\"\\n🧠 Answer:\\n{answer}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.012Z"
        },
        "id": "vUgUZ4ArEbwf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 5: PROVIDE GROUND TRUTH FOR EVALUATION (OPTIONAL)\n",
        "\n",
        "gold_answers = [\n",
        "    \"Dynamic Programming is a technique for solving problems by breaking them into overlapping subproblems, storing intermediate results, and combining them to solve the larger problem efficiently.\"\n",
        "    # \"Dynamic Programming is a powerful technique that can be used to solve many combinatorial problems in polynomial time for which a naive approach would take exponential time. Dynamic Programming is a general approach to solving problems, much like “divide-and-conquer”, except that the subproblems will overlap.\"\n",
        "]"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.013Z"
        },
        "id": "aaNOBSlnEbwf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 6: RUN AUTOMATIC EVALUATION\n",
        "\n",
        "evaluator = RAGEvaluator(rag, llm=local_llm, embeddings=embeddings)\n",
        "results_df = evaluator.evaluate_ragas(questions, gold_answers)\n",
        "print(\"\\n📊 Evaluation Results:\\n\", results_df)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.013Z"
        },
        "id": "fr32Ich5Ebwf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize performance\n",
        "evaluator.visualize_metrics()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.014Z"
        },
        "id": "25CFCtoEEbwf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Show optimization tips\n",
        "print(\"\\n🛠️ Suggestions to Improve RAG System:\")\n",
        "print(evaluator.get_optimization_insights())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.018Z"
        },
        "id": "0tc1zvFKEbwf"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_results = rag.grid_search(\n",
        "    questions=questions,\n",
        "    gold_answers=gold_answers,\n",
        "    chunk_sizes=[100, 250, 500, 800, 1000],\n",
        "    k_values=[3, 4, 5],\n",
        "    search_types=[\"semantic\", \"hybrid\", \"keyword\"],\n",
        "    chunk_overlaps=[100, 200],\n",
        "    output_csv_path=\"/kaggle/working/rag_grid_log.csv\"\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "execution_failed": "2025-04-18T17:59:25.018Z"
        },
        "id": "7_CDByMKxLEO",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}